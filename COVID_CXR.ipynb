{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COVID-CXR.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1Qz5TDGMJ2WEQ5dhm0EYVor4_AVvDQmu8",
      "authorship_tag": "ABX9TyOjpyqDN2ofyugE/coeXlWw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slowvak/AI-Deep-Learning-Lab/blob/master/COVID_CXR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTAtTfiltOam",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "import os\n",
        "\n",
        "\n",
        "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score, classification_report\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import models\n",
        "from torchvision import transforms\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hwy9mMB10dX6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "log = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "\n",
        "def load_model_weights(model, state_dict, verbose=True):\n",
        "    \"\"\"\n",
        "    Loads the model weights from the state dictionary. Function will only load\n",
        "    the weights which have matching key names and dimensions in the state\n",
        "    dictionary.\n",
        "\n",
        "    :param state_dict: Pytorch model state dictionary\n",
        "    :param verbose: bool, If True, the function will print the\n",
        "        weight keys of parametares that can and cannot be loaded from the\n",
        "        checkpoint state dictionary.\n",
        "    :return: The model with loaded weights\n",
        "    \"\"\"\n",
        "    new_state_dict = model.state_dict()\n",
        "    non_loadable, loadable = set(), set()\n",
        "\n",
        "    for k, v in state_dict.items():\n",
        "        if k not in new_state_dict:\n",
        "            non_loadable.add(k)\n",
        "            continue\n",
        "\n",
        "        if v.shape != new_state_dict[k].shape:\n",
        "            non_loadable.add(k)\n",
        "            continue\n",
        "\n",
        "        new_state_dict[k] = v\n",
        "        loadable.add(k)\n",
        "\n",
        "    if verbose:\n",
        "        log.info(\"### Checkpoint weights that WILL be loaded: ###\")\n",
        "        {log.info(k) for k in loadable}\n",
        "\n",
        "        log.info(\"### Checkpoint weights that CANNOT be loaded: ###\")\n",
        "        {log.info(k) for k in non_loadable}\n",
        "\n",
        "    model.load_state_dict(new_state_dict)\n",
        "    return model\n",
        "\n",
        "\n",
        "def to_device(tensor, gpu=False):\n",
        "    \"\"\"\n",
        "    Places a Pytorch Tensor object on a GPU or CPU device.\n",
        "\n",
        "    :param tensor: Pytorch Tensor object\n",
        "    :param gpu: bool, Flag which specifies GPU placement\n",
        "    :return: Tensor object\n",
        "    \"\"\"\n",
        "    return tensor.cuda() if gpu else tensor.cpu()\n",
        "\n",
        "\n",
        "def clf_metrics(predictions, targets, average='macro'):\n",
        "    f1 = f1_score(targets, predictions, average=average)\n",
        "    precision = precision_score(targets, predictions, average=average)\n",
        "    recall = recall_score(targets, predictions, average=average)\n",
        "    acc = accuracy_score(targets, predictions)\n",
        "\n",
        "    return acc, f1, precision, recall\n",
        "\n",
        "\n",
        "def get_learning_rate(optimizer):\n",
        "    \"\"\"\n",
        "    Retrieves the current learning rate. If the optimizer doesn't have\n",
        "    trainable variables, it will raise an error.\n",
        "    :param optimizer: Optimizer object\n",
        "    :return: float, Current learning rate\n",
        "    \"\"\"\n",
        "    if len(optimizer.param_groups) > 0:\n",
        "        return optimizer.param_groups[0]['lr']\n",
        "    else:\n",
        "        raise ValueError('No trainable parameters.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzDzcrtg0CMf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "class COVIDxFolder(Dataset):\n",
        "    def __init__(self, img_dir, labels_file, transforms):\n",
        "        self.img_pths, self.labels = self._prepare_data(img_dir, labels_file)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def _prepare_data(self, img_dir, labels_file):\n",
        "        with open(labels_file, 'r') as f:\n",
        "            labels_raw = f.readlines()\n",
        "\n",
        "        labels, img_pths = [], []\n",
        "        for i in range(len(labels_raw)):\n",
        "            data = labels_raw[i].split()\n",
        "            img_name = data[1]\n",
        "            img_pth = os.path.join(img_dir, img_name)\n",
        "            img_pths.append(img_pth)\n",
        "            labels.append(mapping[data[2]])\n",
        "\n",
        "        return img_pths, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.img_pths[idx]).convert(\"RGB\")\n",
        "        img_tensor = self.transforms(img)\n",
        "\n",
        "        label = self.labels[idx]\n",
        "        label_tensor = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return img_tensor, label_tensor\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccQDv0wI0GEd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def train_transforms(width, height):\n",
        "    trans_list = [\n",
        "        transforms.Resize((height, width)),\n",
        "        transforms.RandomVerticalFlip(p=0.5),\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        transforms.RandomApply([\n",
        "            transforms.RandomAffine(degrees=20,\n",
        "                                    translate=(0.15, 0.15),\n",
        "                                    scale=(0.8, 1.2),\n",
        "                                    shear=5)], p=0.5),\n",
        "        transforms.RandomApply([\n",
        "            transforms.ColorJitter(brightness=0.3, contrast=0.3)], p=0.5),\n",
        "        transforms.ToTensor()\n",
        "    ]\n",
        "    return transforms.Compose(trans_list)\n",
        "\n",
        "\n",
        "def val_transforms(width, height):\n",
        "    trans_list = [\n",
        "        transforms.Resize((height, width)),\n",
        "        transforms.ToTensor()\n",
        "    ]\n",
        "    return transforms.Compose(trans_list)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfkGf5sT0pLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class Trainable(nn.Module):\n",
        "    \"\"\"\n",
        "    Wraps an arbitrary module with a Trainable module. The Trainable module\n",
        "    is used as a wrapper for freezing and thawing module layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, module, name, trainable=True):\n",
        "        super().__init__()\n",
        "        self.module = module\n",
        "        self.name = name\n",
        "        self.trainable_switch(trainable)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        return self.module(*args, **kwargs)\n",
        "\n",
        "    def trainable_switch(self, trainable):\n",
        "        \"\"\"\n",
        "        Makes module layers trainable or not.\n",
        "\n",
        "        :param trainable: bool, False to freeze the layers, True to unfreeze\n",
        "         them.\n",
        "        \"\"\"\n",
        "        for p in self.parameters():\n",
        "            p.requires_grad = trainable\n",
        "\n",
        "\n",
        "def ConvBn2d(in_dim, out_dim, kernel_size,\n",
        "             activation=nn.LeakyReLU(0.1, inplace=True)):\n",
        "    \"\"\"\n",
        "    Wraps Conv2D, Batch Normalization 2D, and an arbitrary activation layers\n",
        "     with a nn.Sequential layer.\n",
        "\n",
        "    :param in_dim: int, Input feature map dimension\n",
        "    :param out_dim: int, Output feature map dimension\n",
        "    :param kernel_size: int or tuple, Convolution kernel size\n",
        "    :return: nn.Sequential structure containing above listed network layers\n",
        "    \"\"\"\n",
        "    padding = kernel_size // 2\n",
        "    net = nn.Sequential(\n",
        "        nn.Conv2d(in_dim, out_dim, kernel_size=kernel_size,\n",
        "                  padding=padding, bias=False),\n",
        "        nn.BatchNorm2d(out_dim),\n",
        "        activation)\n",
        "    return net\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDJw79z71VBS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class COVIDNext50(nn.Module):\n",
        "    def __init__(self, n_classes):\n",
        "        super(COVIDNext50, self).__init__()\n",
        "        self.n_classes = n_classes\n",
        "        trainable = True\n",
        "\n",
        "        # Layers\n",
        "        backbone = models.resnext50_32x4d(pretrained=True)\n",
        "        self.block0 = Trainable(nn.Sequential(\n",
        "                                    backbone.conv1,\n",
        "                                    backbone.bn1,\n",
        "                                    backbone.relu,\n",
        "                                    backbone.maxpool),\n",
        "                                trainable=trainable,\n",
        "                                name=\"conv1\")\n",
        "        self.block1 = Trainable(backbone.layer1,\n",
        "                                trainable=trainable,\n",
        "                                name=\"block1\")\n",
        "        self.block2 = Trainable(backbone.layer2,\n",
        "                                trainable=trainable,\n",
        "                                name=\"block2\")\n",
        "        self.block3 = Trainable(backbone.layer3,\n",
        "                                trainable=trainable,\n",
        "                                name=\"block3\")\n",
        "        self.block4 = Trainable(backbone.layer4,\n",
        "                                trainable=trainable,\n",
        "                                name=\"block4\")\n",
        "        self.backbone_end = Trainable(nn.Sequential(\n",
        "                                        ConvBn2d(2048, 512, 3),\n",
        "                                        ConvBn2d(512, 1024, 1),\n",
        "                                        ConvBn2d(1024, 512, 3)),\n",
        "                                      name=\"back\",\n",
        "                                      trainable=True)\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.logits = Trainable(nn.Linear(512, n_classes),\n",
        "                                name=\"logits\",\n",
        "                                trainable=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        net = input\n",
        "        for layer in [self.block0, self.block1, self.block2, self.block3,\n",
        "                      self.block4]:\n",
        "            net = layer(net)\n",
        "        net = self.backbone_end(net)\n",
        "        net = self.avg_pool(net)\n",
        "        net = torch.squeeze(net)\n",
        "        return self.logits(net)\n",
        "\n",
        "    def probability(self, logits):\n",
        "        return nn.functional.softmax(logits, dim=-1)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liz9-SBItmOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def save_model(model, config):\n",
        "    if isinstance(model, torch.nn.DataParallel):\n",
        "        # Save without the DataParallel module\n",
        "        model_dict = model.module.state_dict()\n",
        "    else:\n",
        "        model_dict = model.state_dict()\n",
        "\n",
        "    state = {\n",
        "        \"state_dict\": model_dict,\n",
        "        \"global_step\": config['global_step'],\n",
        "        \"clf_report\": config['clf_report']\n",
        "    }\n",
        "    f1_macro = config['clf_report']['macro avg']['f1-score'] * 100\n",
        "    name = \"{}_F1_{:.2f}_step_{}.pth\".format(config['name'],\n",
        "                                             f1_macro,\n",
        "                                             config['global_step'])\n",
        "    model_path = os.path.join(config['save_dir'], name)\n",
        "    torch.save(state, model_path)\n",
        "    log.info(\"Saved model to {}\".format(model_path))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twkMxf9CtoYR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def validate(data_loader, model, best_score, global_step):\n",
        "    model.eval()\n",
        "    gts, predictions = [], []\n",
        "\n",
        "    log.info(\"Validation started...\")\n",
        "    for data in data_loader:\n",
        "        imgs, labels = data\n",
        "        imgs = to_device(imgs, gpu=gpu)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(imgs)\n",
        "            #probs = probability(logits)\n",
        "            probs = nn.functional.softmax(logits, dim=-1)\n",
        "            preds = torch.argmax(probs, dim=1).cpu().numpy()\n",
        "\n",
        "        labels = labels.cpu().detach().numpy()\n",
        "\n",
        "        predictions.extend(preds)\n",
        "        gts.extend(labels)\n",
        "\n",
        "    predictions = np.array(predictions, dtype=np.int32)\n",
        "    gts = np.array(gts, dtype=np.int32)\n",
        "    acc, f1, prec, rec = clf_metrics(predictions=predictions,\n",
        "                                          targets=gts,\n",
        "                                          average=\"macro\")\n",
        "    report = classification_report(gts, predictions, output_dict=True)\n",
        "\n",
        "    log.info(\"VALIDATION | Accuracy {:.4f} | F1 {:.4f} | Precision {:.4f} | \"\n",
        "             \"Recall {:.4f}\".format(acc, f1, prec, rec))\n",
        "\n",
        "    if f1 > best_score:\n",
        "        save_config = {\n",
        "                    'name': name,\n",
        "                    'save_dir': ckpts_dir,\n",
        "                    'global_step': global_step,\n",
        "                    'clf_report': report\n",
        "                }\n",
        "        save_model(model=model, config=save_config)\n",
        "        best_score = f1\n",
        "    log.info(\"Validation end\")\n",
        "\n",
        "    model.train()\n",
        "    return best_score\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tB7Tbl4ktrJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NENzmWMWtpNJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "if gpu and not torch.cuda.is_available():\n",
        "    raise ValueError(\"GPU not supported or enabled on this system.\")\n",
        "use_gpu = gpu\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "znJhGrZn3Rjf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "log.info(\"Loading train dataset\")\n",
        "train_dataset = COVIDxFolder(train_imgs, train_labels, train_transforms(width,height))\n",
        "train_loader = DataLoader(train_dataset,\n",
        "                            batch_size=batch_size,\n",
        "                            shuffle=True,\n",
        "                            drop_last=True,\n",
        "                            num_workers=n_threads,\n",
        "                            pin_memory=use_gpu)\n",
        "log.info(\"Number of training examples {}\".format(len(train_dataset)))\n",
        "\n",
        "log.info(\"Loading val dataset\")\n",
        "val_dataset = COVIDxFolder(val_imgs, val_labels, val_transforms(width, height))\n",
        "val_loader = DataLoader(val_dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        shuffle=False,\n",
        "                        num_workers=n_threads,\n",
        "                        pin_memory=use_gpu)\n",
        "log.info(\"Number of validation examples {}\".format(len(val_dataset)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbSVBc_z0VeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# General\n",
        "name = \"COVIDNext50_FlowSIGMA\"\n",
        "gpu = True\n",
        "batch_size = 64\n",
        "n_threads = 20\n",
        "\n",
        "# Model\n",
        "# Model weights path\n",
        "# weights = \"./experiments/ckpts/<model.pth>\"\n",
        "\n",
        "# Optimizer\n",
        "lr = 1e-4\n",
        "weight_decay = 1e-3\n",
        "lr_reduce_factor = 0.7\n",
        "lr_reduce_patience = 5\n",
        "\n",
        "ROOT = '/content/drive/My Drive/Colab Notebooks/COVID-Data/'\n",
        "# Data\n",
        "train_imgs = ROOT+\"train\"\n",
        "train_labels = ROOT+\"train_COVIDx.txt\"\n",
        "\n",
        "val_imgs = ROOT+\"test\"\n",
        "val_labels = ROOT+\"test_COVIDx.txt\"\n",
        "\n",
        "# Categories mapping\n",
        "mapping = {\n",
        "    'normal': 0,\n",
        "    'pneumonia': 1,\n",
        "    'COVID-19': 2\n",
        "}\n",
        "# Loss weigths order follows the order in the category mapping dict\n",
        "loss_weights = [0.05, 0.05, 1.0]\n",
        "\n",
        "width = 256\n",
        "height = 256\n",
        "n_classes = len(mapping)\n",
        "\n",
        "# Training\n",
        "epochs = 300\n",
        "log_steps = 20\n",
        "eval_steps = 40\n",
        "ckpts_dir = ROOT+\"ckpts\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYu2f_4x3WX1",
        "colab_type": "code",
        "outputId": "7958f9cf-c8d9-4141-fdcb-52aa584eddb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "source": [
        "\n",
        "state = None\n",
        "'''\n",
        "if weights:\n",
        "    state = torch.load(weights)\n",
        "    log.info(\"Loaded model weights from: {}\".format(weights))\n",
        "else:\n",
        "    state = None\n",
        "'''\n",
        "\n",
        "state_dict = state[\"state_dict\"] if state else None\n",
        "model = COVIDNext50(n_classes=n_classes)\n",
        "if state_dict:\n",
        "    model = load_model_weights(model=model, state_dict=state_dict)\n",
        "\n",
        "if use_gpu:\n",
        "    model.cuda()\n",
        "    model = torch.nn.DataParallel(model)\n",
        "optim_layers = filter(lambda p: p.requires_grad, model.parameters())\n",
        "\n",
        "# optimizer and lr scheduler\n",
        "optimizer = Adam(optim_layers,\n",
        "                    lr=lr,\n",
        "                    weight_decay=weight_decay)\n",
        "scheduler = ReduceLROnPlateau(optimizer=optimizer,\n",
        "                                factor=lr_reduce_factor,\n",
        "                                patience=lr_reduce_patience,\n",
        "                                mode='max',\n",
        "                                min_lr=1e-7)\n",
        "\n",
        "# Load the last global_step from the checkpoint if existing\n",
        "global_step = 0 if state is None else state['global_step'] + 1\n",
        "\n",
        "class_weights = to_device(torch.FloatTensor(loss_weights), gpu=use_gpu)\n",
        "loss_fn = CrossEntropyLoss(reduction='mean', weight=class_weights)\n",
        "\n",
        "# Reset the best metric score\n",
        "best_score = -1\n",
        "for epoch in range(epochs):\n",
        "    log.info(\"Started epoch {}/{}\".format(epoch + 1, epochs))\n",
        "    for data in train_loader:\n",
        "        imgs, labels = data\n",
        "        imgs = to_device(imgs, gpu=use_gpu)\n",
        "        labels = to_device(labels, gpu=use_gpu)\n",
        "\n",
        "        logits = model(imgs)\n",
        "        loss = loss_fn(logits, labels)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if global_step % log_steps == 0 and global_step > 0:\n",
        "            #probs = probability(logits)\n",
        "            probs = nn.functional.softmax(logits, dim=-1)\n",
        "            preds = torch.argmax(probs, dim=1).detach().cpu().numpy()\n",
        "            labels = labels.cpu().detach().numpy()\n",
        "            acc, f1, _, _ = clf_metrics(preds, labels)\n",
        "            lr = get_learning_rate(optimizer)\n",
        "\n",
        "            log.info(\"Step {} | TRAINING batch: Loss {:.4f} | F1 {:.4f} | \"\n",
        "                        \"Accuracy {:.4f} | LR {:.2e}\".format(global_step,\n",
        "                                                            loss.item(),\n",
        "                                                            f1, acc,\n",
        "                                                            lr))\n",
        "\n",
        "        if global_step % eval_steps == 0 and global_step > 0:\n",
        "            best_score = validate(val_loader,\n",
        "                                    model,\n",
        "                                    best_score=best_score,\n",
        "                                    global_step=global_step\n",
        "                                  )\n",
        "            scheduler.step(best_score)\n",
        "        global_step += 1\n",
        "\n"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:__main__:Started epoch 1/300\n",
            "INFO:__main__:Step 20 | TRAINING batch: Loss 0.4689 | F1 0.8719 | Accuracy 0.8750 | LR 1.00e-04\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "INFO:__main__:Step 40 | TRAINING batch: Loss 0.4239 | F1 0.5981 | Accuracy 0.8906 | LR 1.00e-04\n",
            "INFO:__main__:Validation started...\n",
            "INFO:__main__:VALIDATION | Accuracy 0.7857 | F1 0.6316 | Precision 0.6457 | Recall 0.6700\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-d5b8191746e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     68\u001b[0m                                     \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                                     \u001b[0mbest_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m                                     \u001b[0mglobal_step\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m                                   )\n\u001b[1;32m     72\u001b[0m             \u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-a6d5a330874d>\u001b[0m in \u001b[0;36mvalidate\u001b[0;34m(data_loader, model, best_score, global_step)\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0;34m'clf_report'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 }\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m         \u001b[0mbest_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Validation end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-25-eeaf5516db31>\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, config)\u001b[0m\n\u001b[1;32m     17\u001b[0m                                              config['global_step'])\n\u001b[1;32m     18\u001b[0m     \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'save_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Saved model to {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    328\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/My Drive/Colab Notebooks/COVID-Data/ckpts/COVIDNext50_FlowSIGMA_F1_63.16_step_40.pth'"
          ]
        }
      ]
    }
  ]
}