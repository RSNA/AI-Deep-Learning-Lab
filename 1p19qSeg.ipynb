{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1p19qSeg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slowvak/AI-Deep-Learning-Lab/blob/master/1p19qSeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngjE5zA0F1PE",
        "colab_type": "code",
        "outputId": "f9af5eaf-56d1-46bd-e46c-142cf10e2b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "#Cell 1\n",
        "!pip install pandas\n",
        "\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, Dense, Dropout, Activation, Flatten, BatchNormalization, Reshape\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "from keras.layers.merge import concatenate, add\n",
        "from keras.layers.core import Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "from keras import backend as K\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from natsort import natsorted\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "\n",
        "!rm -rf trainimages\n",
        "!mkdir trainimages\n",
        "!rm -rf trainmasks\n",
        "!mkdir trainmasks\n",
        "\n",
        "!rm -rf validationimages\n",
        "!mkdir validationimages\n",
        "!rm -rf validationmasks\n",
        "!mkdir validationmasks\n",
        "\n",
        "!rm -rf testimages\n",
        "!mkdir testimages\n",
        "!rm -rf testmasks\n",
        "!mkdir testmasks\n",
        "\n",
        "print(\"Tensorflow version: \" + str(tf.__version__))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rED2QjmXAJy9",
        "colab_type": "code",
        "outputId": "b4f7742d-b399-4940-cdac-31ee6f9bdd45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime → \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mon Feb 24 19:37:42 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3epcQ3LVKD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# get the csv file describing images\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pazXvT4N9-T4OAxcuUiuCd3AUmDaAAxF' -O ./data.csv\n",
        "#DATA_FILE = \"./data.csv\"\n",
        "#IMAGE_DIR = './Nifti'\n",
        "# then get the images\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V4zAE19E1kLUK0Z0YIWNpMq0K8lOARtu' -O ./images.zip\n",
        "#!unzip -q -o \"./images.zip\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg-8caamcSTp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqiNgD0zubgQ",
        "colab_type": "code",
        "outputId": "9dac3f6b-7554-4703-fa42-8a4d0e63d064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_FILE = '/content/drive/My Drive/LGG-1p19q/TCIA_LGG_cases_info.csv'\n",
        "IMAGE_DIR = '/content/drive/My Drive/LGG-1p19q/Nifti/'"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v152725DApMh",
        "colab_type": "code",
        "outputId": "5a1fb06e-69d2-4b42-c75e-860ef2ba3385",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "# read and process the csv file to find tumor and annotation slices\n",
        "\n",
        "df = pd.read_csv(DATA_FILE)\n",
        "print (df)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Filename 1p19q  Grade  ... TopOfTumor  BottomOfTrace  TopOfTrace\n",
            "0    LGG-104   d/d      3  ...         47             32          47\n",
            "1    LGG-203   n/n      3  ...         44             21          44\n",
            "2    LGG-210   n/n      2  ...         12              8          12\n",
            "3    LGG-216   d/d      2  ...         35             23          35\n",
            "4    LGG-218   d/d      2  ...         40             32          40\n",
            "..       ...   ...    ...  ...        ...            ...         ...\n",
            "153  LGG-651   d/d      2  ...         39             26          39\n",
            "154  LGG-658   d/d      3  ...         51             45          48\n",
            "155  LGG-659   d/d      2  ...         48             42          46\n",
            "156  LGG-660   d/d      2  ...         32             25          27\n",
            "157  LGG-766   n/n      2  ...         49             38          40\n",
            "\n",
            "[158 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqjlojDN3YmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIM = 256\n",
        "\n",
        "def load_nifti(fname):\n",
        "    # print (fname)\n",
        "    nifti = nib.load(fname)\n",
        "    image = nifti.get_fdata()\n",
        "    dims = np.shape(image)\n",
        "    xd = dims[0]\n",
        "    yd = dims[1]\n",
        "    zd = dims[2]\n",
        "    if xd > DIM or yd > DIM:  # these are all\n",
        "        new_image = np.zeros(shape=(DIM, DIM, zd), dtype = np.float64)\n",
        "        print (\"Resizing \" + str(fname) + \" from \" + str(xd) + \" x \" + str(yd))\n",
        "        for z in range (0, zd):\n",
        "            for W in range(DIM):\n",
        "                for H in range(DIM):\n",
        "                    new_image[H][W][z] = image[2*H][2*W][z]\n",
        "        xd = yd = DIM\n",
        "        try:\n",
        "            image = new_image\n",
        "#            newfname = fname.replace (\".nii.gz\", \"256.nii.gz\")\n",
        "#            nifti.header.set_data_shape([xd, yd, zd])\n",
        "#            nib.save(nib.Nifti1Image(Seg, nifti.affine, nifti.header), newfname)\n",
        "        except:\n",
        "            pass\n",
        "    if (image.max() > 10): # only normalize images, not masks\n",
        "        image.reshape(xd*yd*zd)\n",
        "        image = image - np.min(image)\n",
        "        image = (np.maximum(image, 30) / image.max()) * 255.0\n",
        "    image.reshape(DIM, DIM, zd)\n",
        "    dims = np.shape(image)\n",
        "    if (dims[0] != DIM):\n",
        "        print (\"wrong order\")\n",
        "    return image, nifti"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yObqHvOyXo7-",
        "colab_type": "code",
        "outputId": "04f591ec-d95d-447e-8f4a-520df8024ca1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "Images = []\n",
        "Seg_images = []\n",
        "\n",
        "#studies with 512 matrix: 310, 326, 387, 515\n",
        "\n",
        "for ind in df.index: \n",
        "    if 'LGG' in str(df['Filename'][ind]):\n",
        "        Dir = os.path.join (IMAGE_DIR, df['Filename'][ind])\n",
        "        T1_file = os.path.join (Dir, df['Filename'][ind] + '_T1.nii.gz')\n",
        "        if not os.path.exists(T1_file):\n",
        "            T1_file = os.path.join (Dir, df['Filename'][ind] + '_T1c.nii.gz')\n",
        "        T2_file = os.path.join (Dir, df['Filename'][ind] + '_T2.nii.gz')\n",
        "        Seg_file = os.path.join (Dir, df['Filename'][ind] + '-Segmentation.nii.gz')\n",
        "        Chromosome_status = str(df['1p19q'][ind])\n",
        "        Grade = int(df['Grade'][ind])\n",
        "        Type = str(df['Type'][ind])\n",
        "        BottomOfTumor = int(df['BottomOfTumor'][ind])\n",
        "        TopOfTumor = int(df['TopOfTumor'][ind])\n",
        "        BottomOfTrace = int(df['BottomOfTrace'][ind])\n",
        "        TopOfTrace = int(df['TopOfTrace'][ind])\n",
        "#            print (T1_file + ' size: ' + str(os.path.getsize(T1_file)))\n",
        "        T1, hdr = load_nifti(T1_file)\n",
        "        T2, hdr = load_nifti(T2_file)\n",
        "        Seg, hdr = load_nifti(Seg_file)\n",
        "#Store the training slices into the arrays\n",
        "# note the slice number start from 1 in ITK-Snap, so must start/stop at 1 less\n",
        "        for slc in range (BottomOfTrace-1, TopOfTrace):\n",
        "            # combine the T1 and T2 images\n",
        "            image = np.dstack((T1[:,:, slc], T2[:,:, slc]))\n",
        "            # append them onto prior images\n",
        "            Images.append (image)\n",
        "            # print (\"Sg min: \" + str(Seg[:,:, slc].min()) + \" max: \" + str(Seg[:,:, slc].max()) + \" count: \" + str(Seg[:,:, slc].sum()))\n",
        "            Seg_images.append (Seg[:,:, slc])\n",
        "        print (\"Processed \" + Dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-104\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-203\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-210\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-216\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-218\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-219\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-220\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-225\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-229\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-231\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-233\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-234\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-240\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-241\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-246\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-249\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-254\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-260\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-261\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-263\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-269\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-273\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-274\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-277\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-278\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-280\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-282\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-285\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-286\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-288\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-289\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-293\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-295\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-296\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-297\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-298\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-303\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-304\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-305\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-306\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-307\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-308\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-310/LGG-310_T1c.nii.gz from 512 x 512\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-310/LGG-310_T2.nii.gz from 512 x 512\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-310\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-311\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-313\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-314\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-315\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-316\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-320\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-321\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-325\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-326/LGG-326_T1c.nii.gz from 512 x 512\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-326/LGG-326_T2.nii.gz from 512 x 512\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-326\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-327\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-330\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-331\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-333\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-334\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-337\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-338\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-341\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-343\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-344\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-345\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-346\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-348\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-350\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-351\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-352\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-354\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-355\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-357\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-359\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-360\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-361\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-363\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-365\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-367\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-371\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-373\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-374\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-375\n",
            "Processed /content/drive/My Drive/LGG-1p19q/Nifti/LGG-377\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiNnuEKhkS0t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 2\n",
        "print (str(len(Images)) + \" total labeled images.\")\n",
        "Images = np.asarray(Images)\n",
        "Seg_images = np.asarray(Seg_images)\n",
        "total = len(Images)\n",
        "Num_Train = int(0.8 * total)\n",
        "Num_Val = int(0.1 * total)\n",
        "Num_Test = total - Num_Train - Num_Val\n",
        "train_X = []\n",
        "train_Y = []\n",
        "val_X = []\n",
        "val_Y = []\n",
        "test_X = []\n",
        "test_Y = []\n",
        "\n",
        "train_X = np.asarray(Images[0:Num_Train, :,:])\n",
        "train_X = train_X/255\n",
        "#train_X = train_X[..., np.newaxis]\n",
        "\n",
        "train_Y = Seg_images[0:Num_Train, :,:]\n",
        "train_Y = (train_Y > 0).astype(np.float32)\n",
        "#train_Y = train_Y[..., np.newaxis]\n",
        "train_Y=np.expand_dims(train_Y, axis=-1)\n",
        "val_X = Images[Num_Train:Num_Train+Num_Val, :,:]\n",
        "val_X = val_X/255\n",
        "#val_X = val_X[..., np.newaxis]\n",
        "\n",
        "val_Y = Seg_images[Num_Train: Num_Train + Num_Val, :,:]\n",
        "val_Y = (val_Y > 0).astype(np.float32)\n",
        "#val_Y = val_Y[..., np.newaxis]\n",
        "val_Y=np.expand_dims(val_Y, axis=-1)\n",
        "\n",
        "test_X = Images[-Num_Test:, :,:]\n",
        "test_X = test_X/255\n",
        "#test_X = test_X[..., np.newaxis]\n",
        "\n",
        "test_Y = Seg_images[-Num_Test:, :,:]\n",
        "test_Y = (test_Y > 0).astype(np.float32)\n",
        "#test_Y = test_Y[..., np.newaxis]\n",
        "test_Y=np.expand_dims(test_Y, axis=-1)\n",
        "print(train_X.shape[0],\"images for training,\", val_X.shape[0], \"images for validation, and\", test_X.shape[0], \"images for testing\")\n",
        "\n",
        "WIDTH = train_X.shape[2]\n",
        "HEIGHT = train_X.shape[1]\n",
        "CHANNELS = train_X.shape[3]\n",
        "print ('X and Y dims are ' + str(WIDTH) + 'x' + str(HEIGHT) + \" Channels: \" + str(CHANNELS))\n",
        "print (train_X.shape)\n",
        "print (test_Y.shape)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtR4fbhhhegw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 3\n",
        "def dice_coeff(y_true, y_pred):\n",
        "    # add epsilon to avoid a divide by 0 error in case a slice has no pixels set\n",
        "    # we only care about relative value, not absolute so this alteration doesn't matter\n",
        "    # y_true = tf.expand_dims(y_true, axis=-1)\n",
        "    _epsilon = 10 ** -7\n",
        "    intersections = tf.reduce_sum(y_true * y_pred)\n",
        "    unions = tf.reduce_sum(y_true + y_pred)\n",
        "    dice_scores = (2.0 * intersections + _epsilon) / (unions + _epsilon)\n",
        "    return dice_scores\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "#    y_true = tf.expand_dims(y_true, axis=-1)\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss\n",
        "  \n",
        "get_custom_objects().update({\"dice\": dice_loss})\n",
        "\n",
        "class LayerNormalization (Layer) :\n",
        "    \n",
        "    def call(self, x, mask=None, training=None) :\n",
        "        axis = list (range (1, len (x.shape)))\n",
        "        x /= K.std (x, axis = axis, keepdims = True) + K.epsilon()\n",
        "        x -= K.mean (x, axis = axis, keepdims = True)\n",
        "        return x\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-AbL6fvgPWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 4\n",
        "def build_model(act_fn = 'relu', init_fn = 'he_normal', width=256, height = 256, channels = 2): \n",
        "    inputs = Input((width,height,channels))\n",
        "\n",
        "    # note we use linear function before layer normalization\n",
        "    conv1 = Conv2D(16, 7, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(inputs)\n",
        "    conv1 = LayerNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "#    pool1 = Dropout(0.1)(pool1)\n",
        "    conv2 = Conv2D(16, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "#    pool2 = Dropout(0.1)(pool2)\n",
        "    conv3 = Conv2D(32, 3, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(pool2)\n",
        "    conv3 = LayerNormalization()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(64, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool3)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    pool4 = Dropout(0.1)(pool4)\n",
        "\n",
        "    conv5 = Conv2D(72, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool4)\n",
        "\n",
        "    up6 = Conv2D(64, 2, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv5))\n",
        "    up6 = LayerNormalization()(up6)\n",
        "    merge6 = concatenate([conv4,up6], axis = 3)\n",
        "    conv6 = Conv2D(64, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge6)\n",
        "\n",
        "    up7 = Conv2D(32, 2, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(32, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge7)\n",
        "\n",
        "    up8 = Conv2D(16, 2, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv7))\n",
        "    up8 = LayerNormalization()(up8)\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(16, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge8)\n",
        "\n",
        "    up9 = Conv2D(8, 2, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(8, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "    model = Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = Adam(lr = 1e-4), loss = dice_loss, metrics=[dice_coeff])\n",
        "    return model\n",
        "\n",
        "def build_good_model(act_fn = 'relu', init_fn = 'he_normal', width=256, height = 256, channels = 2): # this gets dice 0.84 at 200 epochs\n",
        "    inputs = Input((width,height,channels))\n",
        "\n",
        "    # note we use linear function before layer normalization\n",
        "    conv1 = Conv2D(8, 5, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(inputs)\n",
        "    conv1 = LayerNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "#    pool1 = Dropout(0.1)(pool1)\n",
        "    conv2 = Conv2D(16, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "#    pool2 = Dropout(0.1)(pool2)\n",
        "    conv3 = Conv2D(32, 3, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(pool2)\n",
        "    conv3 = LayerNormalization()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(64, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool3)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    pool4 = Dropout(0.1)(pool4)\n",
        "\n",
        "    conv5 = Conv2D(72, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool4)\n",
        "\n",
        "    up6 = Conv2D(64, 2, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv5))\n",
        "    up6 = LayerNormalization()(up6)\n",
        "    merge6 = concatenate([conv4,up6], axis = 3)\n",
        "    conv6 = Conv2D(64, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge6)\n",
        "\n",
        "    up7 = Conv2D(32, 2, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(32, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge7)\n",
        "\n",
        "    up8 = Conv2D(16, 2, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv7))\n",
        "    up8 = LayerNormalization()(up8)\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(16, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge8)\n",
        "\n",
        "    up9 = Conv2D(8, 2, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(8, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "    model = Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = Adam(lr = 1e-4), loss = dice_loss, metrics=[dice_coeff])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMOBlqyRmA4w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 5\n",
        "model = build_model(act_fn = 'relu', init_fn = 'he_normal', width=WIDTH, height = HEIGHT, channels = CHANNELS)\n",
        "\n",
        "checkpointer = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n",
        "\n",
        "epochs = 200\n",
        "batch_size = 128\n",
        "results = model.fit(train_X, train_Y, validation_data=(val_X, val_Y), batch_size=batch_size, epochs=epochs, callbacks=[checkpointer, early_stop])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV6XJMaLmeA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 6\n",
        "\n",
        "model.load_weights(\"./model.h5\")\n",
        "preds_test = model.predict(test_X, verbose=1)\n",
        "preds_test = (preds_test > 0.5).astype(np.uint8)\n",
        "\n",
        "\n",
        "from skimage.measure import label\n",
        "\n",
        "def getLargestCC(segmentation):\n",
        "    labels = label(segmentation)\n",
        "    unique, counts = np.unique(labels, return_counts=True)\n",
        "    list_seg=list(zip(unique, counts))[1:] # the 0 label is by default background so take the rest\n",
        "    largest=max(list_seg, key=lambda x:x[1])[0]\n",
        "    labels_max=(labels == largest).astype(np.uint8)\n",
        "    return labels_max\n",
        "\n",
        "def np_dice(true, pred):\n",
        "    intersection = np.sum(true * pred)\n",
        "    dc =(2.0 * intersection) / (np.sum(true) + np.sum(pred))\n",
        "    return dc\n",
        "\n",
        "fig=plt.figure(figsize=(130, 130), dpi = 75)\n",
        "\n",
        "for j in range(0,8,2):\n",
        "    i = random.randint(0,test_X.shape[0]-1)\n",
        "    image = test_X[i,...,0]\n",
        "    mask =  test_Y[i,...,0]\n",
        "    mask = ma.masked_where(mask == 0, mask)\n",
        "    pred = preds_test[i,...,0]\n",
        "    pred = getLargestCC(pred)\n",
        "    pred = ma.masked_where(pred == 0, pred)\n",
        "    \n",
        "    fig.add_subplot(8, 2, j+1)\n",
        "    plt.imshow(image, cmap = \"gray\")\n",
        "    plt.imshow(mask, 'cool', alpha=0.7)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "    fig.add_subplot(8, 2, j+2)\n",
        "    plt.imshow(image, cmap = \"gray\")\n",
        "    plt.imshow(pred, 'cool', alpha=0.7)\n",
        "    plt.title(\"Prediction\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "plt.subplots_adjust(bottom=0.1, left = 0.01, right=0.05, top=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"The dice score for this model is: \", np_dice(test_Y, preds_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "srT8pRuEd8lv",
        "colab": {}
      },
      "source": [
        "# now go back through and segmented the unlabeled slices\n",
        "df = pd.read_csv(DATA_FILE)\n",
        "\n",
        "for ind in df.index: \n",
        "    if 'LGG' in str(df['Filename'][ind]):\n",
        "        Dir = os.path.join (IMAGE_DIR, df['Filename'][ind])\n",
        "        T1_file = os.path.join (Dir, df['Filename'][ind] + '_T1.nii.gz')\n",
        "        if not os.path.exists(T1_file):\n",
        "            T1_file = os.path.join (Dir, df['Filename'][ind] + '_T1c.nii.gz')\n",
        "        T2_file = os.path.join (Dir, df['Filename'][ind] + '_T2.nii.gz')\n",
        "        Seg_file = os.path.join (Dir, df['Filename'][ind] + '-Segmentation.nii.gz')\n",
        "        Chromosome_status = str(df['1p19q'][ind])\n",
        "        Grade = int(df['Grade'][ind])\n",
        "        Type = str(df['Type'][ind])\n",
        "        BottomOfTumor = int(df['BottomOfTumor'][ind])\n",
        "        TopOfTumor = int(df['TopOfTumor'][ind])\n",
        "        BottomOfTrace = int(df['BottomOfTrace'][ind])\n",
        "        TopOfTrace = int(df['TopOfTrace'][ind])\n",
        "#            print (T1_file + ' size: ' + str(os.path.getsize(T1_file)))\n",
        "        T1, hdr = load_nifti(T1_file)\n",
        "        T2, hdr = load_nifti(T2_file)\n",
        "        Seg, hdr = load_nifti(Seg_file) # note saving the header for later use\n",
        "#Store the training slices into the arrays\n",
        "# note the slice number start from 1 in ITK-Snap, so must start/stop at 1 less\n",
        "\n",
        "        nImages = []\n",
        "        if BottomOfTrace < BottomOfTumor or TopOfTrace > TopOfTumor or TopOfTumor-BottomOfTumor<3:\n",
        "            print (\"Error in CSV on \" + str(T2_file) + str(BottomOfTumor) + \", \"+ str(BottomOfTrace)+ \", \"+ str(TopOfTrace)+ \", \"+ str(TopOfTumor))\n",
        "            pass\n",
        "        if BottomOfTrace != BottomOfTumor or TopOfTrace != TopOfTumor:\n",
        "            for slc in range (BottomOfTumor-1, TopOfTumor):\n",
        "                # combine the T1 and T2 images\n",
        "                image = np.dstack((T1[:,:, slc], T2[:,:, slc]))\n",
        "                nImages.append (image)\n",
        "    #            print (str(slc))\n",
        "            nImages = np.asarray(nImages)\n",
        "            preds = model.predict(nImages, verbose=1)\n",
        "            preds = (preds > 0.5).astype(np.uint8)\n",
        "\n",
        "    #       msk = []\n",
        "    #       for slc in range (BottomOfTumor-1, TopOfTumor):\n",
        "    #           msk = np.append(preds[slc-BottomOfTumor+1,...,0])\n",
        "\n",
        "            preds = preds[:,:,:,0]\n",
        "            msk = getLargestCC(preds)\n",
        "            msk = ma.masked_where(msk == 0, msk)\n",
        "\n",
        "            for slc in range(BottomOfTumor-1, TopOfTumor):\n",
        "                Seg[:,:,slc] = msk[slc-BottomOfTumor+1,:,:]\n",
        "            Seg = np.array(Seg, dtype=np.int16)\n",
        "            fname = os.path.join (Dir, df['Filename'][ind] + '-Segmentation.nii.gz')\n",
        "    #        print (\"Saving \" + fname)\n",
        "            nib.save(nib.Nifti1Image(Seg, hdr.affine, hdr.header), fname)\n",
        "            print (\"Processed \" + Dir)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}