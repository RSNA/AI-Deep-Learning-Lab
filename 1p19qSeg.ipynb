{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "1p19qSeg.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slowvak/AI-Deep-Learning-Lab/blob/master/1p19qSeg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngjE5zA0F1PE",
        "colab_type": "code",
        "outputId": "3ad64dc3-d517-4fb2-f2ce-31c86c2d95d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        }
      },
      "source": [
        "#Cell 1\n",
        "!pip install pandas\n",
        "\n",
        "import numpy as np\n",
        "import numpy.ma as ma\n",
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from keras.models import Model, load_model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, Dense, Dropout, Activation, Flatten, BatchNormalization, Reshape\n",
        "from keras.engine.topology import Layer\n",
        "\n",
        "from keras.layers.merge import concatenate, add\n",
        "from keras.layers.core import Lambda\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "from keras import backend as K\n",
        "from keras.utils.generic_utils import get_custom_objects\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio\n",
        "from natsort import natsorted\n",
        "import random\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "\n",
        "!rm -rf trainimages\n",
        "!mkdir trainimages\n",
        "!rm -rf trainmasks\n",
        "!mkdir trainmasks\n",
        "\n",
        "!rm -rf validationimages\n",
        "!mkdir validationimages\n",
        "!rm -rf validationmasks\n",
        "!mkdir validationmasks\n",
        "\n",
        "!rm -rf testimages\n",
        "!mkdir testimages\n",
        "!rm -rf testmasks\n",
        "!mkdir testmasks\n",
        "\n",
        "print(\"Tensorflow version: \" + str(tf.__version__))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.5)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Tensorflow version: 1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rED2QjmXAJy9",
        "colab_type": "code",
        "outputId": "c0a2d6ac-f588-4eeb-dd20-3c8f875126d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        }
      },
      "source": [
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime â†’ \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Feb 22 19:47:07 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 440.48.02    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3epcQ3LVKD8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# get the csv file describing images\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1pazXvT4N9-T4OAxcuUiuCd3AUmDaAAxF' -O ./data.csv\n",
        "#DATA_FILE = \"./data.csv\"\n",
        "#IMAGE_DIR = './Nifti'\n",
        "# then get the images\n",
        "#!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1V4zAE19E1kLUK0Z0YIWNpMq0K8lOARtu' -O ./images.zip\n",
        "#!unzip -q -o \"./images.zip\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg-8caamcSTp",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GqiNgD0zubgQ",
        "colab_type": "code",
        "outputId": "081279be-9e5b-43f2-8d7d-fc68772b93a1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_FILE = '/content/drive/My Drive/LGG-1p19q/TCIA_LGG_cases_info.csv'\n",
        "IMAGE_DIR = '/content/drive/My Drive/LGG-1p19q/Nifti/'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v152725DApMh",
        "colab_type": "code",
        "outputId": "3e39e330-188f-46b6-8bfc-f5da0bda59dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        }
      },
      "source": [
        "# read and process the csv file to find tumor and annotation slices\n",
        "\n",
        "df = pd.read_csv(DATA_FILE)\n",
        "print (df)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Filename 1p19q  Grade  ... TopOfTumor  BottomOfTrace  TopOfTrace\n",
            "0    LGG-104   d/d    3.0  ...       47.0           39.0        41.0\n",
            "1    LGG-203   n/n    3.0  ...       44.0           29.0        31.0\n",
            "2    LGG-210   n/n    2.0  ...       12.0            8.0        12.0\n",
            "3    LGG-216   d/d    2.0  ...       35.0           31.0        33.0\n",
            "4    LGG-218   d/d    2.0  ...       40.0           35.0        37.0\n",
            "..       ...   ...    ...  ...        ...            ...         ...\n",
            "993      NaN   NaN    NaN  ...        NaN            NaN         NaN\n",
            "994      NaN   NaN    NaN  ...        NaN            NaN         NaN\n",
            "995      NaN   NaN    NaN  ...        NaN            NaN         NaN\n",
            "996      NaN   NaN    NaN  ...        NaN            NaN         NaN\n",
            "997      NaN   NaN    NaN  ...        NaN            NaN         NaN\n",
            "\n",
            "[998 rows x 8 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqjlojDN3YmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DIM = 256\n",
        "\n",
        "def load_nifti(fname):\n",
        "    # print (fname)\n",
        "    nifti = nib.load(fname)\n",
        "    image = nifti.get_fdata()\n",
        "    dims = np.shape(image)\n",
        "    xd = dims[0]\n",
        "    yd = dims[1]\n",
        "    zd = dims[2]\n",
        "    if xd > DIM or yd > DIM:  # these are all\n",
        "        new_image = np.zeros(shape=(DIM, DIM, zd), dtype = np.float64)\n",
        "        print (\"Resizing \" + str(fname) + \" from \" + str(xd) + \" x \" + str(yd))\n",
        "        for z in range (0, zd):\n",
        "            for W in range(DIM):\n",
        "                for H in range(DIM):\n",
        "                    new_image[H][W][z] = image[2*H][2*W][z]\n",
        "        xd = yd = DIM\n",
        "        image = new_image\n",
        "#        nifti.set_data_shape((xd, yd, zd))\n",
        "#        new_name = str.replace(fname, \".nii\", \"resize.nii\")\n",
        "#        nib.save(image, new_name)\n",
        "    if (image.max() > 10): # only normalize images, not masks\n",
        "        image.reshape(xd*yd*zd)\n",
        "        image = image - np.min(image)\n",
        "        image = (np.maximum(image, 30) / image.max()) * 255.0\n",
        "        image.reshape(DIM, DIM, zd)\n",
        "        dims = np.shape(image)\n",
        "    if (dims[0] != DIM):\n",
        "        print (\"wrong order\")\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yObqHvOyXo7-",
        "colab_type": "code",
        "outputId": "60ea449a-3a26-4fc7-f305-7ca8d186c5c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 277
        }
      },
      "source": [
        "Images = []\n",
        "Seg_images = []\n",
        "\n",
        "for ind in df.index: \n",
        "    if 'LGG' in str(df['Filename'][ind]):\n",
        "        Dir = os.path.join (IMAGE_DIR, df['Filename'][ind])\n",
        "        T1_file = os.path.join (Dir, df['Filename'][ind] + '_T1.nii.gz')\n",
        "        if not os.path.exists(T1_file):\n",
        "            T1_file = os.path.join (Dir, df['Filename'][ind] + '_T1c.nii.gz')\n",
        "        T2_file = os.path.join (Dir, df['Filename'][ind] + '_T2.nii.gz')\n",
        "        Seg_file = os.path.join (Dir, df['Filename'][ind] + '-Segmentation.nii.gz')\n",
        "        Chromosome_status = str(df['1p19q'][ind])\n",
        "        Grade = int(df['Grade'][ind])\n",
        "        Type = str(df['Type'][ind])\n",
        "        BottomOfTumor = int(df['BottomOfTumor'][ind])\n",
        "        TopOfTumor = int(df['TopOfTumor'][ind])\n",
        "        BottomOfTrace = int(df['BottomOfTrace'][ind])\n",
        "        TopOfTrace = int(df['TopOfTrace'][ind])\n",
        "#            print (T1_file + ' size: ' + str(os.path.getsize(T1_file)))\n",
        "        T1 = load_nifti(T1_file)\n",
        "        T2 = load_nifti(T2_file)\n",
        "        Seg = load_nifti(Seg_file)\n",
        "#Store the training slices into the arrays\n",
        "# note the slice number start from 1 in ITK-Snap, so must start/stop at 1 less\n",
        "        for slc in range (BottomOfTrace-1, TopOfTrace):\n",
        "            # combine the T1 and T2 images\n",
        "            image = np.dstack((T1[:,:, slc], T2[:,:, slc]))\n",
        "            # append them onto prior images\n",
        "            Images.append (image)\n",
        "            # print (\"Sg min: \" + str(Seg[:,:, slc].min()) + \" max: \" + str(Seg[:,:, slc].max()) + \" count: \" + str(Seg[:,:, slc].sum()))\n",
        "            Seg_images.append (Seg[:,:, slc])\n",
        "#        print (\"Processed \" + Dir)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-310/LGG-310_T1c.nii.gz from 512 x 512\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-310/LGG-310_T2.nii.gz from 512 x 512\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-310/LGG-310-Segmentation.nii.gz from 512 x 512\n",
            "wrong order\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-326/LGG-326_T1c.nii.gz from 512 x 512\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-326/LGG-326_T2.nii.gz from 512 x 512\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-326/LGG-326-Segmentation.nii.gz from 512 x 512\n",
            "wrong order\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-387/LGG-387_T1c.nii.gz from 512 x 512\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-387/LGG-387_T2.nii.gz from 512 x 512\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-387/LGG-387-Segmentation.nii.gz from 512 x 512\n",
            "wrong order\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-515/LGG-515_T1c.nii.gz from 512 x 512\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-515/LGG-515_T2.nii.gz from 512 x 512\n",
            "Resizing /content/drive/My Drive/LGG-1p19q/Nifti/LGG-515/LGG-515-Segmentation.nii.gz from 512 x 512\n",
            "wrong order\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iiNnuEKhkS0t",
        "colab_type": "code",
        "outputId": "7b0045fe-9147-4b6f-a357-31386eeaaef1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        }
      },
      "source": [
        "# Cell 2\n",
        "print (str(len(Images)) + \" total labeled images.\")\n",
        "Images = np.asarray(Images)\n",
        "Seg_images = np.asarray(Seg_images)\n",
        "total = len(Images)\n",
        "Num_Train = int(0.8 * total)\n",
        "Num_Val = int(0.1 * total)\n",
        "Num_Test = total - Num_Train - Num_Val\n",
        "train_X = []\n",
        "train_Y = []\n",
        "val_X = []\n",
        "val_Y = []\n",
        "test_X = []\n",
        "test_Y = []\n",
        "\n",
        "train_X = np.asarray(Images[0:Num_Train, :,:])\n",
        "train_X = train_X/255\n",
        "\n",
        "train_Y = Seg_images[0:Num_Train, :,:]\n",
        "train_Y = (train_Y > 0).astype(np.float32)\n",
        "train_Y = np.expand_dims(train_Y, axis=-1)\n",
        "\n",
        "\n",
        "val_X = Images[Num_Train:Num_Train+Num_Val, :,:]\n",
        "val_X = val_X/255\n",
        "val_Y = Seg_images[Num_Train: Num_Train + Num_Val, :,:]\n",
        "val_Y = (val_Y > 0).astype(np.float32)\n",
        "val_Y = val_Y[..., np.newaxis]\n",
        "val_Y=np.expand_dims(val_Y, axis=-1)\n",
        "\n",
        "test_X = Images[-Num_Test:, :,:]\n",
        "test_X = test_X/255\n",
        "test_Y = Seg_images[-Num_Test:, :,:]\n",
        "test_Y = (test_Y > 0).astype(np.float32)\n",
        "test_Y=np.expand_dims(test_Y, axis=-1)\n",
        "\n",
        "print(train_X.shape[0],\"images for training,\", val_X.shape[0], \"images for validation, and\", test_X.shape[0], \"images for testing\")\n",
        "\n",
        "WIDTH = train_X.shape[2]\n",
        "HEIGHT = train_X.shape[1]\n",
        "CHANNELS = train_X.shape[3]\n",
        "print ('X and Y dims are ' + str(WIDTH) + 'x' + str(HEIGHT) + \" Channels: \" + str(CHANNELS))\n",
        "print (train_X.shape)\n",
        "print (test_Y.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "699 total labeled images.\n",
            "559 images for training, 69 images for validation, and 71 images for testing\n",
            "X and Y dims are 256x256 Channels: 2\n",
            "(559, 256, 256, 2)\n",
            "(71, 256, 256, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtR4fbhhhegw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 3\n",
        "def dice_coeff(y_true, y_pred):\n",
        "    # add epsilon to avoid a divide by 0 error in case a slice has no pixels set\n",
        "    # we only care about relative value, not absolute so this alteration doesn't matter\n",
        "    # y_true = tf.expand_dims(y_true, axis=-1)\n",
        "    _epsilon = 10 ** -7\n",
        "    intersections = tf.reduce_sum(y_true * y_pred)\n",
        "    unions = tf.reduce_sum(y_true + y_pred)\n",
        "    dice_scores = (2.0 * intersections + _epsilon) / (unions + _epsilon)\n",
        "    return dice_scores\n",
        "\n",
        "def dice_loss(y_true, y_pred):\n",
        "#    y_true = tf.expand_dims(y_true, axis=-1)\n",
        "    loss = 1 - dice_coeff(y_true, y_pred)\n",
        "    return loss\n",
        "  \n",
        "get_custom_objects().update({\"dice\": dice_loss})\n",
        "\n",
        "class LayerNormalization (Layer) :\n",
        "    \n",
        "    def call(self, x, mask=None, training=None) :\n",
        "        axis = list (range (1, len (x.shape)))\n",
        "        x /= K.std (x, axis = axis, keepdims = True) + K.epsilon()\n",
        "        x -= K.mean (x, axis = axis, keepdims = True)\n",
        "        return x\n",
        "        \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-AbL6fvgPWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 4\n",
        "def build_model(act_fn = 'relu', init_fn = 'he_normal', width=256, height = 256, channels = 2): \n",
        "    inputs = Input((width,height,channels))\n",
        "\n",
        "    # note we use linear function before layer normalization\n",
        "    conv1 = Conv2D(8, 5, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(inputs)\n",
        "    conv1 = LayerNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "#    pool1 = Dropout(0.1)(pool1)\n",
        "    conv2 = Conv2D(16, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "#    pool2 = Dropout(0.1)(pool2)\n",
        "    conv3 = Conv2D(32, 3, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(pool2)\n",
        "    conv3 = LayerNormalization()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(64, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool3)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    pool4 = Dropout(0.1)(pool4)\n",
        "\n",
        "    conv5 = Conv2D(72, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool4)\n",
        "\n",
        "    up6 = Conv2D(64, 2, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv5))\n",
        "    up6 = LayerNormalization()(up6)\n",
        "    merge6 = concatenate([conv4,up6], axis = 3)\n",
        "    conv6 = Conv2D(64, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge6)\n",
        "\n",
        "    up7 = Conv2D(32, 2, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(32, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge7)\n",
        "\n",
        "    up8 = Conv2D(16, 2, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv7))\n",
        "    up8 = LayerNormalization()(up8)\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(16, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge8)\n",
        "\n",
        "    up9 = Conv2D(8, 2, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(8, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "    model = Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = Adam(lr = 1e-4), loss = dice_loss, metrics=[dice_coeff])\n",
        "    return model\n",
        "\n",
        "def build_good_model(act_fn = 'relu', init_fn = 'he_normal', width=256, height = 256, channels = 2): # this gets dice 0.84 at 200 epochs\n",
        "    inputs = Input((width,height,channels))\n",
        "\n",
        "    # note we use linear function before layer normalization\n",
        "    conv1 = Conv2D(8, 5, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(inputs)\n",
        "    conv1 = LayerNormalization()(conv1)\n",
        "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
        "#    pool1 = Dropout(0.1)(pool1)\n",
        "    conv2 = Conv2D(16, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool1)\n",
        "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
        "#    pool2 = Dropout(0.1)(pool2)\n",
        "    conv3 = Conv2D(32, 3, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(pool2)\n",
        "    conv3 = LayerNormalization()(conv3)\n",
        "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
        "    conv4 = Conv2D(64, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool3)\n",
        "    pool4 = MaxPooling2D(pool_size=(2, 2))(conv4)\n",
        "    pool4 = Dropout(0.1)(pool4)\n",
        "\n",
        "    conv5 = Conv2D(72, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(pool4)\n",
        "\n",
        "    up6 = Conv2D(64, 2, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv5))\n",
        "    up6 = LayerNormalization()(up6)\n",
        "    merge6 = concatenate([conv4,up6], axis = 3)\n",
        "    conv6 = Conv2D(64, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge6)\n",
        "\n",
        "    up7 = Conv2D(32, 2, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv6))\n",
        "    merge7 = concatenate([conv3,up7], axis = 3)\n",
        "    conv7 = Conv2D(32, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge7)\n",
        "\n",
        "    up8 = Conv2D(16, 2, activation = 'linear', padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv7))\n",
        "    up8 = LayerNormalization()(up8)\n",
        "    merge8 = concatenate([conv2,up8], axis = 3)\n",
        "    conv8 = Conv2D(16, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge8)\n",
        "\n",
        "    up9 = Conv2D(8, 2, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(UpSampling2D(size = (2,2))(conv8))\n",
        "    merge9 = concatenate([conv1,up9], axis = 3)\n",
        "    conv9 = Conv2D(8, 3, activation = act_fn, padding = 'same', kernel_initializer = init_fn)(merge9)\n",
        "    conv10 = Conv2D(1, 1, activation = 'sigmoid')(conv9)\n",
        "    model = Model(inputs = inputs, outputs = conv10)\n",
        "\n",
        "    model.compile(optimizer = Adam(lr = 1e-4), loss = dice_loss, metrics=[dice_coeff])\n",
        "    return model\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMOBlqyRmA4w",
        "colab_type": "code",
        "outputId": "d45b44fd-c413-4cf9-9c8a-7a90240f2a4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Cell 5\n",
        "model = build_model(act_fn = 'relu', init_fn = 'he_normal', width=WIDTH, height = HEIGHT, channels = CHANNELS)\n",
        "\n",
        "checkpointer = ModelCheckpoint('model.h5', verbose=1, save_best_only=True)\n",
        "\n",
        "epochs = 400\n",
        "batch_size = 128\n",
        "results = model.fit(train_X, train_Y, validation_data=(val_X, val_Y), batch_size=batch_size, epochs=epochs, callbacks=[checkpointer])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4479: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2239: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "Train on 559 samples, validate on 69 samples\n",
            "Epoch 1/400\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "512/559 [==========================>...] - ETA: 1s - loss: 0.9408 - dice_coeff: 0.0592"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "InvalidArgumentError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-0d828ae9a0ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpointer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m     def evaluate(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    216\u001b[0m                                          \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m                                          \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 218\u001b[0;31m                                          verbose=0)\n\u001b[0m\u001b[1;32m    219\u001b[0m                     \u001b[0mval_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                     \u001b[0;31m# Same labels assumed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mtest_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    468\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mInvalidArgumentError\u001b[0m: 2 root error(s) found.\n  (0) Invalid argument: Incompatible shapes: [69,256,256,1] vs. [69,256,256,1,1]\n\t [[{{node loss/conv2d_14_loss/add}}]]\n  (1) Invalid argument: Incompatible shapes: [69,256,256,1] vs. [69,256,256,1,1]\n\t [[{{node loss/conv2d_14_loss/add}}]]\n\t [[metrics/dice_coeff/Mean/_363]]\n0 successful operations.\n0 derived errors ignored."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iV6XJMaLmeA7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 6\n",
        "model.load_weights(\"./model84.h5\")\n",
        "preds_test = model.predict(test_X, verbose=1)\n",
        "preds_test = (preds_test > 0.5).astype(np.uint8)\n",
        "\n",
        "def np_dice(true, pred):\n",
        "    intersection = np.sum(true * pred)\n",
        "    dc =(2.0 * intersection) / (np.sum(true) + np.sum(pred))\n",
        "    return dc\n",
        "\n",
        "fig=plt.figure(figsize=(130, 130), dpi = 75)\n",
        "\n",
        "for j in range(0,8,2):\n",
        "    i = random.randint(0,test_X.shape[0]-1)\n",
        "    image = test_X[i,...,0]\n",
        "    mask =  test_Y[i,...,0]\n",
        "    mask = ma.masked_where(mask == 0, mask)\n",
        "    pred = preds_test[i,...,0]\n",
        "    pred = ma.masked_where(pred == 0, pred)\n",
        "    \n",
        "    fig.add_subplot(8, 2, j+1)\n",
        "    plt.imshow(image, cmap = \"gray\")\n",
        "    plt.imshow(mask, 'cool', alpha=0.7)\n",
        "    plt.title(\"Ground Truth\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "    fig.add_subplot(8, 2, j+2)\n",
        "    plt.imshow(image, cmap = \"gray\")\n",
        "    plt.imshow(pred, 'cool', alpha=0.7)\n",
        "    plt.title(\"Prediction\")\n",
        "    plt.axis('off')\n",
        "    \n",
        "plt.subplots_adjust(bottom=0.1, left = 0.01, right=0.05, top=0.3)\n",
        "plt.show()\n",
        "\n",
        "print(\"The dice score for this model is: \", np_dice(test_Y, preds_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tziHz6hEIIVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# now go back through and segmented the unlabeled slices\n",
        "\n",
        "\n",
        "for ind in df.index: \n",
        "    if 'LGG' in str(df['Filename'][ind]):\n",
        "        Dir = os.path.join (IMAGE_DIR, df['Filename'][ind])\n",
        "        T1_file = os.path.join (Dir, df['Filename'][ind] + '_T1.nii.gz')\n",
        "        if not os.path.exists(T1_file):\n",
        "            T1_file = os.path.join (Dir, df['Filename'][ind] + '_T1c.nii.gz')\n",
        "        T2_file = os.path.join (Dir, df['Filename'][ind] + '_T2.nii.gz')\n",
        "        Seg_file = os.path.join (Dir, df['Filename'][ind] + '-Segmentation.nii.gz')\n",
        "        Chromosome_status = str(df['1p19q'][ind])\n",
        "        Grade = int(df['Grade'][ind])\n",
        "        Type = str(df['Type'][ind])\n",
        "        BottomOfTumor = int(df['BottomOfTumor'][ind])\n",
        "        TopOfTumor = int(df['TopOfTumor'][ind])\n",
        "        BottomOfTrace = int(df['BottomOfTrace'][ind])\n",
        "        TopOfTrace = int(df['TopOfTrace'][ind])\n",
        "#            print (T1_file + ' size: ' + str(os.path.getsize(T1_file)))\n",
        "        T1 = load_nifti(T1_file)\n",
        "        T2 = load_nifti(T2_file)\n",
        "        Seg = load_nifti(Seg_file)\n",
        "#Store the training slices into the arrays\n",
        "# note the slice number start from 1 in ITK-Snap, so must start/stop at 1 less\n",
        "\n",
        "        Aletered = False\n",
        "        nImages = []\n",
        "        for slc in range(BottomOfTumor -1 , BottomOfTrace-1):  # there are unlabeled slices\n",
        "            # combine the T1 and T2 images\n",
        "            image = np.dstack((T1[:,:, slc], T2[:,:, slc]))\n",
        "            # append them onto prior images\n",
        "            nImages.append (image)\n",
        "        if BottomOfTrace > BottomOfTumor:\n",
        "            preds_test = model.predict(nImages, verbose=1)\n",
        "            preds_test = (preds_test > 0.5).astype(np.uint8)\n",
        "            Seg[:,:, BottomOfTumor-1:BottomOfTrace-1] = preds_test\n",
        "            Altered = True\n",
        "        nImages = []\n",
        "        for slc in range(TopOfTrace -1 , TopOfTumor-1):  # there are unlabeled slices\n",
        "            # combine the T1 and T2 images\n",
        "            image = np.dstack((T1[:,:, slc], T2[:,:, slc]))\n",
        "            # append them onto prior images\n",
        "            nImages.append (image)\n",
        "        if TopOfTumor > TopOfTrace:  # there are unlabeled slices\n",
        "            preds_test = model.predict(nImages, verbose=1)\n",
        "            preds_test = (preds_test > 0.5).astype(np.uint8)\n",
        "            Seg[:,:, TopOfTrace:TopOfTumor] = preds_test\n",
        "            Altered = True\n",
        "# now write results out but not over the top of input\n",
        "        if Altered:\n",
        "            nib.save(Seg, os.path.join (Dir, df['Filename'][ind] + '-NewSegmentation.nii.gz'))\n",
        "        print (\"Processed \" + Dir)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}