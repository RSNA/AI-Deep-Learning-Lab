{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KarrasPGGAN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPQTmJGLo91LUqM1nEuvE39",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slowvak/AI-Deep-Learning-Lab/blob/master/KarrasPGGAN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuCibckkQItC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "16035068-2f81-45c0-a5ca-a2219458c59a"
      },
      "source": [
        "# from karras PGGAN\n",
        "\n",
        "import os\n",
        "import time\n",
        "import sys\n",
        "import inspect\n",
        "import importlib\n",
        "import imp\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "from collections import OrderedDict\n",
        "import glob\n",
        "import datetime\n",
        "import pickle\n",
        "import re\n",
        "import scipy.ndimage\n",
        "import PIL.Image\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/compat/v2_compat.py:96: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s35Kio0iW0ok",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "92d29386-7e39-481a-d9fa-697dbae6d931"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp '/content/drive/My Drive/Colab Notebooks/progressive_growing_of_gans/myimages.zip' './myimages.zip'\n",
        "# https://drive.google.com/file/d/1EkBJN46f4rAlBbQFqYMwwVL2nnxRiofX/view?usp=sharing\n",
        "!rm -rf './myimages'\n",
        "!unzip \"./myimages.zip\"\n",
        "#!unzip -q \"'/content/drive/My Drive/Colab Notebooks/progressive_growing_of_gans/myimages.zip -d ./datasets' \n",
        "\n",
        "!mkdir './results'\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "replace myimages/myimages-r04.tfrecords? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "A\n",
            "mkdir: cannot create directory ‘./results’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIkGuidbYf2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#--------------  Config.py\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Convenience class that behaves exactly like dict(), but allows accessing\n",
        "# the keys and values using the attribute syntax, i.e., \"mydict.key = value\".\n",
        "\n",
        "class EasyDict(dict):\n",
        "    def __init__(self, *args, **kwargs): super().__init__(*args, **kwargs)\n",
        "    def __getattr__(self, name): return self[name]\n",
        "    def __setattr__(self, name, value): self[name] = value\n",
        "    def __delattr__(self, name): del self[name]\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Paths.\n",
        "\n",
        "data_dir = './myimages'\n",
        "result_dir = './results'\n",
        "if not os.path.exists(result_dir):\n",
        "    os.makedirs(result_dir)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# TensorFlow options.\n",
        "\n",
        "tf_config = EasyDict()  # TensorFlow session config, set by init_tf().\n",
        "env = EasyDict()        # Environment variables, set by the main program in train.py.\n",
        "\n",
        "tf_config['graph_options.place_pruned_graph']   = True      # False (default) = Check that all ops are available on the designated device. True = Skip the check for ops that are not used.\n",
        "#tf_config['gpu_options.allow_growth']          = False     # False (default) = Allocate all GPU memory at the beginning. True = Allocate only as much GPU memory as needed.\n",
        "#env.CUDA_VISIBLE_DEVICES                       = '0'       # Unspecified (default) = Use all available GPUs. List of ints = CUDA device numbers to use.\n",
        "env.TF_CPP_MIN_LOG_LEVEL                        = '1'       # 0 (default) = Print all available debug info from TensorFlow. 1 = Print warnings and errors, but disable debug info.\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Official training configs, targeted mainly for CelebA-HQ.\n",
        "# To run, comment/uncomment the lines as appropriate and launch train.py.\n",
        "\n",
        "desc        = 'pgan'                                        # Description string included in result subdir name.\n",
        "random_seed = 1000                                          # Global random seed.\n",
        "dataset     = EasyDict()                                    # Options for dataset.load_dataset().\n",
        "train       = EasyDict(func='train_progressive_gan')  # Options for main training func.\n",
        "G           = EasyDict(func='G_paper')             # Options for generator network.\n",
        "D           = EasyDict(func='D_paper')             # Options for discriminator network.\n",
        "G_opt       = EasyDict(beta1=0.0, beta2=0.99, epsilon=1e-8) # Options for generator optimizer.\n",
        "D_opt       = EasyDict(beta1=0.0, beta2=0.99, epsilon=1e-8) # Options for discriminator optimizer.\n",
        "G_loss      = EasyDict(func='G_wgan_acgan')            # Options for generator loss.\n",
        "D_loss      = EasyDict(func='D_wgangp_acgan')          # Options for discriminator loss.\n",
        "sched       = EasyDict()                                    # Options for train.TrainingSchedule.\n",
        "grid        = EasyDict(size='1080p', layout='random')       # Options for train.setup_snapshot_image_grid().\n",
        "\n",
        "# Dataset (choose one).\n",
        "desc += '-myimages';            dataset = EasyDict(tfrecord_dir=data_dir); train.mirror_augment = True\n",
        "#desc += '-celebahq';            dataset = EasyDict(tfrecord_dir='celebahq'); train.mirror_augment = True\n",
        "#desc += '-celeba';              dataset = EasyDict(tfrecord_dir='celeba'); train.mirror_augment = True\n",
        "#desc += '-cifar10';             dataset = EasyDict(tfrecord_dir='cifar10')\n",
        "#desc += '-cifar100';            dataset = EasyDict(tfrecord_dir='cifar100')\n",
        "#desc += '-svhn';                dataset = EasyDict(tfrecord_dir='svhn')\n",
        "#desc += '-mnist';               dataset = EasyDict(tfrecord_dir='mnist')\n",
        "#desc += '-mnistrgb';            dataset = EasyDict(tfrecord_dir='mnistrgb')\n",
        "#desc += '-syn1024rgb';          dataset = EasyDict(class_name='dataset.SyntheticDataset', resolution=1024, num_channels=3)\n",
        "#desc += '-lsun-airplane';       dataset = EasyDict(tfrecord_dir='lsun-airplane-100k');       train.mirror_augment = True\n",
        "#desc += '-lsun-bedroom';        dataset = EasyDict(tfrecord_dir='lsun-bedroom-100k');        train.mirror_augment = True\n",
        "#desc += '-lsun-bicycle';        dataset = EasyDict(tfrecord_dir='lsun-bicycle-100k');        train.mirror_augment = True\n",
        "#desc += '-lsun-bird';           dataset = EasyDict(tfrecord_dir='lsun-bird-100k');           train.mirror_augment = True\n",
        "#desc += '-lsun-boat';           dataset = EasyDict(tfrecord_dir='lsun-boat-100k');           train.mirror_augment = True\n",
        "#desc += '-lsun-bottle';         dataset = EasyDict(tfrecord_dir='lsun-bottle-100k');         train.mirror_augment = True\n",
        "#desc += '-lsun-bridge';         dataset = EasyDict(tfrecord_dir='lsun-bridge-100k');         train.mirror_augment = True\n",
        "#desc += '-lsun-bus';            dataset = EasyDict(tfrecord_dir='lsun-bus-100k');            train.mirror_augment = True\n",
        "#desc += '-lsun-car';            dataset = EasyDict(tfrecord_dir='lsun-car-100k');            train.mirror_augment = True\n",
        "#desc += '-lsun-cat';            dataset = EasyDict(tfrecord_dir='lsun-cat-100k');            train.mirror_augment = True\n",
        "#desc += '-lsun-chair';          dataset = EasyDict(tfrecord_dir='lsun-chair-100k');          train.mirror_augment = True\n",
        "#desc += '-lsun-churchoutdoor';  dataset = EasyDict(tfrecord_dir='lsun-churchoutdoor-100k');  train.mirror_augment = True\n",
        "#desc += '-lsun-classroom';      dataset = EasyDict(tfrecord_dir='lsun-classroom-100k');      train.mirror_augment = True\n",
        "#desc += '-lsun-conferenceroom'; dataset = EasyDict(tfrecord_dir='lsun-conferenceroom-100k'); train.mirror_augment = True\n",
        "#desc += '-lsun-cow';            dataset = EasyDict(tfrecord_dir='lsun-cow-100k');            train.mirror_augment = True\n",
        "#desc += '-lsun-diningroom';     dataset = EasyDict(tfrecord_dir='lsun-diningroom-100k');     train.mirror_augment = True\n",
        "#desc += '-lsun-diningtable';    dataset = EasyDict(tfrecord_dir='lsun-diningtable-100k');    train.mirror_augment = True\n",
        "#desc += '-lsun-dog';            dataset = EasyDict(tfrecord_dir='lsun-dog-100k');            train.mirror_augment = True\n",
        "#desc += '-lsun-horse';          dataset = EasyDict(tfrecord_dir='lsun-horse-100k');          train.mirror_augment = True\n",
        "#desc += '-lsun-kitchen';        dataset = EasyDict(tfrecord_dir='lsun-kitchen-100k');        train.mirror_augment = True\n",
        "#desc += '-lsun-livingroom';     dataset = EasyDict(tfrecord_dir='lsun-livingroom-100k');     train.mirror_augment = True\n",
        "#desc += '-lsun-motorbike';      dataset = EasyDict(tfrecord_dir='lsun-motorbike-100k');      train.mirror_augment = True\n",
        "#desc += '-lsun-person';         dataset = EasyDict(tfrecord_dir='lsun-person-100k');         train.mirror_augment = True\n",
        "#desc += '-lsun-pottedplant';    dataset = EasyDict(tfrecord_dir='lsun-pottedplant-100k');    train.mirror_augment = True\n",
        "#desc += '-lsun-restaurant';     dataset = EasyDict(tfrecord_dir='lsun-restaurant-100k');     train.mirror_augment = True\n",
        "#desc += '-lsun-sheep';          dataset = EasyDict(tfrecord_dir='lsun-sheep-100k');          train.mirror_augment = True\n",
        "#desc += '-lsun-sofa';           dataset = EasyDict(tfrecord_dir='lsun-sofa-100k');           train.mirror_augment = True\n",
        "#desc += '-lsun-tower';          dataset = EasyDict(tfrecord_dir='lsun-tower-100k');          train.mirror_augment = True\n",
        "#desc += '-lsun-train';          dataset = EasyDict(tfrecord_dir='lsun-train-100k');          train.mirror_augment = True\n",
        "#desc += '-lsun-tvmonitor';      dataset = EasyDict(tfrecord_dir='lsun-tvmonitor-100k');      train.mirror_augment = True\n",
        "\n",
        "# Conditioning & snapshot options.\n",
        "#desc += '-cond'; dataset.max_label_size = 'full' # conditioned on full label\n",
        "#desc += '-cond1'; dataset.max_label_size = 1 # conditioned on first component of the label\n",
        "#desc += '-g4k'; grid.size = '4k'\n",
        "#desc += '-grpc'; grid.layout = 'row_per_class'\n",
        "\n",
        "# Config presets (choose one).\n",
        "#desc += '-preset-v1-1gpu'; num_gpus = 1; D.mbstd_group_size = 16; sched.minibatch_base = 16; sched.minibatch_dict = {256: 14, 512: 6, 1024: 3}; sched.lod_training_kimg = 800; sched.lod_transition_kimg = 800; train.total_kimg = 19000\n",
        "desc += '-preset-v2-1gpu'; num_gpus = 1; sched.minibatch_base = 4; sched.minibatch_dict = {4: 128, 8: 128, 16: 128, 32: 64, 64: 32, 128: 16, 256: 8, 512: 4}; sched.G_lrate_dict = {1024: 0.0015}; sched.D_lrate_dict = EasyDict(sched.G_lrate_dict); train.total_kimg = 12000\n",
        "#desc += '-preset-v2-2gpus'; num_gpus = 2; sched.minibatch_base = 8; sched.minibatch_dict = {4: 256, 8: 256, 16: 128, 32: 64, 64: 32, 128: 16, 256: 8}; sched.G_lrate_dict = {512: 0.0015, 1024: 0.002}; sched.D_lrate_dict = EasyDict(sched.G_lrate_dict); train.total_kimg = 12000\n",
        "#desc += '-preset-v2-4gpus'; num_gpus = 4; sched.minibatch_base = 16; sched.minibatch_dict = {4: 512, 8: 256, 16: 128, 32: 64, 64: 32, 128: 16}; sched.G_lrate_dict = {256: 0.0015, 512: 0.002, 1024: 0.003}; sched.D_lrate_dict = EasyDict(sched.G_lrate_dict); train.total_kimg = 12000\n",
        "#desc += '-preset-v2-8gpus'; num_gpus = 8; sched.minibatch_base = 32; sched.minibatch_dict = {4: 512, 8: 256, 16: 128, 32: 64, 64: 32}; sched.G_lrate_dict = {128: 0.0015, 256: 0.002, 512: 0.003, 1024: 0.003}; sched.D_lrate_dict = EasyDict(sched.G_lrate_dict); train.total_kimg = 12000\n",
        "\n",
        "# Numerical precision (choose one).\n",
        "desc += '-fp32'; sched.max_minibatch_per_gpu = {256: 16, 512: 8, 1024: 4}\n",
        "#desc += '-fp16'; G.dtype = 'float16'; D.dtype = 'float16'; G.pixelnorm_epsilon=1e-4; G_opt.use_loss_scaling = True; D_opt.use_loss_scaling = True; sched.max_minibatch_per_gpu = {512: 16, 1024: 8}\n",
        "\n",
        "# Disable individual features.\n",
        "#desc += '-nogrowing'; sched.lod_initial_resolution = 1024; sched.lod_training_kimg = 0; sched.lod_transition_kimg = 0; train.total_kimg = 10000\n",
        "#desc += '-nopixelnorm'; G.use_pixelnorm = False\n",
        "#desc += '-nowscale'; G.use_wscale = False; D.use_wscale = False\n",
        "#desc += '-noleakyrelu'; G.use_leakyrelu = False\n",
        "#desc += '-nosmoothing'; train.G_smoothing = 0.0\n",
        "#desc += '-norepeat'; train.minibatch_repeats = 1\n",
        "#desc += '-noreset'; train.reset_opt_for_new_lod = False\n",
        "\n",
        "# Special modes.\n",
        "#desc += '-BENCHMARK'; sched.lod_initial_resolution = 4; sched.lod_training_kimg = 3; sched.lod_transition_kimg = 3; train.total_kimg = (8*2+1)*3; sched.tick_kimg_base = 1; sched.tick_kimg_dict = {}; train.image_snapshot_ticks = 1000; train.network_snapshot_ticks = 1000\n",
        "#desc += '-BENCHMARK0'; sched.lod_initial_resolution = 1024; train.total_kimg = 10; sched.tick_kimg_base = 1; sched.tick_kimg_dict = {}; train.image_snapshot_ticks = 1000; train.network_snapshot_ticks = 1000\n",
        "#desc += '-VERBOSE'; sched.tick_kimg_base = 1; sched.tick_kimg_dict = {}; train.image_snapshot_ticks = 1; train.network_snapshot_ticks = 100\n",
        "#desc += '-GRAPH'; train.save_tf_graph = True\n",
        "#desc += '-HIST'; train.save_weight_histograms = True\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Utility scripts.\n",
        "# To run, uncomment the appropriate line and launch train.py.\n",
        "\n",
        "#train = EasyDict(func='util_scripts.generate_fake_images', run_id=23, num_pngs=1000); num_gpus = 1; desc = 'fake-images-' + str(train.run_id)\n",
        "#train = EasyDict(func='util_scripts.generate_fake_images', run_id=23, grid_size=[15,8], num_pngs=10, image_shrink=4); num_gpus = 1; desc = 'fake-grids-' + str(train.run_id)\n",
        "#train = EasyDict(func='util_scripts.generate_interpolation_video', run_id=23, grid_size=[1,1], duration_sec=60.0, smoothing_sec=1.0); num_gpus = 1; desc = 'interpolation-video-' + str(train.run_id)\n",
        "#train = EasyDict(func='util_scripts.generate_training_video', run_id=23, duration_sec=20.0); num_gpus = 1; desc = 'training-video-' + str(train.run_id)\n",
        "\n",
        "#train = EasyDict(func='util_scripts.evaluate_metrics', run_id=23, log='metric-swd-16k.txt', metrics=['swd'], num_images=16384, real_passes=2); num_gpus = 1; desc = train.log.split('.')[0] + '-' + str(train.run_id)\n",
        "#train = EasyDict(func='util_scripts.evaluate_metrics', run_id=23, log='metric-fid-10k.txt', metrics=['fid'], num_images=10000, real_passes=1); num_gpus = 1; desc = train.log.split('.')[0] + '-' + str(train.run_id)\n",
        "#train = EasyDict(func='util_scripts.evaluate_metrics', run_id=23, log='metric-fid-50k.txt', metrics=['fid'], num_images=50000, real_passes=1); num_gpus = 1; desc = train.log.split('.')[0] + '-' + str(train.run_id)\n",
        "#train = EasyDict(func='util_scripts.evaluate_metrics', run_id=23, log='metric-is-50k.txt', metrics=['is'], num_images=50000, real_passes=1); num_gpus = 1; desc = train.log.split('.')[0] + '-' + str(train.run_id)\n",
        "#train = EasyDict(func='util_scripts.evaluate_metrics', run_id=23, log='metric-msssim-20k.txt', metrics=['msssim'], num_images=20000, real_passes=1); num_gpus = 1; desc = train.log.split('.')[0] + '-' + str(train.run_id)\n",
        "\n",
        "#----------------------------------------------------------------------------\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1efzxvQGWkeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# loss\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Convenience func that casts all of its arguments to tf.float32.\n",
        "\n",
        "def fp32(*values):\n",
        "    if len(values) == 1 and isinstance(values[0], tuple):\n",
        "        values = values[0]\n",
        "    values = tuple(tf.cast(v, tf.float32) for v in values)\n",
        "    return values if len(values) >= 2 else values[0]\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Generator loss function used in the paper (WGAN + AC-GAN).\n",
        "\n",
        "def G_wgan_acgan(G, D, opt, training_set, minibatch_size,\n",
        "    cond_weight = 1.0): # Weight of the conditioning term.\n",
        "\n",
        "    latents = tf.random_normal([minibatch_size] + G.input_shapes[0][1:])\n",
        "    labels = training_set.get_random_labels_tf(minibatch_size)\n",
        "    fake_images_out = G.get_output_for(latents, labels, is_training=True)\n",
        "    fake_scores_out, fake_labels_out = fp32(D.get_output_for(fake_images_out, is_training=True))\n",
        "    loss = -fake_scores_out\n",
        "\n",
        "    if D.output_shapes[1][1] > 0:\n",
        "        with tf.name_scope('LabelPenalty'):\n",
        "            label_penalty_fakes = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=fake_labels_out)\n",
        "        loss += label_penalty_fakes * cond_weight\n",
        "    return loss\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Discriminator loss function used in the paper (WGAN-GP + AC-GAN).\n",
        "\n",
        "def D_wgangp_acgan(G, D, opt, training_set, minibatch_size, reals, labels,\n",
        "    wgan_lambda     = 10.0,     # Weight for the gradient penalty term.\n",
        "    wgan_epsilon    = 0.001,    # Weight for the epsilon term, \\epsilon_{drift}.\n",
        "    wgan_target     = 1.0,      # Target value for gradient magnitudes.\n",
        "    cond_weight     = 1.0):     # Weight of the conditioning terms.\n",
        "\n",
        "    latents = tf.random_normal([minibatch_size] + G.input_shapes[0][1:])\n",
        "    fake_images_out = G.get_output_for(latents, labels, is_training=True)\n",
        "    real_scores_out, real_labels_out = fp32(D.get_output_for(reals, is_training=True))\n",
        "    fake_scores_out, fake_labels_out = fp32(D.get_output_for(fake_images_out, is_training=True))\n",
        "    real_scores_out = autosummary('Loss/real_scores', real_scores_out)\n",
        "    fake_scores_out = autosummary('Loss/fake_scores', fake_scores_out)\n",
        "    loss = fake_scores_out - real_scores_out\n",
        "\n",
        "    with tf.name_scope('GradientPenalty'):\n",
        "        mixing_factors = tf.random_uniform([minibatch_size, 1, 1, 1], 0.0, 1.0, dtype=fake_images_out.dtype)\n",
        "        mixed_images_out =  lerp(tf.cast(reals, fake_images_out.dtype), fake_images_out, mixing_factors)\n",
        "        mixed_scores_out, mixed_labels_out = fp32(D.get_output_for(mixed_images_out, is_training=True))\n",
        "        mixed_scores_out =  autosummary('Loss/mixed_scores', mixed_scores_out)\n",
        "        mixed_loss = opt.apply_loss_scaling(tf.reduce_sum(mixed_scores_out))\n",
        "        mixed_grads = opt.undo_loss_scaling(fp32(tf.gradients(mixed_loss, [mixed_images_out])[0]))\n",
        "        mixed_norms = tf.sqrt(tf.reduce_sum(tf.square(mixed_grads), axis=[1,2,3]))\n",
        "        mixed_norms = autosummary('Loss/mixed_norms', mixed_norms)\n",
        "        gradient_penalty = tf.square(mixed_norms - wgan_target)\n",
        "    loss += gradient_penalty * (wgan_lambda / (wgan_target**2))\n",
        "\n",
        "    with tf.name_scope('EpsilonPenalty'):\n",
        "        epsilon_penalty = autosummary('Loss/epsilon_penalty', tf.square(real_scores_out))\n",
        "    loss += epsilon_penalty * wgan_epsilon\n",
        "\n",
        "    if D.output_shapes[1][1] > 0:\n",
        "        with tf.name_scope('LabelPenalty'):\n",
        "            label_penalty_reals = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=real_labels_out)\n",
        "            label_penalty_fakes = tf.nn.softmax_cross_entropy_with_logits_v2(labels=labels, logits=fake_labels_out)\n",
        "            label_penalty_reals = autosummary('Loss/label_penalty_reals', label_penalty_reals)\n",
        "            label_penalty_fakes = autosummary('Loss/label_penalty_fakes', label_penalty_fakes)\n",
        "        loss += (label_penalty_reals + label_penalty_fakes) * cond_weight\n",
        "    return loss\n",
        "\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mFIBZ-2pQzJY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tfutil\n",
        "\n",
        "def run(*args, **kwargs):  # Run the specified ops in the default session.\n",
        "    return tf.get_default_session().run(*args, **kwargs)\n",
        "\n",
        "\n",
        "def is_tf_expression(x):\n",
        "    return isinstance(x, tf.Tensor) or isinstance(x, tf.Variable) or isinstance(x, tf.Operation)\n",
        "\n",
        "\n",
        "def shape_to_list(shape):\n",
        "    return [dim.value for dim in shape]\n",
        "\n",
        "\n",
        "def flatten(x):\n",
        "    with tf.name_scope('Flatten'):\n",
        "        return tf.reshape(x, [-1])\n",
        "\n",
        "\n",
        "def log2(x):\n",
        "    with tf.name_scope('Log2'):\n",
        "        return tf.log(x) * np.float32(1.0 / np.log(2.0))\n",
        "\n",
        "\n",
        "def exp2(x):\n",
        "    with tf.name_scope('Exp2'):\n",
        "        return tf.exp(x * np.float32(np.log(2.0)))\n",
        "\n",
        "\n",
        "def lerp(a, b, t):\n",
        "    with tf.name_scope('Lerp'):\n",
        "        return a + (b - a) * t\n",
        "\n",
        "\n",
        "def lerp_clip(a, b, t):\n",
        "    with tf.name_scope('LerpClip'):\n",
        "        return a + (b - a) * tf.clip_by_value(t, 0.0, 1.0)\n",
        "\n",
        "\n",
        "def absolute_name_scope(scope):  # Forcefully enter the specified name scope, ignoring any surrounding scopes.\n",
        "    return tf.name_scope(scope + '/')\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Initialize TensorFlow graph and session using good default settings.\n",
        "\n",
        "def init_tf(config_dict=dict()):\n",
        "    if tf.get_default_session() is None:\n",
        "        tf.set_random_seed(np.random.randint(1 << 31))\n",
        "        create_session(config_dict, force_as_default=True)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Create tf.Session based on config dict of the form\n",
        "# {'gpu_options.allow_growth': True}\n",
        "\n",
        "def create_session(config_dict=dict(), force_as_default=False):\n",
        "    config = tf.ConfigProto()\n",
        "    for key, value in config_dict.items():\n",
        "        fields = key.split('.')\n",
        "        obj = config\n",
        "        for field in fields[:-1]:\n",
        "            obj = getattr(obj, field)\n",
        "        setattr(obj, fields[-1], value)\n",
        "    session = tf.Session(config=config)\n",
        "    if force_as_default:\n",
        "        session._default_session = session.as_default()\n",
        "        session._default_session.enforce_nesting = False\n",
        "        session._default_session.__enter__()\n",
        "    return session\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Initialize all tf.Variables that have not already been initialized.\n",
        "# Equivalent to the following, but more efficient and does not bloat the tf graph:\n",
        "#   tf.variables_initializer(tf.report_unitialized_variables()).run()\n",
        "\n",
        "def init_uninited_vars(vars=None):\n",
        "    if vars is None: vars = tf.global_variables()\n",
        "    test_vars = [];\n",
        "    test_ops = []\n",
        "    with tf.control_dependencies(None):  # ignore surrounding control_dependencies\n",
        "        for var in vars:\n",
        "            assert is_tf_expression(var)\n",
        "            try:\n",
        "                tf.get_default_graph().get_tensor_by_name(var.name.replace(':0', '/IsVariableInitialized:0'))\n",
        "            except KeyError:\n",
        "                # Op does not exist => variable may be uninitialized.\n",
        "                test_vars.append(var)\n",
        "                with absolute_name_scope(var.name.split(':')[0]):\n",
        "                    test_ops.append(tf.is_variable_initialized(var))\n",
        "    init_vars = [var for var, inited in zip(test_vars, run(test_ops)) if not inited]\n",
        "    run([var.initializer for var in init_vars])\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Set the values of given tf.Variables.\n",
        "# Equivalent to the following, but more efficient and does not bloat the tf graph:\n",
        "#    run([tf.assign(var, value) for var, value in var_to_value_dict.items()]\n",
        "\n",
        "def set_vars(var_to_value_dict):\n",
        "    ops = []\n",
        "    feed_dict = {}\n",
        "    for var, value in var_to_value_dict.items():\n",
        "        assert is_tf_expression(var)\n",
        "        try:\n",
        "            setter = tf.get_default_graph().get_tensor_by_name(\n",
        "                var.name.replace(':0', '/setter:0'))  # look for existing op\n",
        "        except KeyError:\n",
        "            with absolute_name_scope(var.name.split(':')[0]):\n",
        "                with tf.control_dependencies(None):  # ignore surrounding control_dependencies\n",
        "                    setter = tf.assign(var, tf.placeholder(var.dtype, var.shape, 'new_value'),\n",
        "                                       name='setter')  # create new setter\n",
        "        ops.append(setter)\n",
        "        feed_dict[setter.op.inputs[1]] = value\n",
        "    run(ops, feed_dict)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TX0QW-LHQwvL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Autosummary creates an identity op that internally keeps track of the input\n",
        "# values and automatically shows up in TensorBoard. The reported value\n",
        "# represents an average over input components. The average is accumulated\n",
        "# constantly over time and flushed when save_summaries() is called.\n",
        "#\n",
        "# Notes:\n",
        "# - The output tensor must be used as an input for something else in the\n",
        "#   graph. Otherwise, the autosummary op will not get executed, and the average\n",
        "#   value will not get accumulated.\n",
        "# - It is perfectly fine to include autosummaries with the same name in\n",
        "#   several places throughout the graph, even if they are executed concurrently.\n",
        "# - It is ok to also pass in a python scalar or numpy array. In this case, it\n",
        "#   is added to the average immediately.\n",
        "\n",
        "_autosummary_vars = OrderedDict()  # name => [var, ...]\n",
        "_autosummary_immediate = OrderedDict()  # name => update_op, update_value\n",
        "_autosummary_finalized = False\n",
        "\n",
        "\n",
        "def autosummary(name, value):\n",
        "    id = name.replace('/', '_')\n",
        "    if is_tf_expression(value):\n",
        "        with tf.name_scope('summary_' + id), tf.device(value.device):\n",
        "            update_op = _create_autosummary_var(name, value)\n",
        "            with tf.control_dependencies([update_op]):\n",
        "                return tf.identity(value)\n",
        "    else:  # python scalar or numpy array\n",
        "        if name not in _autosummary_immediate:\n",
        "            with absolute_name_scope('Autosummary/' + id), tf.device(None), tf.control_dependencies(None):\n",
        "                update_value = tf.placeholder(tf.float32)\n",
        "                update_op = _create_autosummary_var(name, update_value)\n",
        "                _autosummary_immediate[name] = update_op, update_value\n",
        "        update_op, update_value = _autosummary_immediate[name]\n",
        "        run(update_op, {update_value: np.float32(value)})\n",
        "        return value\n",
        "\n",
        "\n",
        "# Create the necessary ops to include autosummaries in TensorBoard report.\n",
        "# Note: This should be done only once per graph.\n",
        "def finalize_autosummaries():\n",
        "    global _autosummary_finalized\n",
        "    if _autosummary_finalized:\n",
        "        return\n",
        "    _autosummary_finalized = True\n",
        "    init_uninited_vars([var for vars in _autosummary_vars.values() for var in vars])\n",
        "    with tf.device(None), tf.control_dependencies(None):\n",
        "        for name, vars in _autosummary_vars.items():\n",
        "            id = name.replace('/', '_')\n",
        "            with absolute_name_scope('Autosummary/' + id):\n",
        "                sum = tf.add_n(vars)\n",
        "                avg = sum[0] / sum[1]\n",
        "                with tf.control_dependencies([avg]):  # read before resetting\n",
        "                    reset_ops = [tf.assign(var, tf.zeros(2)) for var in vars]\n",
        "                    with tf.name_scope(None), tf.control_dependencies(reset_ops):  # reset before reporting\n",
        "                        tf.summary.scalar(name, avg)\n",
        "\n",
        "\n",
        "# Internal helper for creating autosummary accumulators.\n",
        "def _create_autosummary_var(name, value_expr):\n",
        "    assert not _autosummary_finalized\n",
        "    v = tf.cast(value_expr, tf.float32)\n",
        "    if v.shape.ndims is 0:\n",
        "        v = [v, np.float32(1.0)]\n",
        "    elif v.shape.ndims is 1:\n",
        "        v = [tf.reduce_sum(v), tf.cast(tf.shape(v)[0], tf.float32)]\n",
        "    else:\n",
        "        v = [tf.reduce_sum(v), tf.reduce_prod(tf.cast(tf.shape(v), tf.float32))]\n",
        "    v = tf.cond(tf.is_finite(v[0]), lambda: tf.stack(v), lambda: tf.zeros(2))\n",
        "    with tf.control_dependencies(None):\n",
        "        var = tf.Variable(tf.zeros(2))  # [numerator, denominator]\n",
        "    update_op = tf.cond(tf.is_variable_initialized(var), lambda: tf.assign_add(var, v), lambda: tf.assign(var, v))\n",
        "    if name in _autosummary_vars:\n",
        "        _autosummary_vars[name].append(var)\n",
        "    else:\n",
        "        _autosummary_vars[name] = [var]\n",
        "    return update_op\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Call filewriter.add_summary() with all summaries in the default graph,\n",
        "# automatically finalizing and merging them on the first call.\n",
        "\n",
        "_summary_merge_op = None\n",
        "\n",
        "\n",
        "def save_summaries(filewriter, global_step=None):\n",
        "    global _summary_merge_op\n",
        "    if _summary_merge_op is None:\n",
        "        finalize_autosummaries()\n",
        "        with tf.device(None), tf.control_dependencies(None):\n",
        "            _summary_merge_op = tf.summary.merge_all()\n",
        "    filewriter.add_summary(_summary_merge_op.eval(), global_step)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Utilities for importing modules and objects by name.\n",
        "\n",
        "def import_module(module_or_obj_name):\n",
        "    parts = module_or_obj_name.split('.')\n",
        "    parts[0] = {'np': 'numpy', 'tf': 'tensorflow'}.get(parts[0], parts[0])\n",
        "    for i in range(len(parts), 0, -1):\n",
        "        try:\n",
        "            module = importlib.import_module('.'.join(parts[:i]))\n",
        "            relative_obj_name = '.'.join(parts[i:])\n",
        "            return module, relative_obj_name\n",
        "        except ImportError:\n",
        "            pass\n",
        "    raise ImportError(module_or_obj_name)\n",
        "\n",
        "\n",
        "def find_obj_in_module(module, relative_obj_name):\n",
        "    obj = module\n",
        "    for part in relative_obj_name.split('.'):\n",
        "        obj = getattr(obj, part)\n",
        "    return obj\n",
        "\n",
        "\n",
        "def import_obj(obj_name):\n",
        "    module, relative_obj_name = import_module(obj_name)\n",
        "    return find_obj_in_module(module, relative_obj_name)\n",
        "\n",
        "\n",
        "def call_func_by_name(*args, func=None, **kwargs):\n",
        "    assert func is not None\n",
        "    return import_obj(func)(*args, **kwargs)\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nM3zcwyoQujL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Wrapper for tf.train.Optimizer that automatically takes care of:\n",
        "# - Gradient averaging for multi-GPU training.\n",
        "# - Dynamic loss scaling and typecasts for FP16 training.\n",
        "# - Ignoring corrupted gradients that contain NaNs/Infs.\n",
        "# - Reporting statistics.\n",
        "# - Well-chosen default settings.\n",
        "\n",
        "class Optimizer:\n",
        "    def __init__(\n",
        "            self,\n",
        "            name='Train',\n",
        "            tf_optimizer='tf.train.AdamOptimizer',\n",
        "            learning_rate=0.001,\n",
        "            use_loss_scaling=False,\n",
        "            loss_scaling_init=64.0,\n",
        "            loss_scaling_inc=0.0005,\n",
        "            loss_scaling_dec=1.0,\n",
        "            **kwargs):\n",
        "\n",
        "        # Init fields.\n",
        "        self.name = name\n",
        "        self.learning_rate = tf.convert_to_tensor(learning_rate)\n",
        "        self.id = self.name.replace('/', '.')\n",
        "        self.scope = tf.get_default_graph().unique_name(self.id)\n",
        "        self.optimizer_class = import_obj(tf_optimizer)\n",
        "        self.optimizer_kwargs = dict(kwargs)\n",
        "        self.use_loss_scaling = use_loss_scaling\n",
        "        self.loss_scaling_init = loss_scaling_init\n",
        "        self.loss_scaling_inc = loss_scaling_inc\n",
        "        self.loss_scaling_dec = loss_scaling_dec\n",
        "        self._grad_shapes = None  # [shape, ...]\n",
        "        self._dev_opt = OrderedDict()  # device => optimizer\n",
        "        self._dev_grads = OrderedDict()  # device => [[(grad, var), ...], ...]\n",
        "        self._dev_ls_var = OrderedDict()  # device => variable (log2 of loss scaling factor)\n",
        "        self._updates_applied = False\n",
        "\n",
        "    # Register the gradients of the given loss function with respect to the given variables.\n",
        "    # Intended to be called once per GPU.\n",
        "    def register_gradients(self, loss, vars):\n",
        "        assert not self._updates_applied\n",
        "\n",
        "        # Validate arguments.\n",
        "        if isinstance(vars, dict):\n",
        "            vars = list(vars.values())  # allow passing in Network.trainables as vars\n",
        "        assert isinstance(vars, list) and len(vars) >= 1\n",
        "        assert all(is_tf_expression(expr) for expr in vars + [loss])\n",
        "        if self._grad_shapes is None:\n",
        "            self._grad_shapes = [shape_to_list(var.shape) for var in vars]\n",
        "        assert len(vars) == len(self._grad_shapes)\n",
        "        assert all(shape_to_list(var.shape) == var_shape for var, var_shape in zip(vars, self._grad_shapes))\n",
        "        dev = loss.device\n",
        "        assert all(var.device == dev for var in vars)\n",
        "\n",
        "        # Register device and compute gradients.\n",
        "        with tf.name_scope(self.id + '_grad'), tf.device(dev):\n",
        "            if dev not in self._dev_opt:\n",
        "                opt_name = self.scope.replace('/', '_') + '_opt%d' % len(self._dev_opt)\n",
        "                self._dev_opt[dev] = self.optimizer_class(name=opt_name, learning_rate=self.learning_rate,\n",
        "                                                          **self.optimizer_kwargs)\n",
        "                self._dev_grads[dev] = []\n",
        "            loss = self.apply_loss_scaling(tf.cast(loss, tf.float32))\n",
        "            grads = self._dev_opt[dev].compute_gradients(loss, vars,\n",
        "                                                         gate_gradients=tf.train.Optimizer.GATE_NONE)  # disable gating to reduce memory usage\n",
        "            grads = [(g, v) if g is not None else (tf.zeros_like(v), v) for g, v in\n",
        "                     grads]  # replace disconnected gradients with zeros\n",
        "            self._dev_grads[dev].append(grads)\n",
        "\n",
        "    # Construct training op to update the registered variables based on their gradients.\n",
        "    def apply_updates(self):\n",
        "        assert not self._updates_applied\n",
        "        self._updates_applied = True\n",
        "        devices = list(self._dev_grads.keys())\n",
        "        total_grads = sum(len(grads) for grads in self._dev_grads.values())\n",
        "        assert len(devices) >= 1 and total_grads >= 1\n",
        "        ops = []\n",
        "        with absolute_name_scope(self.scope):\n",
        "\n",
        "            # Cast gradients to FP32 and calculate partial sum within each device.\n",
        "            dev_grads = OrderedDict()  # device => [(grad, var), ...]\n",
        "            for dev_idx, dev in enumerate(devices):\n",
        "                with tf.name_scope('ProcessGrads%d' % dev_idx), tf.device(dev):\n",
        "                    sums = []\n",
        "                    for gv in zip(*self._dev_grads[dev]):\n",
        "                        assert all(v is gv[0][1] for g, v in gv)\n",
        "                        g = [tf.cast(g, tf.float32) for g, v in gv]\n",
        "                        g = g[0] if len(g) == 1 else tf.add_n(g)\n",
        "                        sums.append((g, gv[0][1]))\n",
        "                    dev_grads[dev] = sums\n",
        "\n",
        "            # Sum gradients across devices.\n",
        "            if len(devices) > 1:\n",
        "                with tf.name_scope('SumAcrossGPUs'), tf.device(None):\n",
        "                    for var_idx, grad_shape in enumerate(self._grad_shapes):\n",
        "                        g = [dev_grads[dev][var_idx][0] for dev in devices]\n",
        "                        if np.prod(grad_shape):  # nccl does not support zero-sized tensors\n",
        "                            g = tf.contrib.nccl.all_sum(g)\n",
        "                        for dev, gg in zip(devices, g):\n",
        "                            dev_grads[dev][var_idx] = (gg, dev_grads[dev][var_idx][1])\n",
        "\n",
        "            # Apply updates separately on each device.\n",
        "            for dev_idx, (dev, grads) in enumerate(dev_grads.items()):\n",
        "                with tf.name_scope('ApplyGrads%d' % dev_idx), tf.device(dev):\n",
        "\n",
        "                    # Scale gradients as needed.\n",
        "                    if self.use_loss_scaling or total_grads > 1:\n",
        "                        with tf.name_scope('Scale'):\n",
        "                            coef = tf.constant(np.float32(1.0 / total_grads), name='coef')\n",
        "                            coef = self.undo_loss_scaling(coef)\n",
        "                            grads = [(g * coef, v) for g, v in grads]\n",
        "\n",
        "                    # Check for overflows.\n",
        "                    with tf.name_scope('CheckOverflow'):\n",
        "                        grad_ok = tf.reduce_all(tf.stack([tf.reduce_all(tf.is_finite(g)) for g, v in grads]))\n",
        "\n",
        "                    # Update weights and adjust loss scaling.\n",
        "                    with tf.name_scope('UpdateWeights'):\n",
        "                        opt = self._dev_opt[dev]\n",
        "                        ls_var = self.get_loss_scaling_var(dev)\n",
        "                        if not self.use_loss_scaling:\n",
        "                            ops.append(tf.cond(grad_ok, lambda: opt.apply_gradients(grads), tf.no_op))\n",
        "                        else:\n",
        "                            ops.append(tf.cond(grad_ok,\n",
        "                                               lambda: tf.group(tf.assign_add(ls_var, self.loss_scaling_inc),\n",
        "                                                                opt.apply_gradients(grads)),\n",
        "                                               lambda: tf.group(tf.assign_sub(ls_var, self.loss_scaling_dec))))\n",
        "\n",
        "                    # Report statistics on the last device.\n",
        "                    if dev == devices[-1]:\n",
        "                        with tf.name_scope('Statistics'):\n",
        "                            ops.append(autosummary(self.id + '/learning_rate', self.learning_rate))\n",
        "                            ops.append(autosummary(self.id + '/overflow_frequency', tf.where(grad_ok, 0, 1)))\n",
        "                            if self.use_loss_scaling:\n",
        "                                ops.append(autosummary(self.id + '/loss_scaling_log2', ls_var))\n",
        "\n",
        "            # Initialize variables and group everything into a single op.\n",
        "            self.reset_optimizer_state()\n",
        "            init_uninited_vars(list(self._dev_ls_var.values()))\n",
        "            return tf.group(*ops, name='TrainingOp')\n",
        "\n",
        "    # Reset internal state of the underlying optimizer.\n",
        "    def reset_optimizer_state(self):\n",
        "        run([var.initializer for opt in self._dev_opt.values() for var in opt.variables()])\n",
        "\n",
        "    # Get or create variable representing log2 of the current dynamic loss scaling factor.\n",
        "    def get_loss_scaling_var(self, device):\n",
        "        if not self.use_loss_scaling:\n",
        "            return None\n",
        "        if device not in self._dev_ls_var:\n",
        "            with absolute_name_scope(self.scope + '/LossScalingVars'), tf.control_dependencies(None):\n",
        "                self._dev_ls_var[device] = tf.Variable(np.float32(self.loss_scaling_init), name='loss_scaling_var')\n",
        "        return self._dev_ls_var[device]\n",
        "\n",
        "    # Apply dynamic loss scaling for the given expression.\n",
        "    def apply_loss_scaling(self, value):\n",
        "        assert is_tf_expression(value)\n",
        "        if not self.use_loss_scaling:\n",
        "            return value\n",
        "        return value * exp2(self.get_loss_scaling_var(value.device))\n",
        "\n",
        "    # Undo the effect of dynamic loss scaling for the given expression.\n",
        "    def undo_loss_scaling(self, value):\n",
        "        assert is_tf_expression(value)\n",
        "        if not self.use_loss_scaling:\n",
        "            return value\n",
        "        return value * exp2(-self.get_loss_scaling_var(value.device))\n",
        "\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vt517gyXQrwp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Generic network abstraction.\n",
        "#\n",
        "# Acts as a convenience wrapper for a parameterized network construction\n",
        "# function, providing several utility methods and convenient access to\n",
        "# the inputs/outputs/weights.\n",
        "#\n",
        "# Network objects can be safely pickled and unpickled for long-term\n",
        "# archival purposes. The pickling works reliably as long as the underlying\n",
        "# network construction function is defined in a standalone Python module\n",
        "# that has no side effects or application-specific imports.\n",
        "\n",
        "network_import_handlers = []  # Custom import handlers for dealing with legacy data in pickle import.\n",
        "_network_import_modules = []  # Temporary modules create during pickle import.\n",
        "\n",
        "\n",
        "class Network:\n",
        "    def __init__(self,\n",
        "                 name=None,  # Network name. Used to select TensorFlow name and variable scopes.\n",
        "                 func=None,  # Fully qualified name of the underlying network construction function.\n",
        "                 **static_kwargs):  # Keyword arguments to be passed in to the network construction function.\n",
        "\n",
        "        self._init_fields()\n",
        "        self.name = name\n",
        "        self.static_kwargs = dict(static_kwargs)\n",
        "\n",
        "        # Init build func.\n",
        "        module, self._build_func_name = import_module(func)\n",
        "        self._build_module_src = inspect.getsource(module)\n",
        "        self._build_func = find_obj_in_module(module, self._build_func_name)\n",
        "\n",
        "        # Init graph.\n",
        "        self._init_graph()\n",
        "        self.reset_vars()\n",
        "\n",
        "    def _init_fields(self):\n",
        "        self.name = None  # User-specified name, defaults to build func name if None.\n",
        "        self.scope = None  # Unique TF graph scope, derived from the user-specified name.\n",
        "        self.static_kwargs = dict()  # Arguments passed to the user-supplied build func.\n",
        "        self.num_inputs = 0  # Number of input tensors.\n",
        "        self.num_outputs = 0  # Number of output tensors.\n",
        "        self.input_shapes = [[]]  # Input tensor shapes (NC or NCHW), including minibatch dimension.\n",
        "        self.output_shapes = [[]]  # Output tensor shapes (NC or NCHW), including minibatch dimension.\n",
        "        self.input_shape = []  # Short-hand for input_shapes[0].\n",
        "        self.output_shape = []  # Short-hand for output_shapes[0].\n",
        "        self.input_templates = []  # Input placeholders in the template graph.\n",
        "        self.output_templates = []  # Output tensors in the template graph.\n",
        "        self.input_names = []  # Name string for each input.\n",
        "        self.output_names = []  # Name string for each output.\n",
        "        self.vars = OrderedDict()  # All variables (localname => var).\n",
        "        self.trainables = OrderedDict()  # Trainable variables (localname => var).\n",
        "        self._build_func = None  # User-supplied build function that constructs the network.\n",
        "        self._build_func_name = None  # Name of the build function.\n",
        "        self._build_module_src = None  # Full source code of the module containing the build function.\n",
        "        self._run_cache = dict()  # Cached graph data for Network.run().\n",
        "\n",
        "    def _init_graph(self):\n",
        "        # Collect inputs.\n",
        "        self.input_names = []\n",
        "        for param in inspect.signature(self._build_func).parameters.values():\n",
        "            if param.kind == param.POSITIONAL_OR_KEYWORD and param.default is param.empty:\n",
        "                self.input_names.append(param.name)\n",
        "        self.num_inputs = len(self.input_names)\n",
        "        assert self.num_inputs >= 1\n",
        "\n",
        "        # Choose name and scope.\n",
        "        if self.name is None:\n",
        "            self.name = self._build_func_name\n",
        "        self.scope = tf.get_default_graph().unique_name(self.name.replace('/', '_'), mark_as_used=False)\n",
        "\n",
        "        # Build template graph.\n",
        "        with tf.variable_scope(self.scope, reuse=tf.AUTO_REUSE):\n",
        "            assert tf.get_variable_scope().name == self.scope\n",
        "            with absolute_name_scope(self.scope):  # ignore surrounding name_scope\n",
        "                with tf.control_dependencies(None):  # ignore surrounding control_dependencies\n",
        "                    self.input_templates = [tf.placeholder(tf.float32, name=name) for name in self.input_names]\n",
        "                    out_expr = self._build_func(*self.input_templates, is_template_graph=True, **self.static_kwargs)\n",
        "\n",
        "        # Collect outputs.\n",
        "        assert is_tf_expression(out_expr) or isinstance(out_expr, tuple)\n",
        "        self.output_templates = [out_expr] if is_tf_expression(out_expr) else list(out_expr)\n",
        "        self.output_names = [t.name.split('/')[-1].split(':')[0] for t in self.output_templates]\n",
        "        self.num_outputs = len(self.output_templates)\n",
        "        assert self.num_outputs >= 1\n",
        "\n",
        "        # Populate remaining fields.\n",
        "        self.input_shapes = [shape_to_list(t.shape) for t in self.input_templates]\n",
        "        self.output_shapes = [shape_to_list(t.shape) for t in self.output_templates]\n",
        "        self.input_shape = self.input_shapes[0]\n",
        "        self.output_shape = self.output_shapes[0]\n",
        "        self.vars = OrderedDict([(self.get_var_localname(var), var) for var in tf.global_variables(self.scope + '/')])\n",
        "        self.trainables = OrderedDict(\n",
        "            [(self.get_var_localname(var), var) for var in tf.trainable_variables(self.scope + '/')])\n",
        "\n",
        "    # Run initializers for all variables defined by this network.\n",
        "    def reset_vars(self):\n",
        "        run([var.initializer for var in self.vars.values()])\n",
        "\n",
        "    # Run initializers for all trainable variables defined by this network.\n",
        "    def reset_trainables(self):\n",
        "        run([var.initializer for var in self.trainables.values()])\n",
        "\n",
        "    # Get TensorFlow expression(s) for the output(s) of this network, given the inputs.\n",
        "    def get_output_for(self, *in_expr, return_as_list=False, **dynamic_kwargs):\n",
        "        assert len(in_expr) == self.num_inputs\n",
        "        all_kwargs = dict(self.static_kwargs)\n",
        "        all_kwargs.update(dynamic_kwargs)\n",
        "        with tf.variable_scope(self.scope, reuse=True):\n",
        "            assert tf.get_variable_scope().name == self.scope\n",
        "            named_inputs = [tf.identity(expr, name=name) for expr, name in zip(in_expr, self.input_names)]\n",
        "            out_expr = self._build_func(*named_inputs, **all_kwargs)\n",
        "        assert is_tf_expression(out_expr) or isinstance(out_expr, tuple)\n",
        "        if return_as_list:\n",
        "            out_expr = [out_expr] if is_tf_expression(out_expr) else list(out_expr)\n",
        "        return out_expr\n",
        "\n",
        "    # Get the local name of a given variable, excluding any surrounding name scopes.\n",
        "    def get_var_localname(self, var_or_globalname):\n",
        "        assert is_tf_expression(var_or_globalname) or isinstance(var_or_globalname, str)\n",
        "        globalname = var_or_globalname if isinstance(var_or_globalname, str) else var_or_globalname.name\n",
        "        assert globalname.startswith(self.scope + '/')\n",
        "        localname = globalname[len(self.scope) + 1:]\n",
        "        localname = localname.split(':')[0]\n",
        "        return localname\n",
        "\n",
        "    # Find variable by local or global name.\n",
        "    def find_var(self, var_or_localname):\n",
        "        assert is_tf_expression(var_or_localname) or isinstance(var_or_localname, str)\n",
        "        return self.vars[var_or_localname] if isinstance(var_or_localname, str) else var_or_localname\n",
        "\n",
        "    # Get the value of a given variable as NumPy array.\n",
        "    # Note: This method is very inefficient -- prefer to use  run(list_of_vars) whenever possible.\n",
        "    def get_var(self, var_or_localname):\n",
        "        return self.find_var(var_or_localname).eval()\n",
        "\n",
        "    # Set the value of a given variable based on the given NumPy array.\n",
        "    # Note: This method is very inefficient -- prefer to use  set_vars() whenever possible.\n",
        "    def set_var(self, var_or_localname, new_value):\n",
        "        return set_vars({self.find_var(var_or_localname): new_value})\n",
        "\n",
        "    # Pickle export.\n",
        "    def __getstate__(self):\n",
        "        return {\n",
        "            'version': 2,\n",
        "            'name': self.name,\n",
        "            'static_kwargs': self.static_kwargs,\n",
        "            'build_module_src': self._build_module_src,\n",
        "            'build_func_name': self._build_func_name,\n",
        "            'variables': list(zip(self.vars.keys(), run(list(self.vars.values()))))}\n",
        "\n",
        "    # Pickle import.\n",
        "    def __setstate__(self, state):\n",
        "        self._init_fields()\n",
        "\n",
        "        # Execute custom import handlers.\n",
        "        for handler in network_import_handlers:\n",
        "            state = handler(state)\n",
        "\n",
        "        # Set basic fields.\n",
        "        assert state['version'] == 2\n",
        "        self.name = state['name']\n",
        "        self.static_kwargs = state['static_kwargs']\n",
        "        self._build_module_src = state['build_module_src']\n",
        "        self._build_func_name = state['build_func_name']\n",
        "\n",
        "        # Parse imported module.\n",
        "        module = imp.new_module('_tfutil_network_import_module_%d' % len(_network_import_modules))\n",
        "        exec(self._build_module_src, module.__dict__)\n",
        "        self._build_func = find_obj_in_module(module, self._build_func_name)\n",
        "        _network_import_modules.append(module)  # avoid gc\n",
        "\n",
        "        # Init graph.\n",
        "        self._init_graph()\n",
        "        self.reset_vars()\n",
        "        set_vars({self.find_var(name): value for name, value in state['variables']})\n",
        "\n",
        "    # Create a clone of this network with its own copy of the variables.\n",
        "    def clone(self, name=None):\n",
        "        net = object.__new__(Network)\n",
        "        net._init_fields()\n",
        "        net.name = name if name is not None else self.name\n",
        "        net.static_kwargs = dict(self.static_kwargs)\n",
        "        net._build_module_src = self._build_module_src\n",
        "        net._build_func_name = self._build_func_name\n",
        "        net._build_func = self._build_func\n",
        "        net._init_graph()\n",
        "        net.copy_vars_from(self)\n",
        "        return net\n",
        "\n",
        "    # Copy the values of all variables from the given network.\n",
        "    def copy_vars_from(self, src_net):\n",
        "        assert isinstance(src_net, Network)\n",
        "        name_to_value = run({name: src_net.find_var(name) for name in self.vars.keys()})\n",
        "        set_vars({self.find_var(name): value for name, value in name_to_value.items()})\n",
        "\n",
        "    # Copy the values of all trainable variables from the given network.\n",
        "    def copy_trainables_from(self, src_net):\n",
        "        assert isinstance(src_net, Network)\n",
        "        name_to_value = run({name: src_net.find_var(name) for name in self.trainables.keys()})\n",
        "        set_vars({self.find_var(name): value for name, value in name_to_value.items()})\n",
        "\n",
        "    # Create new network with the given parameters, and copy all variables from this network.\n",
        "    def convert(self, name=None, func=None, **static_kwargs):\n",
        "        net = Network(name, func, **static_kwargs)\n",
        "        net.copy_vars_from(self)\n",
        "        return net\n",
        "\n",
        "    # Construct a TensorFlow op that updates the variables of this network\n",
        "    # to be slightly closer to those of the given network.\n",
        "    def setup_as_moving_average_of(self, src_net, beta=0.99, beta_nontrainable=0.0):\n",
        "        assert isinstance(src_net, Network)\n",
        "        with absolute_name_scope(self.scope):\n",
        "            with tf.name_scope('MovingAvg'):\n",
        "                ops = []\n",
        "                for name, var in self.vars.items():\n",
        "                    if name in src_net.vars:\n",
        "                        cur_beta = beta if name in self.trainables else beta_nontrainable\n",
        "                        new_value = lerp(src_net.vars[name], var, cur_beta)\n",
        "                        ops.append(var.assign(new_value))\n",
        "                return tf.group(*ops)\n",
        "\n",
        "    # Run this network for the given NumPy array(s), and return the output(s) as NumPy array(s).\n",
        "    def run(self, *in_arrays,\n",
        "            return_as_list=False,\n",
        "            # True = return a list of NumPy arrays, False = return a single NumPy array, or a tuple if there are multiple outputs.\n",
        "            print_progress=False,  # Print progress to the console? Useful for very large input arrays.\n",
        "            minibatch_size=None,  # Maximum minibatch size to use, None = disable batching.\n",
        "            num_gpus=1,  # Number of GPUs to use.\n",
        "            out_mul=1.0,  # Multiplicative constant to apply to the output(s).\n",
        "            out_add=0.0,  # Additive constant to apply to the output(s).\n",
        "            out_shrink=1,  # Shrink the spatial dimensions of the output(s) by the given factor.\n",
        "            out_dtype=None,  # Convert the output to the specified data type.\n",
        "            **dynamic_kwargs):  # Additional keyword arguments to pass into the network construction function.\n",
        "\n",
        "        assert len(in_arrays) == self.num_inputs\n",
        "        num_items = in_arrays[0].shape[0]\n",
        "        if minibatch_size is None:\n",
        "            minibatch_size = num_items\n",
        "        key = str([list(sorted(dynamic_kwargs.items())), num_gpus, out_mul, out_add, out_shrink, out_dtype])\n",
        "\n",
        "        # Build graph.\n",
        "        if key not in self._run_cache:\n",
        "            with absolute_name_scope(self.scope + '/Run'), tf.control_dependencies(None):\n",
        "                in_split = list(zip(*[tf.split(x, num_gpus) for x in self.input_templates]))\n",
        "                out_split = []\n",
        "                for gpu in range(num_gpus):\n",
        "                    with tf.device('/gpu:%d' % gpu):\n",
        "                        out_expr = self.get_output_for(*in_split[gpu], return_as_list=True, **dynamic_kwargs)\n",
        "                        if out_mul != 1.0:\n",
        "                            out_expr = [x * out_mul for x in out_expr]\n",
        "                        if out_add != 0.0:\n",
        "                            out_expr = [x + out_add for x in out_expr]\n",
        "                        if out_shrink > 1:\n",
        "                            ksize = [1, 1, out_shrink, out_shrink]\n",
        "                            out_expr = [\n",
        "                                tf.nn.avg_pool(x, ksize=ksize, strides=ksize, padding='VALID', data_format='NCHW') for x\n",
        "                                in out_expr]\n",
        "                        if out_dtype is not None:\n",
        "                            if tf.as_dtype(out_dtype).is_integer:\n",
        "                                out_expr = [tf.round(x) for x in out_expr]\n",
        "                            out_expr = [tf.saturate_cast(x, out_dtype) for x in out_expr]\n",
        "                        out_split.append(out_expr)\n",
        "                self._run_cache[key] = [tf.concat(outputs, axis=0) for outputs in zip(*out_split)]\n",
        "\n",
        "        # Run minibatches.\n",
        "        out_expr = self._run_cache[key]\n",
        "        out_arrays = [np.empty([num_items] + shape_to_list(expr.shape)[1:], expr.dtype.name) for expr in out_expr]\n",
        "        for mb_begin in range(0, num_items, minibatch_size):\n",
        "            if print_progress:\n",
        "                print('\\r%d / %d' % (mb_begin, num_items), end='')\n",
        "            mb_end = min(mb_begin + minibatch_size, num_items)\n",
        "            mb_in = [src[mb_begin: mb_end] for src in in_arrays]\n",
        "            mb_out = tf.get_default_session().run(out_expr, dict(zip(self.input_templates, mb_in)))\n",
        "            for dst, src in zip(out_arrays, mb_out):\n",
        "                dst[mb_begin: mb_end] = src\n",
        "\n",
        "        # Done.\n",
        "        if print_progress:\n",
        "            print('\\r%d / %d' % (num_items, num_items))\n",
        "        if not return_as_list:\n",
        "            out_arrays = out_arrays[0] if len(out_arrays) == 1 else tuple(out_arrays)\n",
        "        return out_arrays\n",
        "\n",
        "    # Returns a list of (name, output_expr, trainable_vars) tuples corresponding to\n",
        "    # individual layers of the network. Mainly intended to be used for reporting.\n",
        "    def list_layers(self):\n",
        "        patterns_to_ignore = ['/Setter', '/new_value', '/Shape', '/strided_slice', '/Cast', '/concat']\n",
        "        all_ops = tf.get_default_graph().get_operations()\n",
        "        all_ops = [op for op in all_ops if not any(p in op.name for p in patterns_to_ignore)]\n",
        "        layers = []\n",
        "\n",
        "        def recurse(scope, parent_ops, level):\n",
        "            prefix = scope + '/'\n",
        "            ops = [op for op in parent_ops if op.name == scope or op.name.startswith(prefix)]\n",
        "\n",
        "            # Does not contain leaf nodes => expand immediate children.\n",
        "            if level == 0 or all('/' in op.name[len(prefix):] for op in ops):\n",
        "                visited = set()\n",
        "                for op in ops:\n",
        "                    suffix = op.name[len(prefix):]\n",
        "                    if '/' in suffix:\n",
        "                        suffix = suffix[:suffix.index('/')]\n",
        "                    if suffix not in visited:\n",
        "                        recurse(prefix + suffix, ops, level + 1)\n",
        "                        visited.add(suffix)\n",
        "\n",
        "            # Otherwise => interpret as a layer.\n",
        "            else:\n",
        "                layer_name = scope[len(self.scope) + 1:]\n",
        "                layer_output = ops[-1].outputs[0]\n",
        "                layer_trainables = [op.outputs[0] for op in ops if\n",
        "                                    op.type.startswith('Variable') and self.get_var_localname(\n",
        "                                        op.name) in self.trainables]\n",
        "                layers.append((layer_name, layer_output, layer_trainables))\n",
        "\n",
        "        recurse(self.scope, all_ops, 0)\n",
        "        return layers\n",
        "\n",
        "    # Print a summary table of the network structure.\n",
        "    def print_layers(self, title=None, hide_layers_with_no_params=False):\n",
        "        if title is None: title = self.name\n",
        "        print()\n",
        "        print('%-28s%-12s%-24s%-24s' % (title, 'Params', 'OutputShape', 'WeightShape'))\n",
        "        print('%-28s%-12s%-24s%-24s' % (('---',) * 4))\n",
        "\n",
        "        total_params = 0\n",
        "        for layer_name, layer_output, layer_trainables in self.list_layers():\n",
        "            weights = [var for var in layer_trainables if var.name.endswith('/weight:0')]\n",
        "            num_params = sum(np.prod(shape_to_list(var.shape)) for var in layer_trainables)\n",
        "            total_params += num_params\n",
        "            if hide_layers_with_no_params and num_params == 0:\n",
        "                continue\n",
        "\n",
        "            print('%-28s%-12s%-24s%-24s' % (\n",
        "                layer_name,\n",
        "                num_params if num_params else '-',\n",
        "                layer_output.shape,\n",
        "                weights[0].shape if len(weights) == 1 else '-'))\n",
        "\n",
        "        print('%-28s%-12s%-24s%-24s' % (('---',) * 4))\n",
        "        print('%-28s%-12s%-24s%-24s' % ('Total', total_params, '', ''))\n",
        "        print()\n",
        "\n",
        "    # Construct summary ops to include histograms of all trainable parameters in TensorBoard.\n",
        "    def setup_weight_histograms(self, title=None):\n",
        "        if title is None: title = self.name\n",
        "        with tf.name_scope(None), tf.device(None), tf.control_dependencies(None):\n",
        "            for localname, var in self.trainables.items():\n",
        "                if '/' in localname:\n",
        "                    p = localname.split('/')\n",
        "                    name = title + '_' + p[-1] + '/' + '_'.join(p[:-1])\n",
        "                else:\n",
        "                    name = title + '_toplevel/' + localname\n",
        "                tf.summary.histogram(name, var)\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4h1_3NVQlP0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import dataset\n",
        "#----------------------------------------------------------------------------\n",
        "# Parse individual image from a tfrecords file.\n",
        "\n",
        "def parse_tfrecord_tf(record):\n",
        "    features = tf.parse_single_example(record, features={\n",
        "        'shape': tf.FixedLenFeature([3], tf.int64),\n",
        "        'data': tf.FixedLenFeature([], tf.string)})\n",
        "    data = tf.decode_raw(features['data'], tf.uint8)\n",
        "    return tf.reshape(data, features['shape'])\n",
        "\n",
        "def parse_tfrecord_np(record):\n",
        "    ex = tf.train.Example()\n",
        "    ex.ParseFromString(record)\n",
        "    shape = ex.features.feature['shape'].int64_list.value\n",
        "    data = ex.features.feature['data'].bytes_list.value[0]\n",
        "    return np.fromstring(data, np.uint8).reshape(shape)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Dataset class that loads data from tfrecords files.\n",
        "\n",
        "class TFRecordDataset:\n",
        "    def __init__(self,\n",
        "        tfrecord_dir,               # Directory containing a collection of tfrecords files.\n",
        "        resolution      = None,     # Dataset resolution, None = autodetect.\n",
        "        label_file      = None,     # Relative path of the labels file, None = autodetect.\n",
        "        max_label_size  = 0,        # 0 = no labels, 'full' = full labels, <int> = N first label components.\n",
        "        repeat          = True,     # Repeat dataset indefinitely.\n",
        "        shuffle_mb      = 4096,     # Shuffle data within specified window (megabytes), 0 = disable shuffling.\n",
        "        prefetch_mb     = 2048,     # Amount of data to prefetch (megabytes), 0 = disable prefetching.\n",
        "        buffer_mb       = 256,      # Read buffer size (megabytes).\n",
        "        num_threads     = 2):       # Number of concurrent threads.\n",
        "\n",
        "        self.tfrecord_dir       = tfrecord_dir\n",
        "        self.resolution         = None\n",
        "        self.resolution_log2    = None\n",
        "        self.shape              = []        # [channel, height, width]\n",
        "        self.dtype              = 'uint8'\n",
        "        self.dynamic_range      = [0, 255]\n",
        "        self.label_file         = label_file\n",
        "        self.label_size         = None      # [component]\n",
        "        self.label_dtype        = None\n",
        "        self._np_labels         = None\n",
        "        self._tf_minibatch_in   = None\n",
        "        self._tf_labels_var     = None\n",
        "        self._tf_labels_dataset = None\n",
        "        self._tf_datasets       = dict()\n",
        "        self._tf_iterator       = None\n",
        "        self._tf_init_ops       = dict()\n",
        "        self._tf_minibatch_np   = None\n",
        "        self._cur_minibatch     = -1\n",
        "        self._cur_lod           = -1\n",
        "\n",
        "        # List tfrecords files and inspect their shapes.\n",
        "        assert os.path.isdir(self.tfrecord_dir)\n",
        "        tfr_files = sorted(glob.glob(os.path.join(self.tfrecord_dir, '*.tfrecords')))\n",
        "        assert len(tfr_files) >= 1\n",
        "        tfr_shapes = []\n",
        "        for tfr_file in tfr_files:\n",
        "            tfr_opt = tf.python_io.TFRecordOptions(tf.python_io.TFRecordCompressionType.NONE)\n",
        "            for record in tf.python_io.tf_record_iterator(tfr_file, tfr_opt):\n",
        "                tfr_shapes.append(parse_tfrecord_np(record).shape)\n",
        "                break\n",
        "\n",
        "        # Autodetect label filename.\n",
        "        if self.label_file is None:\n",
        "            guess = sorted(glob.glob(os.path.join(self.tfrecord_dir, '*.labels')))\n",
        "            if len(guess):\n",
        "                self.label_file = guess[0]\n",
        "        elif not os.path.isfile(self.label_file):\n",
        "            guess = os.path.join(self.tfrecord_dir, self.label_file)\n",
        "            if os.path.isfile(guess):\n",
        "                self.label_file = guess\n",
        "\n",
        "        # Determine shape and resolution.\n",
        "        max_shape = max(tfr_shapes, key=lambda shape: np.prod(shape))\n",
        "        self.resolution = resolution if resolution is not None else max_shape[1]\n",
        "        self.resolution_log2 = int(np.log2(self.resolution))\n",
        "        self.shape = [max_shape[0], self.resolution, self.resolution]\n",
        "        tfr_lods = [self.resolution_log2 - int(np.log2(shape[1])) for shape in tfr_shapes]\n",
        "        assert all(shape[0] == max_shape[0] for shape in tfr_shapes)\n",
        "        assert all(shape[1] == shape[2] for shape in tfr_shapes)\n",
        "        assert all(shape[1] == self.resolution // (2**lod) for shape, lod in zip(tfr_shapes, tfr_lods))\n",
        "        assert all(lod in tfr_lods for lod in range(self.resolution_log2 - 1))\n",
        "\n",
        "        # Load labels.\n",
        "        assert max_label_size == 'full' or max_label_size >= 0\n",
        "        self._np_labels = np.zeros([1<<20, 0], dtype=np.float32)\n",
        "        if self.label_file is not None and max_label_size != 0:\n",
        "            self._np_labels = np.load(self.label_file)\n",
        "            assert self._np_labels.ndim == 2\n",
        "        if max_label_size != 'full' and self._np_labels.shape[1] > max_label_size:\n",
        "            self._np_labels = self._np_labels[:, :max_label_size]\n",
        "        self.label_size = self._np_labels.shape[1]\n",
        "        self.label_dtype = self._np_labels.dtype.name\n",
        "\n",
        "        # Build TF expressions.\n",
        "        with tf.name_scope('Dataset'), tf.device('/cpu:0'):\n",
        "            self._tf_minibatch_in = tf.placeholder(tf.int64, name='minibatch_in', shape=[])\n",
        "            tf_labels_init = tf.zeros(self._np_labels.shape, self._np_labels.dtype)\n",
        "            self._tf_labels_var = tf.Variable(tf_labels_init, name='labels_var')\n",
        "            set_vars({self._tf_labels_var: self._np_labels})\n",
        "            self._tf_labels_dataset = tf.data.Dataset.from_tensor_slices(self._tf_labels_var)\n",
        "            for tfr_file, tfr_shape, tfr_lod in zip(tfr_files, tfr_shapes, tfr_lods):\n",
        "                if tfr_lod < 0:\n",
        "                    continue\n",
        "                dset = tf.data.TFRecordDataset(tfr_file, compression_type='', buffer_size=buffer_mb<<20)\n",
        "                dset = dset.map(parse_tfrecord_tf, num_parallel_calls=num_threads)\n",
        "                dset = tf.data.Dataset.zip((dset, self._tf_labels_dataset))\n",
        "                bytes_per_item = np.prod(tfr_shape) * np.dtype(self.dtype).itemsize\n",
        "                if shuffle_mb > 0:\n",
        "                    dset = dset.shuffle(((shuffle_mb << 20) - 1) // bytes_per_item + 1)\n",
        "                if repeat:\n",
        "                    dset = dset.repeat()\n",
        "                if prefetch_mb > 0:\n",
        "                    dset = dset.prefetch(((prefetch_mb << 20) - 1) // bytes_per_item + 1)\n",
        "                dset = dset.batch(self._tf_minibatch_in)\n",
        "                self._tf_datasets[tfr_lod] = dset\n",
        "            self._tf_iterator = tf.data.Iterator.from_structure(self._tf_datasets[0].output_types, self._tf_datasets[0].output_shapes)\n",
        "            self._tf_init_ops = {lod: self._tf_iterator.make_initializer(dset) for lod, dset in self._tf_datasets.items()}\n",
        "\n",
        "    # Use the given minibatch size and level-of-detail for the data returned by get_minibatch_tf().\n",
        "    def configure(self, minibatch_size, lod=0):\n",
        "        lod = int(np.floor(lod))\n",
        "        assert minibatch_size >= 1 and lod in self._tf_datasets\n",
        "        if self._cur_minibatch != minibatch_size or self._cur_lod != lod:\n",
        "            self._tf_init_ops[lod].run({self._tf_minibatch_in: minibatch_size})\n",
        "            self._cur_minibatch = minibatch_size\n",
        "            self._cur_lod = lod\n",
        "\n",
        "    # Get next minibatch as TensorFlow expressions.\n",
        "    def get_minibatch_tf(self): # => images, labels\n",
        "        return self._tf_iterator.get_next()\n",
        "\n",
        "    # Get next minibatch as NumPy arrays.\n",
        "    def get_minibatch_np(self, minibatch_size, lod=0): # => images, labels\n",
        "        self.configure(minibatch_size, lod)\n",
        "        if self._tf_minibatch_np is None:\n",
        "            self._tf_minibatch_np = self.get_minibatch_tf()\n",
        "        return  run(self._tf_minibatch_np)\n",
        "\n",
        "    # Get random labels as TensorFlow expression.\n",
        "    def get_random_labels_tf(self, minibatch_size): # => labels\n",
        "        if self.label_size > 0:\n",
        "            return tf.gather(self._tf_labels_var, tf.random_uniform([minibatch_size], 0, self._np_labels.shape[0], dtype=tf.int32))\n",
        "        else:\n",
        "            return tf.zeros([minibatch_size, 0], self.label_dtype)\n",
        "\n",
        "    # Get random labels as NumPy array.\n",
        "    def get_random_labels_np(self, minibatch_size): # => labels\n",
        "        if self.label_size > 0:\n",
        "            return self._np_labels[np.random.randint(self._np_labels.shape[0], size=[minibatch_size])]\n",
        "        else:\n",
        "            return np.zeros([minibatch_size, 0], self.label_dtype)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Base class for datasets that are generated on the fly.\n",
        "\n",
        "class SyntheticDataset:\n",
        "    def __init__(self, resolution=1024, num_channels=3, dtype='uint8', dynamic_range=[0,255], label_size=0, label_dtype='float32'):\n",
        "        self.resolution         = resolution\n",
        "        self.resolution_log2    = int(np.log2(resolution))\n",
        "        self.shape              = [num_channels, resolution, resolution]\n",
        "        self.dtype              = dtype\n",
        "        self.dynamic_range      = dynamic_range\n",
        "        self.label_size         = label_size\n",
        "        self.label_dtype        = label_dtype\n",
        "        self._tf_minibatch_var  = None\n",
        "        self._tf_lod_var        = None\n",
        "        self._tf_minibatch_np   = None\n",
        "        self._tf_labels_np      = None\n",
        "\n",
        "        assert self.resolution == 2 ** self.resolution_log2\n",
        "        with tf.name_scope('Dataset'):\n",
        "            self._tf_minibatch_var = tf.Variable(np.int32(0), name='minibatch_var')\n",
        "            self._tf_lod_var = tf.Variable(np.int32(0), name='lod_var')\n",
        "\n",
        "    def configure(self, minibatch_size, lod=0):\n",
        "        lod = int(np.floor(lod))\n",
        "        assert minibatch_size >= 1 and lod >= 0 and lod <= self.resolution_log2\n",
        "        set_vars({self._tf_minibatch_var: minibatch_size, self._tf_lod_var: lod})\n",
        "\n",
        "    def get_minibatch_tf(self): # => images, labels\n",
        "        with tf.name_scope('SyntheticDataset'):\n",
        "            shrink = tf.cast(2.0 ** tf.cast(self._tf_lod_var, tf.float32), tf.int32)\n",
        "            shape = [self.shape[0], self.shape[1] // shrink, self.shape[2] // shrink]\n",
        "            images = self._generate_images(self._tf_minibatch_var, self._tf_lod_var, shape)\n",
        "            labels = self._generate_labels(self._tf_minibatch_var)\n",
        "            return images, labels\n",
        "\n",
        "    def get_minibatch_np(self, minibatch_size, lod=0): # => images, labels\n",
        "        self.configure(minibatch_size, lod)\n",
        "        if self._tf_minibatch_np is None:\n",
        "            self._tf_minibatch_np = self.get_minibatch_tf()\n",
        "        return  run(self._tf_minibatch_np)\n",
        "\n",
        "    def get_random_labels_tf(self, minibatch_size): # => labels\n",
        "        with tf.name_scope('SyntheticDataset'):\n",
        "            return self._generate_labels(minibatch_size)\n",
        "\n",
        "    def get_random_labels_np(self, minibatch_size): # => labels\n",
        "        self.configure(minibatch_size)\n",
        "        if self._tf_labels_np is None:\n",
        "            self._tf_labels_np = self.get_random_labels_tf()\n",
        "        return  run(self._tf_labels_np)\n",
        "\n",
        "    def _generate_images(self, minibatch, lod, shape): # to be overridden by subclasses\n",
        "        return tf.zeros([minibatch] + shape, self.dtype)\n",
        "\n",
        "    def _generate_labels(self, minibatch): # to be overridden by subclasses\n",
        "        return tf.zeros([minibatch, self.label_size], self.label_dtype)\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Helper func for constructing a dataset object using the given options.\n",
        "\n",
        "def load_dataset(class_name='dataset.TFRecordDataset', data_dir=None, verbose=False, **kwargs):\n",
        "    adjusted_kwargs = dict(kwargs)\n",
        "    if 'tfrecord_dir' in adjusted_kwargs and data_dir is not None:\n",
        "        adjusted_kwargs['tfrecord_dir'] = os.path.join(data_dir, adjusted_kwargs['tfrecord_dir'])\n",
        "    if verbose:\n",
        "        print('Streaming data using %s...' % class_name)\n",
        "    dataset =  import_obj(class_name)(**adjusted_kwargs)\n",
        "    if verbose:\n",
        "        print('Dataset shape =', np.int32(dataset.shape).tolist())\n",
        "        print('Dynamic range =', dataset.dynamic_range)\n",
        "        print('Label size    =', dataset.label_size)\n",
        "    return dataset\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1rFUMjQQeLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import misc\n",
        "# import legacy\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Convenience wrappers for pickle that are able to load data produced by\n",
        "# older versions of the code.\n",
        "\n",
        "def load_pkl(filename):\n",
        "    print(\"Don't call me--load_pkl\")\n",
        "\n",
        "\n",
        "#    with open(filename, 'rb') as file:\n",
        "#        return legacy.LegacyUnpickler(file, encoding='latin1').load()\n",
        "\n",
        "def save_pkl(obj, filename):\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(obj, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkVZ7oArQb92",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Image utils.\n",
        "\n",
        "def adjust_dynamic_range(data, drange_in, drange_out):\n",
        "    if drange_in != drange_out:\n",
        "        scale = (np.float32(drange_out[1]) - np.float32(drange_out[0])) / (\n",
        "                    np.float32(drange_in[1]) - np.float32(drange_in[0]))\n",
        "        bias = (np.float32(drange_out[0]) - np.float32(drange_in[0]) * scale)\n",
        "        data = data * scale + bias\n",
        "    return data\n",
        "\n",
        "\n",
        "def create_image_grid(images, grid_size=None):\n",
        "    assert images.ndim == 3 or images.ndim == 4\n",
        "    num, img_w, img_h = images.shape[0], images.shape[-1], images.shape[-2]\n",
        "\n",
        "    if grid_size is not None:\n",
        "        grid_w, grid_h = tuple(grid_size)\n",
        "    else:\n",
        "        grid_w = max(int(np.ceil(np.sqrt(num))), 1)\n",
        "        grid_h = max((num - 1) // grid_w + 1, 1)\n",
        "\n",
        "    grid = np.zeros(list(images.shape[1:-2]) + [grid_h * img_h, grid_w * img_w], dtype=images.dtype)\n",
        "    for idx in range(num):\n",
        "        x = (idx % grid_w) * img_w\n",
        "        y = (idx // grid_w) * img_h\n",
        "        grid[..., y: y + img_h, x: x + img_w] = images[idx]\n",
        "    return grid\n",
        "\n",
        "\n",
        "def convert_to_pil_image(image, drange=[0, 1]):\n",
        "    assert image.ndim == 2 or image.ndim == 3\n",
        "    if image.ndim == 3:\n",
        "        if image.shape[0] == 1:\n",
        "            image = image[0]  # grayscale CHW => HW\n",
        "        else:\n",
        "            image = image.transpose(1, 2, 0)  # CHW -> HWC\n",
        "\n",
        "    image = adjust_dynamic_range(image, drange, [0, 255])\n",
        "    image = np.rint(image).clip(0, 255).astype(np.uint8)\n",
        "    format = 'RGB' if image.ndim == 3 else 'L'\n",
        "    return PIL.Image.fromarray(image, format)\n",
        "\n",
        "\n",
        "def save_image(image, filename, drange=[0, 1], quality=95):\n",
        "    img = convert_to_pil_image(image, drange)\n",
        "    if '.jpg' in filename:\n",
        "        img.save(filename, \"JPEG\", quality=quality, optimize=True)\n",
        "    else:\n",
        "        img.save(filename)\n",
        "\n",
        "\n",
        "def save_image_grid(images, filename, drange=[0, 1], grid_size=None):\n",
        "    convert_to_pil_image(create_image_grid(images, grid_size), drange).save(filename)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Logging of stdout and stderr to a file.\n",
        "\n",
        "class OutputLogger(object):\n",
        "    def __init__(self):\n",
        "        self.file = None\n",
        "        self.buffer = ''\n",
        "\n",
        "    def set_log_file(self, filename, mode='wt'):\n",
        "        assert self.file is None\n",
        "        self.file = open(filename, mode)\n",
        "        if self.buffer is not None:\n",
        "            self.file.write(self.buffer)\n",
        "            self.buffer = None\n",
        "\n",
        "    def write(self, data):\n",
        "        if self.file is not None:\n",
        "            self.file.write(data)\n",
        "        if self.buffer is not None:\n",
        "            self.buffer += data\n",
        "\n",
        "    def flush(self):\n",
        "        if self.file is not None:\n",
        "            self.file.flush()\n",
        "\n",
        "\n",
        "class TeeOutputStream(object):\n",
        "    def __init__(self, child_streams, autoflush=False):\n",
        "        self.child_streams = child_streams\n",
        "        self.autoflush = autoflush\n",
        "\n",
        "    def write(self, data):\n",
        "        for stream in self.child_streams:\n",
        "            stream.write(data)\n",
        "        if self.autoflush:\n",
        "            self.flush()\n",
        "\n",
        "    def flush(self):\n",
        "        for stream in self.child_streams:\n",
        "            stream.flush()\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nk9l6yHSQZQz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "output_logger = None\n",
        "\n",
        "\n",
        "def init_output_logging():\n",
        "    global output_logger\n",
        "    if output_logger is None:\n",
        "        output_logger = OutputLogger()\n",
        "        sys.stdout = TeeOutputStream([sys.stdout, output_logger], autoflush=True)\n",
        "        sys.stderr = TeeOutputStream([sys.stderr, output_logger], autoflush=True)\n",
        "\n",
        "\n",
        "def set_output_log_file(filename, mode='wt'):\n",
        "    if output_logger is not None:\n",
        "        output_logger.set_log_file(filename, mode)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Reporting results.\n",
        "\n",
        "def create_result_subdir(result_dir, desc):\n",
        "    # Select run ID and create subdir.\n",
        "    while True:\n",
        "        run_id = 0\n",
        "        for fname in glob.glob(os.path.join(result_dir, '*')):\n",
        "            try:\n",
        "                fbase = os.path.basename(fname)\n",
        "                ford = int(fbase[:fbase.find('-')])\n",
        "                run_id = max(run_id, ford + 1)\n",
        "            except ValueError:\n",
        "                pass\n",
        "\n",
        "        result_subdir = os.path.join(result_dir, '%03d-%s' % (run_id, desc))\n",
        "        try:\n",
        "            os.makedirs(result_subdir)\n",
        "            break\n",
        "        except OSError:\n",
        "            if os.path.isdir(result_subdir):\n",
        "                continue\n",
        "            raise\n",
        "\n",
        "    print(\"Saving results to\", result_subdir)\n",
        "    set_output_log_file(os.path.join(result_subdir, 'log.txt'))\n",
        "\n",
        "    # Export config.\n",
        "    try:\n",
        "        with open(os.path.join(result_subdir, 'config.txt'), 'wt') as fout:\n",
        "            for k, v in sorted(config.__dict__.items()):\n",
        "                if not k.startswith('_'):\n",
        "                    fout.write(\"%s = %s\\n\" % (k, str(v)))\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return result_subdir\n",
        "\n",
        "\n",
        "def format_time(seconds):\n",
        "    s = int(np.rint(seconds))\n",
        "    if s < 60:\n",
        "        return '%ds' % (s)\n",
        "    elif s < 60 * 60:\n",
        "        return '%dm %02ds' % (s // 60, s % 60)\n",
        "    elif s < 24 * 60 * 60:\n",
        "        return '%dh %02dm %02ds' % (s // (60 * 60), (s // 60) % 60, s % 60)\n",
        "    else:\n",
        "        return '%dd %02dh %02dm' % (s // (24 * 60 * 60), (s // (60 * 60)) % 24, (s // 60) % 60)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Locating results.\n",
        "\n",
        "def locate_result_subdir(run_id_or_result_subdir):\n",
        "    if isinstance(run_id_or_result_subdir, str) and os.path.isdir(run_id_or_result_subdir):\n",
        "        return run_id_or_result_subdir\n",
        "\n",
        "    searchdirs = []\n",
        "    searchdirs += ['']\n",
        "    searchdirs += ['results']\n",
        "    searchdirs += ['networks']\n",
        "\n",
        "    for searchdir in searchdirs:\n",
        "        dir = config.result_dir if searchdir == '' else os.path.join(config.result_dir, searchdir)\n",
        "        dir = os.path.join(dir, str(run_id_or_result_subdir))\n",
        "        if os.path.isdir(dir):\n",
        "            return dir\n",
        "        prefix = '%03d' % run_id_or_result_subdir if isinstance(run_id_or_result_subdir, int) else str(\n",
        "            run_id_or_result_subdir)\n",
        "        dirs = sorted(glob.glob(os.path.join(config.result_dir, searchdir, prefix + '-*')))\n",
        "        dirs = [dir for dir in dirs if os.path.isdir(dir)]\n",
        "        if len(dirs) == 1:\n",
        "            return dirs[0]\n",
        "    raise IOError('Cannot locate result subdir for run', run_id_or_result_subdir)\n",
        "\n",
        "\n",
        "def list_network_pkls(run_id_or_result_subdir, include_final=True):\n",
        "    result_subdir = locate_result_subdir(run_id_or_result_subdir)\n",
        "    pkls = sorted(glob.glob(os.path.join(result_subdir, 'network-*.pkl')))\n",
        "    if len(pkls) >= 1 and os.path.basename(pkls[0]) == 'network-final.pkl':\n",
        "        if include_final:\n",
        "            pkls.append(pkls[0])\n",
        "        del pkls[0]\n",
        "    return pkls\n",
        "\n",
        "\n",
        "def locate_network_pkl(run_id_or_result_subdir_or_network_pkl, snapshot=None):\n",
        "    if isinstance(run_id_or_result_subdir_or_network_pkl, str) and os.path.isfile(\n",
        "            run_id_or_result_subdir_or_network_pkl):\n",
        "        return run_id_or_result_subdir_or_network_pkl\n",
        "\n",
        "    pkls = list_network_pkls(run_id_or_result_subdir_or_network_pkl)\n",
        "    if len(pkls) >= 1 and snapshot is None:\n",
        "        return pkls[-1]\n",
        "    for pkl in pkls:\n",
        "        try:\n",
        "            name = os.path.splitext(os.path.basename(pkl))[0]\n",
        "            number = int(name.split('-')[-1])\n",
        "            if number == snapshot:\n",
        "                return pkl\n",
        "        except ValueError:\n",
        "            pass\n",
        "        except IndexError:\n",
        "            pass\n",
        "    raise IOError('Cannot locate network pkl for snapshot', snapshot)\n",
        "\n",
        "\n",
        "def get_id_string_for_network_pkl(network_pkl):\n",
        "    p = network_pkl.replace('.pkl', '').replace('\\\\', '/').split('/')\n",
        "    return '-'.join(p[max(len(p) - 2, 0):])\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Loading and using trained networks.\n",
        "\n",
        "def load_network_pkl(run_id_or_result_subdir_or_network_pkl, snapshot=None):\n",
        "    return load_pkl(locate_network_pkl(run_id_or_result_subdir_or_network_pkl, snapshot))\n",
        "\n",
        "\n",
        "def random_latents(num_latents, G, random_state=None):\n",
        "    if random_state is not None:\n",
        "        return random_state.randn(num_latents, *G.input_shape[1:]).astype(np.float32)\n",
        "    else:\n",
        "        return np.random.randn(num_latents, *G.input_shape[1:]).astype(np.float32)\n",
        "\n",
        "\n",
        "def load_dataset_for_previous_run(run_id, **kwargs):  # => dataset_obj, mirror_augment\n",
        "    result_subdir = locate_result_subdir(run_id)\n",
        "\n",
        "    # Parse config.txt.\n",
        "    parsed_cfg = dict()\n",
        "    with open(os.path.join(result_subdir, 'config.txt'), 'rt') as f:\n",
        "        for line in f:\n",
        "            if line.startswith('dataset =') or line.startswith('train ='):\n",
        "                exec(line, parsed_cfg, parsed_cfg)\n",
        "    dataset_cfg = parsed_cfg.get('dataset', dict())\n",
        "    train_cfg = parsed_cfg.get('train', dict())\n",
        "    mirror_augment = train_cfg.get('mirror_augment', False)\n",
        "\n",
        "    # Handle legacy options.\n",
        "    if 'h5_path' in dataset_cfg:\n",
        "        dataset_cfg['tfrecord_dir'] = dataset_cfg.pop('h5_path').replace('.h5', '')\n",
        "    if 'mirror_augment' in dataset_cfg:\n",
        "        mirror_augment = dataset_cfg.pop('mirror_augment')\n",
        "    if 'max_labels' in dataset_cfg:\n",
        "        v = dataset_cfg.pop('max_labels')\n",
        "        if v is None: v = 0\n",
        "        if v == 'all': v = 'full'\n",
        "        dataset_cfg['max_label_size'] = v\n",
        "    if 'max_images' in dataset_cfg:\n",
        "        dataset_cfg.pop('max_images')\n",
        "\n",
        "    # Handle legacy dataset names.\n",
        "    v = dataset_cfg['tfrecord_dir']\n",
        "    v = v.replace('-32x32', '').replace('-32', '')\n",
        "    v = v.replace('-128x128', '').replace('-128', '')\n",
        "    v = v.replace('-256x256', '').replace('-256', '')\n",
        "    v = v.replace('-1024x1024', '').replace('-1024', '')\n",
        "    v = v.replace('celeba-hq', 'celebahq')\n",
        "    v = v.replace('cifar-10', 'cifar10')\n",
        "    v = v.replace('cifar-100', 'cifar100')\n",
        "    v = v.replace('mnist-rgb', 'mnistrgb')\n",
        "    v = re.sub('lsun-100k-([^-]*)', 'lsun-\\\\1-100k', v)\n",
        "    v = re.sub('lsun-full-([^-]*)', 'lsun-\\\\1-full', v)\n",
        "    dataset_cfg['tfrecord_dir'] = v\n",
        "\n",
        "    # Load dataset.\n",
        "    dataset_cfg.update(kwargs)\n",
        "    dataset_obj = dataset.load_dataset(data_dir=config.data_dir, **dataset_cfg)\n",
        "    return dataset_obj, mirror_augment\n",
        "\n",
        "\n",
        "def apply_mirror_augment(minibatch):\n",
        "    mask = np.random.rand(minibatch.shape[0]) < 0.5\n",
        "    minibatch = np.array(minibatch)\n",
        "    minibatch[mask] = minibatch[mask, :, :, ::-1]\n",
        "    return minibatch\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n",
        "# Text labels.\n",
        "\n",
        "_text_label_cache = OrderedDict()\n",
        "\n",
        "\n",
        "def draw_text_label(img, text, x, y, alignx=0.5, aligny=0.5, color=255, opacity=1.0, glow_opacity=1.0, **kwargs):\n",
        "    color = np.array(color).flatten().astype(np.float32)\n",
        "    assert img.ndim == 3 and img.shape[2] == color.size or color.size == 1\n",
        "    alpha, glow = setup_text_label(text, **kwargs)\n",
        "    xx, yy = int(np.rint(x - alpha.shape[1] * alignx)), int(np.rint(y - alpha.shape[0] * aligny))\n",
        "    xb, yb = max(-xx, 0), max(-yy, 0)\n",
        "    xe, ye = min(alpha.shape[1], img.shape[1] - xx), min(alpha.shape[0], img.shape[0] - yy)\n",
        "    img = np.array(img)\n",
        "    slice = img[yy + yb: yy + ye, xx + xb: xx + xe, :]\n",
        "    slice[:] = slice * (1.0 - (\n",
        "                1.0 - (1.0 - alpha[yb:ye, xb:xe]) * (1.0 - glow[yb:ye, xb:xe] * glow_opacity)) * opacity)[:, :,\n",
        "                       np.newaxis]\n",
        "    slice[:] = slice + alpha[yb:ye, xb:xe, np.newaxis] * (color * opacity)[np.newaxis, np.newaxis, :]\n",
        "    return img\n",
        "\n",
        "\n",
        "def setup_text_label(text, font='Calibri', fontsize=32, padding=6, glow_size=2.0, glow_coef=3.0, glow_exp=2.0,\n",
        "                     cache_size=100):  # => (alpha, glow)\n",
        "    # Lookup from cache.\n",
        "    key = (text, font, fontsize, padding, glow_size, glow_coef, glow_exp)\n",
        "    if key in _text_label_cache:\n",
        "        value = _text_label_cache[key]\n",
        "        del _text_label_cache[key]  # LRU policy\n",
        "        _text_label_cache[key] = value\n",
        "        return value\n",
        "\n",
        "    # Limit cache size.\n",
        "    while len(_text_label_cache) >= cache_size:\n",
        "        _text_label_cache.popitem(last=False)\n",
        "\n",
        "    # Render text.\n",
        "    import moviepy.editor  # pip install moviepy\n",
        "    alpha = moviepy.editor.TextClip(text, font=font, fontsize=fontsize).mask.make_frame(0)\n",
        "    alpha = np.pad(alpha, padding, mode='constant', constant_values=0.0)\n",
        "    glow = scipy.ndimage.gaussian_filter(alpha, glow_size)\n",
        "    glow = 1.0 - np.maximum(1.0 - glow * glow_coef, 0.0) ** glow_exp\n",
        "\n",
        "    # Add to cache.\n",
        "    value = (alpha, glow)\n",
        "    _text_label_cache[key] = value\n",
        "    return value\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqUl9Xg9QUCo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Choose the size and contents of the image snapshot grids that are exported\n",
        "# periodically during training.\n",
        "\n",
        "def setup_snapshot_image_grid(G, training_set,\n",
        "    size    = '1080p',      # '1080p' = to be viewed on 1080p display, '4k' = to be viewed on 4k display.\n",
        "    layout  = 'random'):    # 'random' = grid contents are selected randomly, 'row_per_class' = each row corresponds to one class label.\n",
        "\n",
        "    # Select size.\n",
        "    gw = 1; gh = 1\n",
        "    if size == '1080p':\n",
        "        gw = np.clip(1920 // G.output_shape[3], 3, 32)\n",
        "        gh = np.clip(1080 // G.output_shape[2], 2, 32)\n",
        "    if size == '4k':\n",
        "        gw = np.clip(3840 // G.output_shape[3], 7, 32)\n",
        "        gh = np.clip(2160 // G.output_shape[2], 4, 32)\n",
        "\n",
        "    # Fill in reals and labels.\n",
        "    reals = np.zeros([gw * gh] + training_set.shape, dtype=training_set.dtype)\n",
        "    labels = np.zeros([gw * gh, training_set.label_size], dtype=training_set.label_dtype)\n",
        "    for idx in range(gw * gh):\n",
        "        x = idx % gw; y = idx // gw\n",
        "        while True:\n",
        "            real, label = training_set.get_minibatch_np(1)\n",
        "            if layout == 'row_per_class' and training_set.label_size > 0:\n",
        "                if label[0, y % training_set.label_size] == 0.0:\n",
        "                    continue\n",
        "            reals[idx] = real[0]\n",
        "            labels[idx] = label[0]\n",
        "            break\n",
        "\n",
        "    # Generate latents.\n",
        "    latents = random_latents(gw * gh, G)\n",
        "    return (gw, gh), reals, labels, latents\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Just-in-time processing of training images before feeding them to the networks.\n",
        "\n",
        "def process_reals(x, lod, mirror_augment, drange_data, drange_net):\n",
        "    with tf.name_scope('ProcessReals'):\n",
        "        with tf.name_scope('DynamicRange'):\n",
        "            x = tf.cast(x, tf.float32)\n",
        "            x = adjust_dynamic_range(x, drange_data, drange_net)\n",
        "        if mirror_augment:\n",
        "            with tf.name_scope('MirrorAugment'):\n",
        "                s = tf.shape(x)\n",
        "                mask = tf.random_uniform([s[0], 1, 1, 1], 0.0, 1.0)\n",
        "                mask = tf.tile(mask, [1, s[1], s[2], s[3]])\n",
        "                x = tf.where(mask < 0.5, x, tf.reverse(x, axis=[3]))\n",
        "        with tf.name_scope('FadeLOD'): # Smooth crossfade between consecutive levels-of-detail.\n",
        "            s = tf.shape(x)\n",
        "            y = tf.reshape(x, [-1, s[1], s[2]//2, 2, s[3]//2, 2])\n",
        "            y = tf.reduce_mean(y, axis=[3, 5], keepdims=True)\n",
        "            y = tf.tile(y, [1, 1, 1, 2, 1, 2])\n",
        "            y = tf.reshape(y, [-1, s[1], s[2], s[3]])\n",
        "            x =  lerp(x, y, lod - tf.floor(lod))\n",
        "        with tf.name_scope('UpscaleLOD'): # Upscale to match the expected input/output size of the networks.\n",
        "            s = tf.shape(x)\n",
        "            factor = tf.cast(2 ** tf.floor(lod), tf.int32)\n",
        "            x = tf.reshape(x, [-1, s[1], s[2], 1, s[3], 1])\n",
        "            x = tf.tile(x, [1, 1, 1, factor, 1, factor])\n",
        "            x = tf.reshape(x, [-1, s[1], s[2] * factor, s[3] * factor])\n",
        "        return x\n",
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Class for evaluating and storing the values of time-varying training parameters.\n",
        "\n",
        "class TrainingSchedule:\n",
        "    def __init__(\n",
        "        self,\n",
        "        cur_nimg,\n",
        "        training_set,\n",
        "        lod_initial_resolution  = 4,        # Image resolution used at the beginning.\n",
        "        lod_training_kimg       = 600,      # Thousands of real images to show before doubling the resolution.\n",
        "        lod_transition_kimg     = 600,      # Thousands of real images to show when fading in new layers.\n",
        "        minibatch_base          = 16,       # Maximum minibatch size, divided evenly among GPUs.\n",
        "        minibatch_dict          = {},       # Resolution-specific overrides.\n",
        "        max_minibatch_per_gpu   = {},       # Resolution-specific maximum minibatch size per GPU.\n",
        "        G_lrate_base            = 0.001,    # Learning rate for the generator.\n",
        "        G_lrate_dict            = {},       # Resolution-specific overrides.\n",
        "        D_lrate_base            = 0.001,    # Learning rate for the discriminator.\n",
        "        D_lrate_dict            = {},       # Resolution-specific overrides.\n",
        "        tick_kimg_base          = 160,      # Default interval of progress snapshots.\n",
        "        tick_kimg_dict          = {4: 160, 8:140, 16:120, 32:100, 64:80, 128:60, 256:40, 512:20, 1024:10}): # Resolution-specific overrides.\n",
        "\n",
        "        # Training phase.\n",
        "        self.kimg = cur_nimg / 1000.0\n",
        "        phase_dur = lod_training_kimg + lod_transition_kimg\n",
        "        phase_idx = int(np.floor(self.kimg / phase_dur)) if phase_dur > 0 else 0\n",
        "        phase_kimg = self.kimg - phase_idx * phase_dur\n",
        "\n",
        "        # Level-of-detail and resolution.\n",
        "        self.lod = training_set.resolution_log2\n",
        "        self.lod -= np.floor(np.log2(lod_initial_resolution))\n",
        "        self.lod -= phase_idx\n",
        "        if lod_transition_kimg > 0:\n",
        "            self.lod -= max(phase_kimg - lod_training_kimg, 0.0) / lod_transition_kimg\n",
        "        self.lod = max(self.lod, 0.0)\n",
        "        self.resolution = 2 ** (training_set.resolution_log2 - int(np.floor(self.lod)))\n",
        "\n",
        "        # Minibatch size.\n",
        "        self.minibatch = minibatch_dict.get(self.resolution, minibatch_base)\n",
        "        self.minibatch -= self.minibatch % config.num_gpus\n",
        "        if self.resolution in max_minibatch_per_gpu:\n",
        "            self.minibatch = min(self.minibatch, max_minibatch_per_gpu[self.resolution] * config.num_gpus)\n",
        "\n",
        "        # Other parameters.\n",
        "        self.G_lrate = G_lrate_dict.get(self.resolution, G_lrate_base)\n",
        "        self.D_lrate = D_lrate_dict.get(self.resolution, D_lrate_base)\n",
        "        self.tick_kimg = tick_kimg_dict.get(self.resolution, tick_kimg_base)\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7phW1CKQRQo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Main training script.\n",
        "# To run, comment/uncomment appropriate lines in config.py and launch train.py.\n",
        "\n",
        "def train_progressive_gan(\n",
        "    G_smoothing             = 0.999,        # Exponential running average of generator weights.\n",
        "    D_repeats               = 1,            # How many times the discriminator is trained per G iteration.\n",
        "    minibatch_repeats       = 4,            # Number of minibatches to run before adjusting training parameters.\n",
        "    reset_opt_for_new_lod   = True,         # Reset optimizer internal state (e.g. Adam moments) when new layers are introduced?\n",
        "    total_kimg              = 15000,        # Total length of the training, measured in thousands of real images.\n",
        "    mirror_augment          = False,        # Enable mirror augment?\n",
        "    drange_net              = [-1,1],       # Dynamic range used when feeding image data to the networks.\n",
        "    image_snapshot_ticks    = 1,            # How often to export image snapshots?\n",
        "    network_snapshot_ticks  = 10,           # How often to export network snapshots?\n",
        "    save_tf_graph           = False,        # Include full TensorFlow computation graph in the tfevents file?\n",
        "    save_weight_histograms  = False,        # Include weight histograms in the tfevents file?\n",
        "    resume_run_id           = None,         # Run ID or network pkl to resume training from, None = start from scratch.\n",
        "    resume_snapshot         = None,         # Snapshot index to resume training from, None = autodetect.\n",
        "    resume_kimg             = 0.0,          # Assumed training progress at the beginning. Affects reporting and training schedule.\n",
        "    resume_time             = 0.0):         # Assumed wallclock time at the beginning. Affects reporting.\n",
        "\n",
        "    maintenance_start_time = time.time()\n",
        "    training_set = dataset.load_dataset(data_dir=config.data_dir, verbose=True, **config.dataset)\n",
        "\n",
        "    # Construct networks.\n",
        "    with tf.device('/gpu:0'):\n",
        "        if resume_run_id is not None:\n",
        "            network_pkl = misc.locate_network_pkl(resume_run_id, resume_snapshot)\n",
        "            print('Loading networks from \"%s\"...' % network_pkl)\n",
        "            G, D, Gs = misc.load_pkl(network_pkl)\n",
        "        else:\n",
        "            print('Constructing networks...')\n",
        "            G =  Network('G', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.G)\n",
        "            D =  Network('D', num_channels=training_set.shape[0], resolution=training_set.shape[1], label_size=training_set.label_size, **config.D)\n",
        "            Gs = G.clone('Gs')\n",
        "        Gs_update_op = Gs.setup_as_moving_average_of(G, beta=G_smoothing)\n",
        "    G.print_layers(); D.print_layers()\n",
        "\n",
        "    print('Building TensorFlow graph...')\n",
        "    with tf.name_scope('Inputs'):\n",
        "        lod_in          = tf.placeholder(tf.float32, name='lod_in', shape=[])\n",
        "        lrate_in        = tf.placeholder(tf.float32, name='lrate_in', shape=[])\n",
        "        minibatch_in    = tf.placeholder(tf.int32, name='minibatch_in', shape=[])\n",
        "        minibatch_split = minibatch_in // config.num_gpus\n",
        "        reals, labels   = training_set.get_minibatch_tf()\n",
        "        reals_split     = tf.split(reals, config.num_gpus)\n",
        "        labels_split    = tf.split(labels, config.num_gpus)\n",
        "    G_opt =  Optimizer(name='TrainG', learning_rate=lrate_in, **config.G_opt)\n",
        "    D_opt =  Optimizer(name='TrainD', learning_rate=lrate_in, **config.D_opt)\n",
        "    for gpu in range(config.num_gpus):\n",
        "        with tf.name_scope('GPU%d' % gpu), tf.device('/gpu:%d' % gpu):\n",
        "            G_gpu = G if gpu == 0 else G.clone(G.name + '_shadow')\n",
        "            D_gpu = D if gpu == 0 else D.clone(D.name + '_shadow')\n",
        "            lod_assign_ops = [tf.assign(G_gpu.find_var('lod'), lod_in), tf.assign(D_gpu.find_var('lod'), lod_in)]\n",
        "            reals_gpu = process_reals(reals_split[gpu], lod_in, mirror_augment, training_set.dynamic_range, drange_net)\n",
        "            labels_gpu = labels_split[gpu]\n",
        "            with tf.name_scope('G_loss'), tf.control_dependencies(lod_assign_ops):\n",
        "                G_loss =  call_func_by_name(G=G_gpu, D=D_gpu, opt=G_opt, training_set=training_set, minibatch_size=minibatch_split, **config.G_loss)\n",
        "            with tf.name_scope('D_loss'), tf.control_dependencies(lod_assign_ops):\n",
        "                D_loss =  call_func_by_name(G=G_gpu, D=D_gpu, opt=D_opt, training_set=training_set, minibatch_size=minibatch_split, reals=reals_gpu, labels=labels_gpu, **config.D_loss)\n",
        "            G_opt.register_gradients(tf.reduce_mean(G_loss), G_gpu.trainables)\n",
        "            D_opt.register_gradients(tf.reduce_mean(D_loss), D_gpu.trainables)\n",
        "    G_train_op = G_opt.apply_updates()\n",
        "    D_train_op = D_opt.apply_updates()\n",
        "\n",
        "    print('Setting up snapshot image grid...')\n",
        "    grid_size, grid_reals, grid_labels, grid_latents = setup_snapshot_image_grid(G, training_set, **config.grid)\n",
        "    sched = TrainingSchedule(total_kimg * 1000, training_set, **config.sched)\n",
        "    grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)\n",
        "\n",
        "    print('Setting up result dir...')\n",
        "    result_subdir = misc.create_result_subdir(config.result_dir, config.desc)\n",
        "    misc.save_image_grid(grid_reals, os.path.join(result_subdir, 'reals.png'), drange=training_set.dynamic_range, grid_size=grid_size)\n",
        "    save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % 0), drange=drange_net, grid_size=grid_size)\n",
        "    summary_log = tf.summary.FileWriter(result_subdir)\n",
        "    if save_tf_graph:\n",
        "        summary_log.add_graph(tf.get_default_graph())\n",
        "    if save_weight_histograms:\n",
        "        G.setup_weight_histograms(); D.setup_weight_histograms()\n",
        "\n",
        "    print('Training...')\n",
        "    cur_nimg = int(resume_kimg * 1000)\n",
        "    cur_tick = 0\n",
        "    tick_start_nimg = cur_nimg\n",
        "    tick_start_time = time.time()\n",
        "    train_start_time = tick_start_time - resume_time\n",
        "    prev_lod = -1.0\n",
        "    while cur_nimg < total_kimg * 1000:\n",
        "\n",
        "        # Choose training parameters and configure training ops.\n",
        "        sched = TrainingSchedule(cur_nimg, training_set, **config.sched)\n",
        "        training_set.configure(sched.minibatch, sched.lod)\n",
        "        if reset_opt_for_new_lod:\n",
        "            if np.floor(sched.lod) != np.floor(prev_lod) or np.ceil(sched.lod) != np.ceil(prev_lod):\n",
        "                G_opt.reset_optimizer_state(); D_opt.reset_optimizer_state()\n",
        "        prev_lod = sched.lod\n",
        "\n",
        "        # Run training ops.\n",
        "        for repeat in range(minibatch_repeats):\n",
        "            for _ in range(D_repeats):\n",
        "                run([D_train_op, Gs_update_op], {lod_in: sched.lod, lrate_in: sched.D_lrate, minibatch_in: sched.minibatch})\n",
        "                cur_nimg += sched.minibatch\n",
        "            run([G_train_op], {lod_in: sched.lod, lrate_in: sched.G_lrate, minibatch_in: sched.minibatch})\n",
        "\n",
        "        # Perform maintenance tasks once per tick.\n",
        "        done = (cur_nimg >= total_kimg * 1000)\n",
        "        if cur_nimg >= tick_start_nimg + sched.tick_kimg * 1000 or done:\n",
        "            cur_tick += 1\n",
        "            cur_time = time.time()\n",
        "            tick_kimg = (cur_nimg - tick_start_nimg) / 1000.0\n",
        "            tick_start_nimg = cur_nimg\n",
        "            tick_time = cur_time - tick_start_time\n",
        "            total_time = cur_time - train_start_time\n",
        "            maintenance_time = tick_start_time - maintenance_start_time\n",
        "            maintenance_start_time = cur_time\n",
        "\n",
        "            # Report progress.\n",
        "            print('tick %-5d kimg %-8.1f lod %-5.2f minibatch %-4d time %-12s sec/tick %-7.1f sec/kimg %-7.2f maintenance %.1f' % (\n",
        "                autosummary('Progress/tick', cur_tick),\n",
        "                autosummary('Progress/kimg', cur_nimg / 1000.0),\n",
        "                autosummary('Progress/lod', sched.lod),\n",
        "                autosummary('Progress/minibatch', sched.minibatch),\n",
        "                format_time(autosummary('Timing/total_sec', total_time)),\n",
        "                autosummary('Timing/sec_per_tick', tick_time),\n",
        "                autosummary('Timing/sec_per_kimg', tick_time / tick_kimg),\n",
        "                autosummary('Timing/maintenance_sec', maintenance_time)))\n",
        "            autosummary('Timing/total_hours', total_time / (60.0 * 60.0))\n",
        "            autosummary('Timing/total_days', total_time / (24.0 * 60.0 * 60.0))\n",
        "            save_summaries(summary_log, cur_nimg)\n",
        "\n",
        "            # Save snapshots.\n",
        "            if cur_tick % image_snapshot_ticks == 0 or done:\n",
        "                grid_fakes = Gs.run(grid_latents, grid_labels, minibatch_size=sched.minibatch//config.num_gpus)\n",
        "                save_image_grid(grid_fakes, os.path.join(result_subdir, 'fakes%06d.png' % (cur_nimg // 1000)), drange=drange_net, grid_size=grid_size)\n",
        "            if cur_tick % network_snapshot_ticks == 0 or done:\n",
        "                save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-snapshot-%06d.pkl' % (cur_nimg // 1000)))\n",
        "\n",
        "            # Record start time of the next tick.\n",
        "            tick_start_time = time.time()\n",
        "\n",
        "    # Write final results.\n",
        "    save_pkl((G, D, Gs), os.path.join(result_subdir, 'network-final.pkl'))\n",
        "    summary_log.close()\n",
        "    open(os.path.join(result_subdir, '_training-done.txt'), 'wt').close()\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tmhrAvKQOUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "outputId": "bacc714a-d181-4f0b-826b-48c0b35c6912"
      },
      "source": [
        "\n",
        "#----------------------------------------------------------------------------\n",
        "# Main entry point.\n",
        "# Calls the function indicated in config.py.\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    init_output_logging()\n",
        "    np.random.seed(random_seed)\n",
        "    print('Initializing TensorFlow...')\n",
        "    os.environ.update(env)\n",
        "    init_tf(tf_config)\n",
        "    print('Running %s()...' % train['func'])\n",
        "    call_func_by_name(**train)\n",
        "    print('Exiting...')\n",
        "\n",
        "#----------------------------------------------------------------------------\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-dffe70d0feb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0minit_tf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Running %s()...'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'func'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mcall_func_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Exiting...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-211591d9757d>\u001b[0m in \u001b[0;36mcall_func_by_name\u001b[0;34m(func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcall_func_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mimport_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-211591d9757d>\u001b[0m in \u001b[0;36mimport_obj\u001b[0;34m(obj_name)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimport_obj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m     \u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_obj_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfind_obj_in_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrelative_obj_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-211591d9757d>\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(module_or_obj_name)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule_or_obj_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: train_progressive_gan",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}