{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IntrotoDeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/slowvak/AI-Deep-Learning-Lab/blob/master/IntrotoDeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": false,
        "id": "0kCscBj8rPbg",
        "colab_type": "text"
      },
      "source": [
        "# Introduction to Deep Learning\n",
        "# RSNA AI Deep Learning Lab\n",
        "\n",
        "###by: Bradley J Erickson, MD PhD\n",
        "*Copyright 2019\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "hide_input": true,
        "id": "n0yBAzDzrPbh",
        "colab_type": "text"
      },
      "source": [
        "In this session we will:\n",
        "1) download and unzip 6 different classes of radiological images (CT Head, CT Chest, CT Abdomen, MRBrain, MRBreast, and Chest X-Ray). \n",
        "2) We will use a Convolutional Neural Network (CNN) pretrained on routine photographic images and using the ResNet-34 architecture to classify each of the 3 types of images\n",
        "3) We will review the performance of the system, note the 'worst' errors, and consider how we might improve performance\n",
        "4) Build a network from scratch using the tensorflow framework and do the same classification problem\n",
        "5) Train a CNN to de-noise low dose CT images  (Starts at Cell 20)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "hide_input": false,
        "id": "EsCBNolBrPbi",
        "colab_type": "code",
        "outputId": "d0327fc5-ed16-4c1e-fb12-acc79dd76064",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 700
        }
      },
      "source": [
        "#Cell 1\n",
        "# first, need to install and then import the fastai library\n",
        "!pip3 install fastai\n",
        "from fastai.vision import *"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: fastai in /usr/local/lib/python3.6/dist-packages (1.0.59)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from fastai) (19.2)\n",
            "Requirement already satisfied: fastprogress>=0.1.19 in /usr/local/lib/python3.6/dist-packages (from fastai) (0.1.21)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (from fastai) (4.6.3)\n",
            "Requirement already satisfied: spacy>=2.0.18 in /usr/local/lib/python3.6/dist-packages (from fastai) (2.1.9)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (from fastai) (0.4.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fastai) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.17.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fastai) (1.3.2)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from fastai) (1.3.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from fastai) (0.7)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.6/dist-packages (from fastai) (7.352.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from fastai) (3.1.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fastai) (3.13)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from fastai) (0.25.3)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from fastai) (2.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.6/dist-packages (from fastai) (4.3.0)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from fastai) (1.3.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (2.4.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->fastai) (1.12.0)\n",
            "Requirement already satisfied: thinc<7.1.0,>=7.0.8 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (7.0.8)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.4.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.3)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.9.6)\n",
            "Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.4)\n",
            "Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (2.0.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (1.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.0.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.0.18->fastai) (0.2.0)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2019.9.11)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fastai) (2.8)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (2.6.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->fastai) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->fastai) (2018.9)\n",
            "Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from Pillow->fastai) (0.46)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.8->spacy>=2.0.18->fastai) (4.28.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib->fastai) (41.6.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JWMnwRXA0vfv",
        "colab_type": "code",
        "outputId": "0398fbde-805a-4f08-9a5f-d8ff56f439c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "#Cell 2\n",
        "# clean out any old data just to be sure, such as if re-running cells\n",
        "!rm -rf MagiciansCorner\n",
        "!rm -rf images\n",
        "!rm -rf sample_data . # Google supplies this but not needed\n",
        "import os\n",
        "\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1mqgBKTB0MtGf8Fhc8HaedJyiD8yMoXOh' -O ./MedNIST.zip\n",
        "\n",
        "!mkdir images\n",
        "!cd images; unzip -q \"../MedNIST.zip\" \n",
        "!rm -rf MagiciansCorner\n",
        "# get rid of MAC garbage stuff\n",
        "!rm -rf ./images/__MACOSX\n",
        "!ls images\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rm: refusing to remove '.' or '..' directory: skipping '.'\n",
            "--2019-11-21 17:30:41--  https://docs.google.com/uc?export=download&id=1mqgBKTB0MtGf8Fhc8HaedJyiD8yMoXOh\n",
            "Resolving docs.google.com (docs.google.com)... 74.125.133.100, 74.125.133.139, 74.125.133.113, ...\n",
            "Connecting to docs.google.com (docs.google.com)|74.125.133.100|:443... connected.\n",
            "HTTP request sent, awaiting response... "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qudSFXxQ7whs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cell 3\n",
        "classes_dir = \"./images\"\n",
        "flist = os.listdir(classes_dir)\n",
        "print (flist)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0r1wat14rPbr",
        "colab_type": "text"
      },
      "source": [
        "### We have already gone through and converted the DICOM images to JPEG (grayscale) images, and also sized so they are all 64x64 (if we kept CXRs as some size other than 64x64, it would cause the CNN to fail--all image MUST be the same size. The next article will explain more about this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDYOXZsPrPcR",
        "colab_type": "text"
      },
      "source": [
        "## View data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md0NwvtbrPcS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cell 4\n",
        "np.random.seed(42)\n",
        "data = ImageDataBunch.from_folder(classes_dir, train=\".\", valid_pct=0.2,\n",
        "        ds_tfms=get_transforms(), size=64, num_workers=4).normalize(imagenet_stats)\n",
        "data.classes\n",
        "data.classes, data.c, len(data.train_ds), len(data.valid_ds)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "In_a6GdirPcZ",
        "colab_type": "text"
      },
      "source": [
        "Good! Let's take a look at some of our pictures then."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9116qi8XrPcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cell 5\n",
        "data.show_batch(rows=3, figsize=(7,8))\n",
        "\n",
        "def get_img(img_url): return open_image(img_url)\n",
        "\n",
        "# Function that displays many transformations of an image\n",
        "def plots_of_one_image(img_url, tfms, rows=1, cols=3, width=15, height=5, **kwargs):\n",
        "    img = get_img(img_url)\n",
        "    [img.apply_tfms(tfms, **kwargs).show(ax=ax)\n",
        "         for i,ax in enumerate(plt.subplots(rows,cols,figsize=(width,height))[1].flatten())]\n",
        "tfms = get_transforms(flip_vert=False,                # flip vertical and horizontal\n",
        "                      max_rotate=20.0,                # rotation between -30° and 30°\n",
        "                      max_zoom=1.2)                   # zoom between 1 and 1.2\n",
        "# Uncomment the line below to turn off augmentation (sets the transformations to nothing. Note that you will still see many images, but they are all the same\n",
        "# tfms=[[],[]]\n",
        "\n",
        "# Uncomment these 3 lines to show examples of artificial/augmented images from 1 starting image\n",
        "# note that 00000124.jpg is my randomly selected head CT\n",
        "# all displayed images are variants of that 1 image\n",
        "#plots_of_one_image('./images/MRBrain/00000129.jpg',tfms[0],9,14,11,7, size=64)\n",
        "#plt.subplots_adjust(left=0, bottom=0,wspace=0, hspace=0)\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gY-nJzEarPcn",
        "colab_type": "text"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUKWKC7yrPco",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cell 6\n",
        "learn = cnn_learner(data, models.resnet34, metrics=error_rate)\n",
        "#learn = cnn_learner(data, models.resnet50, metrics=error_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uj9t1CDFrPcs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cell 7\n",
        "learn.fit_one_cycle(3)\n",
        "learn.save(\"MedNIST-34-1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72n4nM0mbyk6",
        "colab_type": "text"
      },
      "source": [
        "#Evaluation\n",
        "* During the training process, the data is split into 3 parts: training, testing and validation. The training data is used to adjust the weights. The GPU does not have enough RAM to store the entire training set of images, so it is split into 'batches'. When all of the images have been used once for training, then an 'epoch' has passed. Once trained for that epoch, it evaluates how well it has learned using the 'test' data set. The performance on the training set is the train_loss and the performance on the validation set is the valid_loss, and the error_rate is also the percentage of cases wrong in the validation set.\n",
        "* It is common practice that after 'acceptable' performance is achieved on the vclidation set, that the system is tested on the 'test' data, and that is what is considered the 'real' performance.\n",
        "* Note that some use 'test' for what is called validation here, and vice versa.\n",
        "\n",
        "* But sometimes the overall error rate doesn't really tell the story. We might care more about false positives than false negatives, and vica versa. Looking at early results can provide valuable insight into the training process, and how to improve results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGIeWkp3bw1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cell 8\n",
        "interp = ClassificationInterpretation.from_learner(learn)\n",
        "interp.plot_confusion_matrix()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6o6WLPNelGa",
        "colab_type": "text"
      },
      "source": [
        "# Looking closer\n",
        "* The confusion matrix shows that there is more confusion between chests and abdomens than with heads. Does that surprise you?\n",
        "* Lets look a little closer at those. FastAI has a nice function that can show you the cases the it did the worst on. Think about that--there are 'errors' but what are the worst errors?\n",
        "* Well, the class assigned to an image is the class that gets the highest score. So the 'worst' would be those where the score for the correct class was lowest. The function 'plot_top_losses' will show the predicted class, the real class, and the score, as well as the image for the N \n",
        "(in our case, 9) worst scored cases.\n",
        "* The second line of code in the cell shows another nice feature of FastAI: to get documentation on any function, just type 'doc(function)' and it will print the documentation for that function. AND it also has a link you can click to then see teh actual source code that implements that function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j04p1iASeZe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cell 9\n",
        "interp.plot_top_losses(9, figsize=(10,10))\n",
        "#doc(interp.plot_top_losses)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAfypUMmf_ea",
        "colab_type": "text"
      },
      "source": [
        "# What do we see?\n",
        "* Most of the errant classes are slices that contain BOTH lung and abdomen. \n",
        "* This is an important point: Data preparation and curation is critical to getting good results\n",
        "* We can argue about how to handle these cases. The correct answer probably depends on your use case. The point is that without seeing these error cases, fyou might never know what was going wrong...\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-rpC_If1rPc0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cell 10\n",
        "learn.unfreeze()\n",
        "learn.lr_find()\n",
        "learn.recorder.plot()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oplN8lro3Rs-",
        "colab_type": "text"
      },
      "source": [
        "# Improving Performance:\n",
        "* We 'cheated' by starting with a network that was already trained on more than 1,000,000 images. That means the system really only had to learn the specific features of these body parts, but the lower level features like edges and lines were already 'known' to be important to the network.\n",
        "* On the other hand, the 'pretrained' network was trained on photographic images, which are color, not gray scale, and had a matrix size other than 64x64. \n",
        "* At the end of this notebook we create a network from scratch and also have random values for all the weights, forcing the network to 'learn it all'. This will generally take a longer time, and may also require more data to prevent over-fitting, since all weights are free to learn, rather than just a few. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2byn4o2FrPc8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Cell 11\n",
        "learn.fit_one_cycle(5, max_lr=slice(3e-6,3e-5))\n",
        "learn.save(\"Unfreeze-34-1\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfE36Dx5ieRI",
        "colab_type": "text"
      },
      "source": [
        "# Image Classifier in Tensorflow\n",
        "Another framework for deep learning that is probably the most popular is called tensorflow. Version 2 was recently released, and this release is much easier to develop with, at least for many.\n",
        "Here we create a rather simple image classifier which can serve as a good starting point. We will use the same data as for the FastAI example, and will also do image augmentation of similar form.\n",
        "An important difference is that we will explicitly use a data generator, which means that the system will stream examples from disk to the algorithm. The advantage is that this enables us to have training sets much larger than what will fit in the memory of our computer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ka0aX3n6jZM8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 12\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osDVSPQhjsN1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 13\n",
        "import shutil\n",
        "!rm -rf './images/models'\n",
        "!rm -rf './images/train'\n",
        "!rm -rf './images/val'\n",
        "!rm -rf './tf'\n",
        "\n",
        "# Here we will explicitly separate the training and validation data\n",
        "classes_dir = \"./images\"\n",
        "class_list = os.listdir(classes_dir)\n",
        "num_classes = len(class_list)\n",
        "\n",
        "#print (flist)\n",
        "try:\n",
        "    os.mkdir('./tf')\n",
        "    os.mkdir(\"./tf/train\")\n",
        "    os.mkdir(\"./tf/val\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "train_dir = './tf/train'\n",
        "val_dir = './tf/val'\n",
        "\n",
        "for cls in class_list:\n",
        "    tot_files = os.listdir(os.path.join(classes_dir, cls))\n",
        "    num_trn_files = int(len(tot_files) * 0.8)\n",
        "    trn_files = tot_files[0:num_trn_files]\n",
        "    val_files = tot_files[num_trn_files:-1]\n",
        "    src_dir = os.path.join (classes_dir, cls)\n",
        "    target_dir = os.path.join(train_dir, cls)\n",
        "    try:\n",
        "        os.mkdir(target_dir)\n",
        "    except:\n",
        "        pass\n",
        "    for f in trn_files:\n",
        "        shutil.copyfile(os.path.join(src_dir, f), os.path.join(target_dir, f))\n",
        "\n",
        "    target_dir = os.path.join(val_dir, cls)\n",
        "    try:\n",
        "        os.mkdir(target_dir)\n",
        "    except:\n",
        "        pass\n",
        "    for f in val_files:\n",
        "        shutil.copyfile(os.path.join(src_dir, f), os.path.join(target_dir, f))\n",
        "\n",
        "image_gen_train = ImageDataGenerator(\n",
        "                    rescale=1./255,\n",
        "                    rotation_range=10,\n",
        "                    width_shift_range=.15,\n",
        "                    height_shift_range=.15,\n",
        "                    horizontal_flip=True,\n",
        "                    zoom_range=0.1\n",
        "                    )\n",
        "batch_size = 64\n",
        "IMG_SHAPE = 64\n",
        "\n",
        "train_data_gen = image_gen_train.flow_from_directory(\n",
        "                                                batch_size=batch_size,\n",
        "                                                directory=train_dir,\n",
        "                                                shuffle=True,\n",
        "                                                target_size=(IMG_SHAPE,IMG_SHAPE),\n",
        "                                                class_mode='sparse'\n",
        "                                                )\n",
        "# also use generator for validation data\n",
        "image_gen_val = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "val_data_gen = image_gen_val.flow_from_directory(batch_size=batch_size,\n",
        "                                                 directory=val_dir,\n",
        "                                                 target_size=(IMG_SHAPE, IMG_SHAPE),\n",
        "                                                 class_mode='sparse')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_R1V_buPkcos",
        "colab_type": "text"
      },
      "source": [
        "# Create the CNN\n",
        "In the cell below, we create a convolutional neural network that consists of 3 convolution blocks. Each convolutional block contains a Conv2D layer followed by a max pool layer. The first convolutional block should have 16 filters, the second one should have 32 filters, and the third one should have 64 filters. All convolutional filters should be 3 x 3. All max pool layers should have a pool_size of (2, 2).\n",
        "\n",
        "After the 3 convolutional blocks you should have a flatten layer followed by a fully connected layer with 512 units. The CNN should output class probabilities based on 3 classes which is done by the softmax activation function. All other layers should use a relu activation function. You should also add Dropout layers with a probability of 20%, where appropriate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0DM8NcLkiUX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 14\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(16, 5, padding='same', activation='relu', input_shape=(IMG_SHAPE,IMG_SHAPE, 3)))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(32, 3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(64, 3, padding='same', activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnbYtUt-koNc",
        "colab_type": "text"
      },
      "source": [
        "If you want to load a pre-crated model, you can do that\n",
        "See code in teh cell below. We will give it a different\n",
        "model name, and not use it. This is just here in case you want to try\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pMPrMf3k5Sf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 15\n",
        "base_res = tf.keras.applications.ResNet50V2(weights= None, include_top=False, \n",
        "                                          input_shape= (IMG_SHAPE,IMG_SHAPE,3))\n",
        "x = base_res.output\n",
        "x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(num_classes, activation= 'softmax')(x)\n",
        "resnet_model = tf.keras.Model(inputs = base_res.input, outputs = predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbbyGqCWk_km",
        "colab_type": "text"
      },
      "source": [
        "# Compile the Model\n",
        "In the cell below, compile your model using the ADAM optimizer, the sparse cross entropy function as a loss function. We would also like to look at training and validation accuracy on each epoch as we train our network, so make sure you also pass the metrics argument.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jI1yifR8lFZj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 16\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCwnS2g4lJjS",
        "colab_type": "text"
      },
      "source": [
        "# Train the Model\n",
        "Here we actually do the work of having the generator feed the data to the model and train it.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE4XFpxMlR-O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 17\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "epochs = 10\n",
        "checkpoint_path = \"training_1/tf2.ckpt\"\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create a callback that saves the model's weights\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
        "                                                 save_weights_only=True,\n",
        "                                                 verbose=1)\n",
        "\n",
        "\n",
        "history = model.fit_generator(\n",
        "    train_data_gen,\n",
        "    steps_per_epoch=int(np.ceil(train_data_gen.n / float(batch_size))),\n",
        "    epochs=epochs,\n",
        "    callbacks=[cp_callback],\n",
        "    validation_data=val_data_gen,\n",
        "    validation_steps=int(np.ceil(val_data_gen.n / float(batch_size)))\n",
        "    )\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5a-bQi-LlZ0A",
        "colab_type": "text"
      },
      "source": [
        "# Evaluate performance\n",
        "Here we see how well the model performed, including seeing performance on the training and validation sets\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHvCTFYQmgTp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 18\n",
        "\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs_range = range(epochs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs_range, loss, label='Training Loss')\n",
        "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEA9C5NF00v4",
        "colab_type": "text"
      },
      "source": [
        "#  Convolutional Neural Network Denoising: Computed Tomography\n",
        "*Authors: Nathan R. Huber and Andrew D. Missert*\n",
        "\n",
        "CT Clinical Innovation Center, Department of Radiology, Mayo Clinic, Rochester, MN\n",
        "\n",
        "This tutorial demonstrates an application of a deep convolutional neural network (CNN) for reducing noise in computed tomography (CT) images. The CNN is trained to map input low dose CT (LDCT) images to output images which approximate the routine dose CT (RDCT) images. By leveraging the prior information contained in many examples of matched LDCT and RDCT images, CNN denoising can reduce noise while maintaining high levels of anatomic detail.This demo presents a simple overview of the training procedure for illustrative purposes. This is not intended to produce an optimal result. A comparison with more advanced methods is demonstrated at the end of the notebook. \n",
        "\n",
        "# Loading data: Training, validation, and testing\n",
        "In order to train the CNN, we need many examples of LDCT and RDCT images. We must also be careful to evaluate the denoising performance on different images than those use for training. This can be done by partitioning the data into training examples, which are used to optimize the CNN parameters, validation examples, which are used to monitor the optimization process, and testing examples, which are used to check performance after optimization.\n",
        "\n",
        "The data used in this tutorial is made available through the AAPM and Mayo Clinic Low Dose CT Grand Challenge (Medical Physics 44(10), 2017). Each training example consists of a pair of images: the input, which is a simulated 25% dose CT image, and the target, which is the corresponding full-dose CT image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyYogXbfm-bf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 20\n",
        "\n",
        "# load data for tutorial from github\n",
        "\n",
        "!rm -rf images_train\n",
        "!rm -rf images_val\n",
        "!rm -rf images_test\n",
        "!rm -rf Denoising_Data\n",
        "\n",
        "!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1-ZqL_1cqWeG6LsRAB0TwiddW8TgQ-q70' -O ./Denoising_Data.zip\n",
        "!unzip -q \"./Denoising_Data.zip\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhFFEVTkf9KM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 21\n",
        "\n",
        "# load training data, used to update CNN weights\n",
        "# 2000 30x30 image patches from 8 patients\n",
        "train_input = np.load('./Denoising_Data/train_input.npy')\n",
        "train_target = np.load('./Denoising_Data/train_target.npy')\n",
        "# load validation data, used to monitor for overfitting\n",
        "# 1000 30x30 image patches from 1 patient\n",
        "val_input = np.load('./Denoising_Data/val_input.npy')\n",
        "val_target = np.load('./Denoising_Data/val_target.npy')\n",
        "\n",
        "# load testing data, used for evaluating performance\n",
        "# 5 512x512 images from 1 patient\n",
        "test_input = np.load('./Denoising_Data/test_input.npy')\n",
        "test_target = np.load('./Denoising_Data/test_target.npy')\n",
        "\n",
        "# Load examples images from state-of-the-art CNN denoising for CT images\n",
        "test_example = np.load('./Denoising_Data/test_input_denoised.npy')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcuwBLzn1uLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 22\n",
        "\n",
        "# show some examples of training patches\n",
        "nexample = 12\n",
        "plt.figure(figsize=(15, 4))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Low Dose Patch', fontsize=16)\n",
        "plt.imshow(train_input[nexample, :, :, 0], cmap='gray', vmin=-160, vmax=240)\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Noise Map', fontsize=16)\n",
        "plt.imshow(train_target[nexample, :, :, 0] - train_input[nexample, :, :, 0], cmap='gray', vmin=-160, vmax=240)\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title('Routine Dose Patch', fontsize=16)\n",
        "plt.imshow(train_target[nexample, :, :, 0], cmap='gray', vmin=-160, vmax=240)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJnVj5p91zkp",
        "colab_type": "text"
      },
      "source": [
        "# Building Keras model:\n",
        "CNNs are simply mathematical functions that consist of repeated convolution operations. Each convolutional layer convolves multiple filters (kernels) over the input images. The values used for each filter are free parameters that are adjusted during the training procedure. A simple non-linear activation function is also applied between the convolutional layers.Provided below is a basic model for CNN denoising containing convolutional layers and relu activation layers. Each layer operates on the output of the previous layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CstzKnvc1vF5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 23\n",
        "from tensorflow.keras.layers import Activation, Input, Conv2D, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "# model parameters\n",
        "n_layers = 5 #< total number of convolutional layers in this model\n",
        "filters = 16 #< number of feature maps in each layer\n",
        "kernel_size = (3, 3) #< size of convolution kernel for each layer (in pixels)\n",
        "strides = (1, 1) #< number of pixels the kernel moves at each convolutional step\n",
        "\n",
        "# for preprocessing\n",
        "shift_mean = train_input.mean()\n",
        "rescale = train_input.std()\n",
        "\n",
        "# define the model\n",
        "def build_model():\n",
        "\n",
        "    # clear previous models\n",
        "    tf.keras.backend.clear_session()\n",
        "    \n",
        "    # input tensor with arbitrary shape\n",
        "    xin = Input(shape=(None, None, 1), name='input_CT_images')\n",
        "    \n",
        "    # normalize input\n",
        "    x = Lambda(lambda x: (x - shift_mean) / rescale, name='normalize')(xin)\n",
        "    \n",
        "    # create the core convolutional number of layers\n",
        "    for i in range(n_layers - 1):\n",
        "        x = Conv2D(filters=filters,\n",
        "                   kernel_size=kernel_size,\n",
        "                   strides=strides,\n",
        "                   padding='same')(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "    # final layer has just one feature map corresponding to output image\n",
        "    x = Conv2D(filters=1,\n",
        "                  kernel_size=kernel_size,\n",
        "                  strides=strides,\n",
        "                  padding='same')(x)\n",
        "    \n",
        "    # rescale output\n",
        "    xout = Lambda(lambda x: (x * rescale) + shift_mean, name='output_CT_images')(x)\n",
        "    \n",
        "    # to define a model, simply specify the input and output tensors\n",
        "    model = Model(inputs=xin, outputs=xout, name=\"CTDenoiser\")\n",
        "    return model\n",
        "    \n",
        "# build the model\n",
        "model_CT = build_model()\n",
        "model_CT.summary()\n",
        "initial_weights = model_CT.get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjGvEST4143a",
        "colab_type": "text"
      },
      "source": [
        "# Optimization: Training the network\n",
        "Take a moment to appreciate the number of trainable parameters listed in the model summary above. These parameters start off randomly initialized and must be optimized to perform a denoising task.For this tutorial, these parameters will be optimized with one goal in mind: to minimize the mean-squared-error (MSE) difference between the CNN output images and the low-noise target images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2WMHdvi2AJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 24\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# Training parameters \n",
        "epochs = 15             #< determines how many iterations through training dataset during optimization\n",
        "batch_size = 512         #< number of example images used to calculate a single weight update\n",
        "learning_rate = 0.0001  #< adjusts magnitude of changes to CNN weights after each batch\n",
        "print (str(len(train_input)) + ' training examples')\n",
        "# Setup optimzer\n",
        "optimizer = Adam(lr=learning_rate)\n",
        "model_CT.set_weights(initial_weights)\n",
        "model_CT.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "# monitor progress\n",
        "progress_example = 2\n",
        "buffer = 128\n",
        "progress_ims = []\n",
        "\n",
        "#Train the model:\n",
        "for epoch in range(epochs):\n",
        "# evaluate model in current state on reserved validation data\n",
        "    val_mse = model_CT.evaluate(val_input, val_target)\n",
        "    test_img = model_CT.predict(test_input, batch_size=1)\n",
        "    test_img = test_img[progress_example, buffer:-buffer, buffer:-buffer, 0]\n",
        "    progress_ims.append(test_img)\n",
        "    \n",
        "    # update model using training data\n",
        "    istart = 0\n",
        "    while istart < (len(train_input) - batch_size):\n",
        "        x = train_input[istart:istart+batch_size]\n",
        "        y = train_target[istart:istart+batch_size]\n",
        "        model_CT.train_on_batch(x=x, y=y)\n",
        "        istart += batch_size\n",
        "\n",
        "        \n",
        "progress_ims = np.stack(progress_ims, axis=0)\n",
        "\n",
        "print('Training phase complete.')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUMSrnSx2HVC",
        "colab_type": "text"
      },
      "source": [
        " The CNN starts with randomly initialized parameters. During optimization, the parameters are adjusted to minimize the loss function. In order to minimize the loss function, the CNN must compute output images which resemble the corresponding RDCT images. Let's take a look at the outputs at some intermediate steps during training to see how the optimization has progressed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuocDfeB2IeL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 25\n",
        "import scipy\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.style as style\n",
        "# show intermediate images at each epoch:\n",
        "plt.figure(figsize=(16, 16))\n",
        "window_width = 500\n",
        "window_level = 0\n",
        "vmin = window_level - window_width\n",
        "vmax = window_level + window_width\n",
        "\n",
        "vmin = np.min(progress_ims)\n",
        "vmax = np.max(progress_ims)\n",
        "middle_epoch = epochs // 2\n",
        "\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.title('Before Optimization', fontsize=16)\n",
        "plt.axis('off')\n",
        "vmin = np.min(progress_ims[0,:,:])\n",
        "vmax = np.max(progress_ims[0,:,:])\n",
        "plt.imshow(progress_ims[0, :, :], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "\n",
        "plt.subplot(2, 2, 2)\n",
        "plt.title('Epoch %d/%d' %(1, epochs), fontsize=16)\n",
        "plt.axis('off')\n",
        "vmin = np.min(progress_ims[1,:,:])\n",
        "vmax = np.max(progress_ims[1,:,:])\n",
        "plt.imshow(progress_ims[1, :, :], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "\n",
        "plt.subplot(2, 2, 3)\n",
        "plt.title('Epoch %d/%d' %(middle_epoch, epochs), fontsize=16)\n",
        "plt.axis('off')\n",
        "vmin = np.min(progress_ims[epoch,:,:])\n",
        "vmax = np.max(progress_ims[epoch,:,:])\n",
        "plt.imshow(progress_ims[epoch, :, :], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "\n",
        "plt.subplot(2, 2, 4)\n",
        "plt.title('Epoch %d/%d' %(epoch - 1, epochs), fontsize=16)\n",
        "plt.axis('off')\n",
        "plt.imshow(progress_ims[epoch, :, :], cmap='gray', vmin=vmin, vmax=vmax)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nn_cwHp2QeL",
        "colab_type": "text"
      },
      "source": [
        "# Inference: Testing the network\n",
        "Now that the network is fully trained, we can apply to our CNN in inference mode to the reserved testing data to check the performance. Since all CNN parameters are fixed at this point, the processing time for this phase is typically very fast."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKEu1EqZ2Skb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 26\n",
        "import time\n",
        "\n",
        "#Load full quarter dose test image and display result:\n",
        "nexample = 1\n",
        "start = time.time()\n",
        "CNNout = model_CT.predict(test_input)\n",
        "end = time.time()\n",
        "print('Time to apply CNN: ' + str((end - start)/test_input.shape[0]) + ' seconds per image\\n')\n",
        "\n",
        "window_width = 200\n",
        "window_level = 50\n",
        "\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "plt.subplot(1, 3, 1)\n",
        "#vmin = window_level - window_width\n",
        "#vmax = window_level + window_width\n",
        "plt.title('Low Dose Input', fontsize=16)\n",
        "plt.imshow(test_input[nexample, :, :, 0],cmap='gray',vmin=vmin,vmax=vmax)\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Noise Prediction', fontsize=16)\n",
        "plt.imshow(test_input[nexample, :, :, 0] - CNNout[nexample, :, :, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title('CNN Denoised Image', fontsize=16)\n",
        "plt.imshow(CNNout[nexample, :, :, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Low Dose Input - Zoom', fontsize=16)\n",
        "plt.imshow(test_input[nexample, 150:350, 150:350, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('CNN Denoised Image - Zoom', fontsize=16)\n",
        "plt.imshow(CNNout[nexample, 150:350, 150:350, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yy0uxTK02Zys",
        "colab_type": "text"
      },
      "source": [
        "# Analysis: Noise level and line profiles\n",
        "Denoising performance is commonly evaluated on the basis of noise level and spatial resolution. One way to measure the noise level is to calculate root mean square error (RMSE) noise at a uniform region, such as the aorta. Resolution can be evaluated by looking at line profiles or the difference image.For comparison, we have also included some examples from a more advanced CNN-based denoising approach: CNN autotuning. This is a method for optimizing the CNN denoising performance for individual patient exams. More details can be found at the talk \"Patient-Specific Noise Reduction Using a Deep Convolutional Neural Network\" (SSE24-02).   The non-linearity of the CNN function means that the resolution and RMSE performance are highly context dependent. The measurements here may not apply to different input images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U-sYIipH2b-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 27\n",
        "\n",
        "# Standard deviation noise in approximately uniform region (aorta)\n",
        "std_in = test_input[0, 219:239, 201:221, 0].std()\n",
        "std_demo = CNNout[0, 219:239, 201:221, 0].std()\n",
        "std_autotune = test_example[0, 219:239, 201:221, 0].std()\n",
        "\n",
        "plt.figure(figsize=(15, 15))\n",
        "\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Low Dose Input\\n (RMSE) = %.4g HU' % std_in, fontsize=16)\n",
        "plt.imshow(test_input[0, 206:306, 151:231, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.plot([7, 57], [74, 74], 'black', lw=3)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Demo CNN\\n (RMSE) = %.4g HU' % std_demo, fontsize=16)\n",
        "plt.imshow(CNNout[0, 206:306, 151:231, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.plot([7, 57], [74, 74], 'crimson', lw=3)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title('Autotuned CNN\\n (RMSE) = %.4g HU' % std_autotune, fontsize=16)\n",
        "plt.imshow(test_example[0, 206:306, 151:231, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.plot([7, 57], [74, 74], 'mediumblue', lw=3)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# Line profile comparison\n",
        "style.use('ggplot') \n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.title('\\n\\n Line Profile Comparison', fontsize=16)\n",
        "plt.plot(test_input[0, 280, 158:210, 0], label=\"Low Dose Input\",color='black')\n",
        "plt.plot(CNNout[0, 280, 158:210, 0], label=\"Demo CNN\",color='crimson')\n",
        "plt.plot(test_example[0, 280, 158:210, 0], label=\"Autotuned CNN\",color='mediumblue')\n",
        "plt.legend(fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l5sQZQvL2kLM",
        "colab_type": "text"
      },
      "source": [
        "# Comparison: Gaussian and Median Filter\n",
        "To further demonstrate the benefits of deep learning for CT image denoising, we can compare these result with other basic filters. Here we have implemented a Gaussian and median filter. Notice that these basic filters are capable of producing results that are low noise but at the cost of significant resolution loss. Notice how CNN denoising was able to reduce noise while maintaining small features. Again, we provide an example from our research (\"Autotuned CNN\") to demonstrate the performance using more advanced methods than those found in this demo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VuYGfPb2mpE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 28\n",
        "\n",
        "# Visual comparison with basic filtering strategies\n",
        "\n",
        "example_index = 1   #< change this to view a different image\n",
        "frame = 64\n",
        "medfilt = scipy.ndimage.median_filter(test_input[example_index, :, :, 0], size=5)\n",
        "gaussfilt = scipy.ndimage.gaussian_filter(test_input[example_index, :, :, 0], sigma=1.4)\n",
        "\n",
        "plt.figure(figsize=(20, 6))\n",
        "\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.title('Gaussian Filter', fontsize=16)\n",
        "plt.imshow(gaussfilt[frame:-frame, frame:-frame], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 2)\n",
        "plt.title('Median Filter', fontsize=16)\n",
        "plt.imshow(medfilt[frame:-frame, frame:-frame], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 3)\n",
        "plt.title('Demo CNN', fontsize=16)\n",
        "plt.imshow(CNNout[example_index, frame:-frame, frame:-frame, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 4, 4)\n",
        "plt.title('Autotuned CNN', fontsize=16)\n",
        "plt.imshow(test_example[example_index, frame:-frame, frame:-frame, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "If5RsY932vRT",
        "colab_type": "text"
      },
      "source": [
        "I# ndividualized CNN Denoising\n",
        "CNN-based noise reduction leverages prior information from the training data to distinguish between noise and anatomic background in CT images. One potential drawback of this approach is that it it may fail when applied to images that are not sufficiently similar to those used during training. One way around this issue is to fine-tune the CNN to the anatomic features and typical noise textures for individual patient exams. This can help ensure optimal performance on a case-by-case basis. \n",
        "#For more details, see the talk \"Patient-Specific Noise Reduction Using a Deep Convolutional Neural Network\" (SSE24-02) at RSNA this year. \n",
        "Some examples using this technique can be seen below. Notice the increased resolution of anatomic features compared to conventional CNN denoising."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YyFIok5c3AI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Cell 29\n",
        "\n",
        "#Load full quarter dose test image and display result:\n",
        "nexample = 3  #< change this to view a different example (up to 7)\n",
        "\n",
        "window_width = 400\n",
        "window_level = 40\n",
        "vmin = window_level - window_width // 2\n",
        "vmax = window_level + window_width // 2\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Low Dose Input', fontsize=16)\n",
        "plt.imshow(test_input[nexample, :, :, 0],cmap='gray',vmin=vmin,vmax=vmax)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Individualized CNN Denoising\", fontsize=16)\n",
        "plt.imshow(test_example[nexample, :, :, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title('Demo CNN Output', fontsize=16)\n",
        "plt.imshow(CNNout[nexample, :, :, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(18, 9))\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title('Low Dose Input - Zoom', fontsize=16)\n",
        "plt.imshow(test_input[nexample, 150:350, 150:350, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title('Individualized CNN Denoising - Zoom', fontsize=16)\n",
        "plt.imshow(test_example[nexample, 150:350, 150:350, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title('Demo CNN Output - Zoom', fontsize=16)\n",
        "plt.imshow(CNNout[nexample, 150:350, 150:350, 0], cmap='gray', vmin=vmin, vmax=vmax)\n",
        "plt.axis('off')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehv3M8xI3BX2",
        "colab_type": "text"
      },
      "source": [
        "# Challenge:\n",
        "There are many parameters that can be edited in the framework provided above. Can you improve the CNN denoising performance? To get you started try editing the following:\n",
        "Change kernel size\n",
        "Change number of filters\n",
        "Change number of training epochs\n",
        "Add convolutional layers\n",
        "Include more training data (10000 patches available)"
      ]
    }
  ]
}